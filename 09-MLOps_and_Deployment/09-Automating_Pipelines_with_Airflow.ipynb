{
 "nbformat": 5,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automating ML Pipelines with Apache Airflow\n",
    "\n",
    "In this notebook, we'll learn how to automate machine learning workflows using **Apache Airflow**, an open-source platform for programmatically authoring, scheduling, and monitoring workflows.\n",
    "\n",
    "We'll:\n",
    "- Understand Airflow concepts (DAGs, tasks, operators)\n",
    "- Create a simple ML pipeline DAG\n",
    "- Schedule and visualize tasks\n",
    "- Discuss production deployment tips\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ What is Apache Airflow?\n",
    "\n",
    "Airflow allows you to automate data and ML workflows as **Directed Acyclic Graphs (DAGs)**.\n",
    "\n",
    "**Key components:**\n",
    "- **DAG (Directed Acyclic Graph):** Defines the workflow structure.\n",
    "- **Task:** A single unit of work (e.g., data loading, model training).\n",
    "- **Operator:** Defines what each task does (e.g., PythonOperator, BashOperator).\n",
    "- **Scheduler:** Executes tasks as per schedule.\n",
    "- **Web UI:** Monitors runs and dependencies visually.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Installing Airflow (Local Setup)\n",
    "\n",
    "Airflow can be installed using pip (within a virtual environment):\n",
    "```bash\n",
    "pip install apache-airflow\n",
    "```\n",
    "\n",
    "Then initialize Airflow and start the web server:\n",
    "```bash\n",
    "airflow db init\n",
    "airflow webserver --port 8080\n",
    "airflow scheduler\n",
    "```\n",
    "\n",
    "Visit the web UI at [http://localhost:8080](http://localhost:8080).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Creating a Simple ML DAG\n",
    "\n",
    "Below is an example of a DAG automating a small ML pipeline:\n",
    "- **Extract data** ‚Üí **Train model** ‚Üí **Evaluate model** ‚Üí **Save model**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def extract_data():\n",
    "    print('Extracting data...')\n",
    "\n",
    "def train_model():\n",
    "    print('Training ML model...')\n",
    "\n",
    "def evaluate_model():\n",
    "    print('Evaluating model performance...')\n",
    "\n",
    "def save_model():\n",
    "    print('Saving model to storage...')\n",
    "\n",
    "# DAG Definition\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5)\n",
    "}\n",
    "\n",
    "with DAG(\n",
    "    dag_id='ml_pipeline_demo',\n",
    "    default_args=default_args,\n",
    "    start_date=datetime(2025, 1, 1),\n",
    "    schedule_interval='@daily',\n",
    "    catchup=False\n",
    ") as dag:\n",
    "\n",
    "    t1 = PythonOperator(task_id='extract_data', python_callable=extract_data)\n",
    "    t2 = PythonOperator(task_id='train_model', python_callable=train_model)\n",
    "    t3 = PythonOperator(task_id='evaluate_model', python_callable=evaluate_model)\n",
    "    t4 = PythonOperator(task_id='save_model', python_callable=save_model)\n",
    "\n",
    "    t1 >> t2 >> t3 >> t4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß† Explanation\n",
    "- **`@daily`** means the pipeline runs once every day.\n",
    "- Tasks are connected using `>>`, defining the execution order.\n",
    "- You can replace `print()` functions with real ML pipeline code (e.g., using scikit-learn or TensorFlow)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Airflow UI Visualization\n",
    "\n",
    "When you run the Airflow web server, your DAG will appear as a flow chart where you can:\n",
    "- Manually trigger DAG runs\n",
    "- Check task logs\n",
    "- Monitor success/failure\n",
    "- View dependency graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Integrating with MLflow or FastAPI\n",
    "\n",
    "You can enhance automation by:\n",
    "- Using Airflow to trigger **MLflow experiments**.\n",
    "- Deploying trained models automatically to **FastAPI endpoints**.\n",
    "- Sending performance reports via **SlackOperator** or **EmailOperator**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Best Practices for ML Pipelines in Airflow\n",
    "\n",
    "‚úÖ Keep tasks modular ‚Äî one logical operation per task.\n",
    "\n",
    "‚úÖ Store intermediate data in S3, GCS, or local storage.\n",
    "\n",
    "‚úÖ Use XComs for small data passing between tasks.\n",
    "\n",
    "‚úÖ Log metrics and artifacts using MLflow.\n",
    "\n",
    "‚úÖ Use Dockerized Airflow setups for reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Summary\n",
    "- Airflow automates and orchestrates ML pipelines.\n",
    "- DAGs define dependencies and schedules.\n",
    "- Combine Airflow with MLflow, Docker, and FastAPI for end-to-end automation.\n",
    "\n",
    "**Next Step ‚Üí** Try deploying this DAG locally and visualize it in Airflow UI!"
   ]
  }
 ]
}

