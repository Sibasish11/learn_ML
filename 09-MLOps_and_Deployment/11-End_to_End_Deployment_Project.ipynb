{
 "nbformat": 5,
 "nbformat_minor": 10,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End Deployment Project\n",
    "\n",
    "This notebook walks through a compact **end-to-end ML deployment project** ‚Äî from data ‚Üí training ‚Üí packaging ‚Üí API ‚Üí Docker ‚Üí CI ‚Üí monitoring. It's a template you can copy into a real repo and adapt for larger projects.\n",
    "\n",
    "The example uses the **Iris** dataset and demonstrates pragmatic steps and files you would add to a GitHub repository for productionizing a small model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Objectives\n",
    "\n",
    "- Train a simple model and save it as an artifact.\n",
    "- Create a lightweight FastAPI app to serve predictions.\n",
    "- Provide a `Dockerfile` to containerize the API.\n",
    "- Show a sample GitHub Actions workflow for CI (train + test + build image).\n",
    "- Outline basic monitoring and rollback recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Project structure (suggested)\n",
    "\n",
    "```\n",
    "end-to-end-deploy/\n",
    "‚îú‚îÄ‚îÄ data/                # optional: raw or sample data\n",
    "‚îú‚îÄ‚îÄ model/               # saved model artifacts\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ iris_model.joblib\n",
    "‚îú‚îÄ‚îÄ api/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ app.py           # FastAPI app\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt\n",
    "‚îú‚îÄ‚îÄ Dockerfile\n",
    "‚îú‚îÄ‚îÄ .github/workflows/ci-cd.yml\n",
    "‚îî‚îÄ‚îÄ notebooks/\n",
    "    ‚îî‚îÄ‚îÄ 11-End_to_End_Deployment_Project.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 ‚Äî Train & save a simple model (Iris)\n",
    "This code trains a RandomForest and saves it to `model/iris_model.joblib`. Run locally or as part of CI to produce the artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "os.makedirs('model', exist_ok=True)\n",
    "\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "preds = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, preds)\n",
    "print(f\"Trained RandomForest ‚Äî test accuracy: {acc:.4f}\")\n",
    "\n",
    "# Save model artifact\n",
    "joblib.dump(clf, 'model/iris_model.joblib')\n",
    "print('Model saved to model/iris_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 ‚Äî FastAPI app (`api/app.py`)\n",
    "\n",
    "Below is a minimal FastAPI application to load the saved model and expose a `/predict` endpoint. Save as `api/app.py` in your repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile api/app.py\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import joblib\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "MODEL_PATH = os.path.join(os.path.dirname(__file__), '..', 'model', 'iris_model.joblib')\n",
    "MODEL_PATH = os.path.abspath(MODEL_PATH)\n",
    "\n",
    "app = FastAPI(title='Iris Classifier')\n",
    "\n",
    "class Features(BaseModel):\n",
    "    features: list\n",
    "\n",
    "@app.on_event('startup')\n",
    "def load_model():\n",
    "    global model\n",
    "    model = joblib.load(MODEL_PATH)\n",
    "\n",
    "@app.get('/')\n",
    "def root():\n",
    "    return {'message': 'Iris classifier up. POST to /predict'}\n",
    "\n",
    "@app.post('/predict')\n",
    "def predict(data: Features):\n",
    "    arr = np.array(data.features).reshape(1, -1)\n",
    "    pred = model.predict(arr)\n",
    "    return {'prediction': int(pred[0])}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also add a simple `api/requirements.txt`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile api/requirements.txt\n",
    "fastapi\n",
    "uvicorn[standard]\n",
    "joblib\n",
    "scikit-learn\n",
    "numpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 ‚Äî Dockerfile (root)\n",
    "\n",
    "A concise Dockerfile to build the API image. Save as `Dockerfile` at repo root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile\n",
    "FROM python:3.10-slim\n",
    "WORKDIR /app\n",
    "\n",
    "COPY api/ ./api/\n",
    "COPY model/ ./model/\n",
    "COPY --chown=root:root api/requirements.txt ./requirements.txt\n",
    "\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "EXPOSE 8000\n",
    "CMD [\"uvicorn\", \"api.app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 ‚Äî GitHub Actions (CI) ‚Äî `.github/workflows/ci-cd.yml`\n",
    "\n",
    "Sample pipeline that trains model, runs tests, builds Docker image (simulated), and pushes when on `main`.\n",
    "Place this YAML at `.github/workflows/ci-cd.yml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile .github/workflows/ci-cd.yml\n",
    "name: CI-CD pipeline\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    branches: [ main ]\n",
    "  pull_request:\n",
    "    branches: [ main ]\n",
    "\n",
    "jobs:\n",
    "  build-and-test:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v3\n",
    "      - name: Set up Python\n",
    "        uses: actions/setup-python@v4\n",
    "        with:\n",
    "          python-version: '3.10'\n",
    "      - name: Install deps\n",
    "        run: |\n",
    "          pip install -r api/requirements.txt\n",
    "      - name: Train model (artifact)\n",
    "        run: |\n",
    "          python -c \"from sklearn.datasets import load_iris; from sklearn.ensemble import RandomForestClassifier; import joblib; X,y=load_iris(return_X_y=True); clf=RandomForestClassifier(); clf.fit(X,y); joblib.dump(clf,'model/iris_model.joblib')\"\n",
    "      - name: Basic smoke test\n",
    "        run: |\n",
    "          python -c \"import joblib; m=joblib.load('model/iris_model.joblib'); import numpy as np; print(m.predict(np.array([ [5.1,3.5,1.4,0.2] ])))\"\n",
    "\n",
    "  build-image:\n",
    "    needs: build-and-test\n",
    "    runs-on: ubuntu-latest\n",
    "    if: github.ref == 'refs/heads/main'\n",
    "    steps:\n",
    "      - uses: actions/checkout@v3\n",
    "      - name: Build Docker image\n",
    "        run: echo 'Build Docker image step (configure with DockerHub / ECR credentials)'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 ‚Äî How to run locally\n",
    "\n",
    "1. Train & save model (or use the saved artifact in `model/`).\n",
    "```bash\n",
    "python -c \"from sklearn.datasets import load_iris; from sklearn.ensemble import RandomForestClassifier; import joblib; X,y=load_iris(return_X_y=True); clf=RandomForestClassifier(); clf.fit(X,y); joblib.dump(clf,'model/iris_model.joblib')\"\n",
    "```\n",
    "2. Start API locally (inside repo root):\n",
    "```bash\n",
    "pip install -r api/requirements.txt\n",
    "uvicorn api.app:app --reload --port 8000\n",
    "```\n",
    "3. Test prediction:\n",
    "```bash\n",
    "curl -X POST 'http://127.0.0.1:8000/predict' -H 'Content-Type: application/json' -d '{\"features\":[5.1,3.5,1.4,0.2]}'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 ‚Äî Monitoring & Observability (high-level)\n",
    "\n",
    "- **Metrics to collect:** request latency, request rates, prediction distribution, input feature statistics, model accuracy (if labels available).\n",
    "- **Tools:** Prometheus + Grafana for metrics, ELK for logs, Evidently or WhyLabs for drift detection, MLflow for model registry.\n",
    "- **Alerts:** trigger retraining when model accuracy falls below threshold or PSI/KS drift exceeds threshold.\n",
    "- **Logging:** add structured logs in the API (request id, timestamp, features, prediction) and ship to a log store.\n",
    "\n",
    "Example (pseudo) ‚Äî push a metric to Prometheus client inside the API:\n",
    "```python\n",
    "from prometheus_client import Summary\n",
    "REQUEST_LATENCY = Summary('request_latency_seconds', 'Latency of prediction')\n",
    "\n",
    "@REQUEST_LATENCY.time()\n",
    "def predict(...):\n",
    "    ...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 ‚Äî Rollback & Canary strategy (notes)\n",
    "\n",
    "- **Canary deployments:** deploy new version to small % of traffic and monitor metrics before full rollout.\n",
    "- **Blue/Green:** keep previous version running and switch traffic when new version is validated.\n",
    "- **Rollback:** automate rollback if key metrics degrade beyond thresholds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Summary & Next steps\n",
    "\n",
    "- This template gives you a **reproducible, minimal** end-to-end deployment flow.\n",
    "- Next steps to harden for production:\n",
    "  - Add authentication, rate-limiting, and request validation.\n",
    "  - Store models in a model registry (MLflow) and reference by version in deploys.\n",
    "  - Implement automated retraining pipelines (Airflow / Prefect) triggered by drift.\n",
    "  - Add tests (unit, integration, load) into CI pipeline.\n",
    "\n",
    "If you want, I can now:\n",
    "1. generate all files in a downloadable ZIP (code + Dockerfile + workflow), or\n",
    "2. expand this template into a production-ready repo with Prometheus metrics and a tutorial README.\n",
    "\n",
    "Which would you like next?"
   ]
  }
 ],
 "metadata": {}
}

