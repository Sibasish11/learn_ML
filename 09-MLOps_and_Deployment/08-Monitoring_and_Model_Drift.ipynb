{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitoring and Model Drift\n",
    "\n",
    "In this notebook, we'll learn how to **monitor machine learning models** in production and detect **model drift** ‚Äî when a model's performance degrades over time due to changing data patterns.\n",
    "\n",
    "## üéØ Objectives\n",
    "- Understand the importance of ML model monitoring.\n",
    "- Learn different types of drift (data, concept, and prediction drift).\n",
    "- Implement drift detection techniques.\n",
    "- Visualize drift using Python tools.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† 1. Why Monitoring is Important\n",
    "\n",
    "Monitoring ensures that your deployed ML model remains **accurate, reliable, and consistent** over time.\n",
    "\n",
    "Without monitoring:\n",
    "- Model accuracy can degrade silently.\n",
    "- Predictions may become biased or irrelevant.\n",
    "- Business decisions might rely on outdated insights.\n",
    "\n",
    "### Key Monitoring Metrics\n",
    "- **Performance Metrics:** Accuracy, Precision, Recall, F1-score.\n",
    "- **Data Drift:** Input data distribution changes.\n",
    "- **Concept Drift:** Relationship between inputs and outputs changes.\n",
    "- **Prediction Drift:** Shift in model predictions distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß© 2. Setup ‚Äî Simulating an ML Model\n",
    "\n",
    "We‚Äôll train a simple classification model and then simulate new incoming data that has drifted from the original distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Generate initial training data\n",
    "X_train, y_train = make_classification(n_samples=1000, n_features=5, random_state=42)\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Generate new (drifted) data\n",
    "X_new, y_new = make_classification(n_samples=500, n_features=5, shift=1.5, random_state=99)\n",
    "\n",
    "# Predictions and accuracy\n",
    "y_pred = model.predict(X_new)\n",
    "acc = accuracy_score(y_new, y_pred)\n",
    "print(f\"Model accuracy on new (possibly drifted) data: {acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 3. Visualizing Data Drift\n",
    "\n",
    "We'll compare feature distributions from training and new data to detect **data drift**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(X_train, columns=[f'feature_{i}' for i in range(5)])\n",
    "new_df = pd.DataFrame(X_new, columns=[f'feature_{i}' for i in range(5)])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "sns.kdeplot(train_df['feature_0'], fill=True, ax=axes[0], label='Train')\n",
    "sns.kdeplot(new_df['feature_0'], fill=True, ax=axes[0], label='New')\n",
    "axes[0].set_title('Feature 0 Distribution Drift')\n",
    "axes[0].legend()\n",
    "\n",
    "sns.kdeplot(train_df['feature_1'], fill=True, ax=axes[1], label='Train')\n",
    "sns.kdeplot(new_df['feature_1'], fill=True, ax=axes[1], label='New')\n",
    "axes[1].set_title('Feature 1 Distribution Drift')\n",
    "axes[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see how the distributions for some features may have **shifted** ‚Äî indicating possible drift in the data source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìè 4. Quantifying Drift ‚Äî Population Stability Index (PSI)\n",
    "\n",
    "PSI measures how much the distribution of a variable has changed over time.\n",
    "\n",
    "Typical PSI interpretation:\n",
    "- **< 0.1:** No significant drift.\n",
    "- **0.1 ‚Äì 0.25:** Moderate drift.\n",
    "- **> 0.25:** Significant drift detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_psi(expected, actual, buckets=10):\n",
    "    def scale_range(input, min_val, max_val):\n",
    "        input_std = (input - input.min()) / (input.max() - input.min())\n",
    "        input_scaled = input_std * (max_val - min_val) + min_val\n",
    "        return input_scaled\n",
    "    \n",
    "    breakpoints = np.arange(0, buckets + 1) / buckets\n",
    "    breakpoints = scale_range(expected.rank(pct=True), 0, 1).quantile(breakpoints)\n",
    "    expected_percents = np.histogram(expected, bins=breakpoints)[0] / len(expected)\n",
    "    actual_percents = np.histogram(actual, bins=breakpoints)[0] / len(actual)\n",
    "    psi_value = np.sum((expected_percents - actual_percents) * np.log(expected_percents / actual_percents))\n",
    "    return psi_value\n",
    "\n",
    "psi_values = {f'feature_{i}': calculate_psi(train_df[f'feature_{i}'], new_df[f'feature_{i}']) for i in range(5)}\n",
    "psi_df = pd.DataFrame(list(psi_values.items()), columns=['Feature', 'PSI'])\n",
    "psi_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If PSI > 0.25 for any feature, it‚Äôs a clear sign that the input distribution has **drifted significantly** from the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ 5. Monitoring Prediction Drift\n",
    "\n",
    "We can track changes in **model prediction probabilities** or **label distributions** to identify drift in predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = model.predict(X_train)\n",
    "new_preds = model.predict(X_new)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.kdeplot(train_preds, label='Train Predictions', fill=True)\n",
    "sns.kdeplot(new_preds, label='New Predictions', fill=True)\n",
    "plt.title('Prediction Distribution Drift')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the model's prediction distribution shifts dramatically, it might indicate **concept drift** ‚Äî the relationship between input features and output labels has changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß© 6. Automating Drift Monitoring\n",
    "\n",
    "In real-world scenarios, drift monitoring is automated using tools such as:\n",
    "- **Evidently AI** ‚Äì Open-source library for model monitoring.\n",
    "- **WhyLabs** ‚Äì Data logging and drift alerts.\n",
    "- **Prometheus + Grafana** ‚Äì Metric visualization and alerting.\n",
    "- **MLflow / Neptune.ai** ‚Äì Tracking model metrics over time.\n",
    "\n",
    "Example using `evidently`:\n",
    "\n",
    "```python\n",
    "from evidently.report import Report\n",
    "from evidently.metrics import DataDriftPreset\n",
    "\n",
    "report = Report(metrics=[DataDriftPreset()])\n",
    "report.run(reference_data=train_df, current_data=new_df)\n",
    "report.show(mode='inline')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 7. Model Retraining Trigger\n",
    "\n",
    "Once significant drift is detected (PSI > 0.25 or performance drop > threshold), retraining can be triggered automatically:\n",
    "\n",
    "```python\n",
    "if acc < 0.8 or any(psi_df['PSI'] > 0.25):\n",
    "    print('‚ö†Ô∏è Drift detected! Triggering model retraining pipeline...')\n",
    "else:\n",
    "    print('‚úÖ Model performance stable. No retraining needed.')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Summary\n",
    "\n",
    "- Monitoring is essential for maintaining ML model reliability.\n",
    "- We visualized and measured **data**, **concept**, and **prediction drift**.\n",
    "- PSI was used to quantify drift.\n",
    "- Automated retraining can be triggered based on thresholds.\n",
    "\n",
    "---\n",
    "Next ‚Üí **09-ML_Model_Registry_and_Versioning.ipynb** üóÇÔ∏è"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
