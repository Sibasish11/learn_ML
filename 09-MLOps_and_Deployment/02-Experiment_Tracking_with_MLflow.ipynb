{
 "nbformat": 5,
 "nbformat_minor": 10,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Tracking with MLflow\n",
    "\n",
    "In this notebook, we‚Äôll explore **experiment tracking** ‚Äî a key MLOps practice ‚Äî using **MLflow**, one of the most popular open-source tools for managing machine learning workflows.\n",
    "\n",
    "MLflow helps track experiments, log parameters and metrics, version models, and even deploy them efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Why Experiment Tracking?\n",
    "\n",
    "When building ML models, we try different algorithms, hyperparameters, and preprocessing steps. Without a systematic way to record them, it becomes difficult to:\n",
    "- Compare model versions\n",
    "- Understand why one performed better than another\n",
    "- Reproduce past results\n",
    "\n",
    "**Experiment tracking** solves this problem by automatically recording metadata for each run ‚Äî parameters, metrics, artifacts, and source code."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Install MLflow if not already installed\n",
    "# !pip install mlflow"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Setting Up MLflow\n",
    "\n",
    "MLflow can be run locally or connected to a central tracking server.\n",
    "\n",
    "By default, MLflow logs to a local directory called `mlruns/`. You can also set an experiment name."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Set experiment name\n",
    "mlflow.set_experiment(\"Iris_Classification_Experiment\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Example: Tracking a Simple Model\n",
    "\n",
    "Let‚Äôs train a simple **Logistic Regression** classifier on the **Iris dataset** and log parameters, metrics, and the model with MLflow."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load data\n",
    "iris = datasets.load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
    "\n",
    "with mlflow.start_run(run_name=\"Logistic_Regression_Run\"):\n",
    "    \n",
    "    # Model with different regularization strength\n",
    "    model = LogisticRegression(C=1.0, max_iter=200)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    preds = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    \n",
    "    # Log parameters and metrics\n",
    "    mlflow.log_param(\"C\", 1.0)\n",
    "    mlflow.log_param(\"max_iter\", 200)\n",
    "    mlflow.log_metric(\"accuracy\", acc)\n",
    "    \n",
    "    # Log model\n",
    "    mlflow.sklearn.log_model(model, \"model\")\n",
    "    \n",
    "    print(f\"Logged run with accuracy: {acc:.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÇ Viewing Results\n",
    "\n",
    "Once the experiment is logged, you can view it with:\n",
    "\n",
    "```bash\n",
    "mlflow ui\n",
    "```\n",
    "\n",
    "Then open [http://localhost:5000](http://localhost:5000) to explore runs, metrics, and artifacts visually.\n",
    "\n",
    "You‚Äôll see all your experiment runs with their corresponding hyperparameters and performance metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Logging Multiple Runs\n",
    "\n",
    "We can easily run multiple experiments with different hyperparameters and log them all for comparison."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for C in [0.1, 1.0, 10.0]:\n",
    "    with mlflow.start_run(run_name=f\"LR_C={C}\"):\n",
    "        model = LogisticRegression(C=C, max_iter=200)\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_test)\n",
    "        acc = accuracy_score(y_test, preds)\n",
    "        \n",
    "        mlflow.log_param(\"C\", C)\n",
    "        mlflow.log_metric(\"accuracy\", acc)\n",
    "        \n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "        print(f\"C={C}, Accuracy={acc:.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß© Logging Artifacts\n",
    "\n",
    "You can also log **plots, confusion matrices, or datasets** as artifacts using `mlflow.log_artifact()`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_mat = confusion_matrix(y_test, preds)\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(conf_mat, annot=True, cmap='Blues', fmt='g')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.savefig('conf_matrix.png')\n",
    "\n",
    "# Log the image as an artifact\n",
    "mlflow.log_artifact('conf_matrix.png')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ MLflow Components Overview\n",
    "\n",
    "MLflow consists of 4 key components:\n",
    "\n",
    "1. **Tracking** ‚Äì Log experiments, metrics, and artifacts.\n",
    "2. **Projects** ‚Äì Package code for reproducibility.\n",
    "3. **Models** ‚Äì Manage model versions and formats.\n",
    "4. **Model Registry** ‚Äì Central repository for managing model lifecycle (staging ‚Üí production).\n",
    "\n",
    "Together, they form a complete workflow from **experiment ‚Üí deployment**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Summary\n",
    "\n",
    "In this notebook, you learned how to:\n",
    "- Track experiments using **MLflow**\n",
    "- Log parameters, metrics, and artifacts\n",
    "- Compare multiple runs visually in MLflow UI\n",
    "- Manage and version ML models efficiently\n",
    "\n",
    "Next, we‚Äôll explore **03-Model_Registry_and_Packaging.ipynb**, where we manage model lifecycle and deployment using MLflow‚Äôs Model Registry."
   ]
  }
 ]
}
