{
 "nbformat": 5,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLOps with Kubernetes üöÄ\n",
    "\n",
    "In this notebook, we‚Äôll explore how **Kubernetes (K8s)** enables scalable, reliable, and automated deployment of machine learning systems.\n",
    "\n",
    "**You‚Äôll learn:**\n",
    "- Basics of Kubernetes for ML\n",
    "- Running model containers on K8s\n",
    "- Managing scaling, health, and updates\n",
    "- Example YAML configuration for model serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ What is Kubernetes?\n",
    "\n",
    "**Kubernetes** is an open source container orchestration platform that automates:\n",
    "- **Deployment** of applications (like ML models)\n",
    "- **Scaling** to handle varying workloads\n",
    "- **Load balancing**, **self-healing**, and **rollbacks**\n",
    "\n",
    "It‚Äôs essential for MLOps because ML models often require continuous updates, monitoring, and scaling across environments (dev ‚Üí staging ‚Üí production)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Core Kubernetes Concepts\n",
    "\n",
    "| Concept | Description |\n",
    "|----------|--------------|\n",
    "| **Pod** | Smallest deployable unit; runs one or more containers. |\n",
    "| **Deployment** | Defines desired state of Pods (e.g., how many replicas). |\n",
    "| **Service** | Provides stable access (IP/port) to Pods. |\n",
    "| **Ingress** | Manages external access (like HTTP routes). |\n",
    "| **ConfigMap / Secret** | Stores configuration and credentials. |\n",
    "| **Namespace** | Organizes multiple projects or teams. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Example: Deploying an ML Model on Kubernetes\n",
    "\n",
    "Below is an example of deploying a Dockerized **FastAPI ML model** to Kubernetes."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Example deployment YAML (save as ml-model-deployment.yaml)\n",
    "deployment_yaml = '''\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: ml-model-deployment\n",
    "spec:\n",
    "  replicas: 2\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: ml-model\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: ml-model\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: ml-model-container\n",
    "        image: your-dockerhub-username/ml-model:latest\n",
    "        ports:\n",
    "        - containerPort: 8000\n",
    "'''\n",
    "\n",
    "print(deployment_yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß© Service YAML for Model Exposure\n",
    "\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: ml-model-service\n",
    "spec:\n",
    "  selector:\n",
    "    app: ml-model\n",
    "  ports:\n",
    "    - protocol: TCP\n",
    "      port: 80\n",
    "      targetPort: 8000\n",
    "  type: LoadBalancer\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèÉ Deploy Commands\n",
    "Once your Docker image is pushed to Docker Hub:\n",
    "```bash\n",
    "kubectl apply -f ml-model-deployment.yaml\n",
    "kubectl apply -f ml-model-service.yaml\n",
    "```\n",
    "\n",
    "Check running Pods:\n",
    "```bash\n",
    "kubectl get pods\n",
    "kubectl get services\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Autoscaling in Kubernetes\n",
    "\n",
    "Use the **Horizontal Pod Autoscaler (HPA)** to automatically adjust the number of Pods based on CPU utilization.\n",
    "\n",
    "```bash\n",
    "kubectl autoscale deployment ml-model-deployment --cpu-percent=70 --min=2 --max=10\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Monitoring and Logging\n",
    "\n",
    "Integrate monitoring tools:\n",
    "- **Prometheus + Grafana** for metrics.\n",
    "- **ELK Stack (Elasticsearch, Logstash, Kibana)** for logs.\n",
    "- **Kubernetes Dashboard** for visual management.\n",
    "\n",
    "These tools help ensure that model deployments are healthy and performant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ CI/CD Integration with Kubernetes\n",
    "\n",
    "Combine Kubernetes with tools like:\n",
    "- **GitHub Actions / GitLab CI** : automate builds and tests.\n",
    "- **Argo CD or Jenkins X** : for continuous delivery to K8s.\n",
    "- **Helm Charts** : simplify deployment configurations.\n",
    "\n",
    "This ensures that new model versions can be automatically tested and deployed without downtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Summary\n",
    "\n",
    "- Kubernetes enables scalable and automated ML model deployment.\n",
    "- Use YAML manifests for deployments, services, and autoscaling.\n",
    "- Integrate with CI/CD tools for smooth MLOps.\n",
    "- Combine with Docker, Airflow, and MLflow for a full production pipeline.\n",
    "\n",
    "**Next Step ‚Üí** Try deploying your FastAPI model to a local Minikube or cloud-based Kubernetes cluster!"
   ]
  }
 ]
}

