{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dockerizing ML Models\n",
    "\n",
    "In this notebook, you'll learn how to **containerize Machine Learning models** using **Docker** ‚Äî making them portable, reproducible, and ready for deployment.\n",
    "\n",
    "## üéØ Objectives\n",
    "- Understand what Docker is and why it‚Äôs useful for ML workflows.\n",
    "- Learn how to create a Dockerfile for your model.\n",
    "- Build and run a Docker container serving your ML API.\n",
    "- Manage dependencies and environment consistency across systems.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 1. What is Docker?\n",
    "\n",
    "Docker allows you to **package applications and dependencies into containers**, which run the same way on any system.\n",
    "\n",
    "**Benefits for ML:**\n",
    "- Ensures consistent environment (no more *‚Äúit works on my machine‚Äù* issues)\n",
    "- Portable deployment on servers or cloud\n",
    "- Lightweight and faster than full virtual machines\n",
    "\n",
    "üß© Example use case: Packaging a FastAPI model server into a Docker container."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† 2. Prepare the Model and API\n",
    "\n",
    "We‚Äôll reuse the Iris classifier and FastAPI app created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "\n",
    "# Train and save the model\n",
    "iris = load_iris()\n",
    "model = RandomForestClassifier()\n",
    "model.fit(iris.data, iris.target)\n",
    "joblib.dump(model, 'iris_model.pkl')\n",
    "print('‚úÖ Model saved as iris_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let‚Äôs create our **FastAPI app** (`app.py`) that will serve predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app.py\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "model = joblib.load('iris_model.pkl')\n",
    "app = FastAPI(title='Iris Classifier API')\n",
    "\n",
    "class IrisInput(BaseModel):\n",
    "    sepal_length: float\n",
    "    sepal_width: float\n",
    "    petal_length: float\n",
    "    petal_width: float\n",
    "\n",
    "@app.get('/')\n",
    "def root():\n",
    "    return {'message': 'Welcome to the Iris Model API'}\n",
    "\n",
    "@app.post('/predict')\n",
    "def predict(data: IrisInput):\n",
    "    X = np.array([[data.sepal_length, data.sepal_width, data.petal_length, data.petal_width]])\n",
    "    pred = model.predict(X)\n",
    "    return {'prediction': int(pred[0])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üêã 3. Writing the Dockerfile\n",
    "\n",
    "A **Dockerfile** defines how to build your container image. Let‚Äôs create one that:\n",
    "- Uses a base Python image\n",
    "- Installs dependencies\n",
    "- Copies code and model\n",
    "- Runs the FastAPI app via Uvicorn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile\n",
    "FROM python:3.10\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy files\n",
    "COPY app.py /app/\n",
    "COPY iris_model.pkl /app/\n",
    "\n",
    "# Install dependencies\n",
    "RUN pip install fastapi uvicorn scikit-learn joblib\n",
    "\n",
    "# Expose port\n",
    "EXPOSE 8000\n",
    "\n",
    "# Run API\n",
    "CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî® 4. Building the Docker Image\n",
    "\n",
    "Run the following command in your terminal (in the same folder as `Dockerfile`):\n",
    "\n",
    "```bash\n",
    "docker build -t iris-ml-api .\n",
    "```\n",
    "\n",
    "This will:\n",
    "1. Download the Python base image.\n",
    "2. Copy code & model.\n",
    "3. Install dependencies.\n",
    "4. Package everything into a Docker image named **iris-ml-api**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ñ∂Ô∏è 5. Running the Container\n",
    "\n",
    "Once built, start the container using:\n",
    "```bash\n",
    "docker run -d -p 8000:8000 iris-ml-api\n",
    "```\n",
    "\n",
    "Visit:\n",
    "- Swagger UI ‚Üí http://127.0.0.1:8000/docs\n",
    "- Root endpoint ‚Üí http://127.0.0.1:8000/\n",
    "\n",
    "üéâ You now have your ML model running inside a **Docker container!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ 6. Testing the API Inside Docker\n",
    "\n",
    "You can send requests using Python or `curl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'http://127.0.0.1:8000/predict'\n",
    "sample = {\n",
    "    'sepal_length': 5.1,\n",
    "    'sepal_width': 3.5,\n",
    "    'petal_length': 1.4,\n",
    "    'petal_width': 0.2\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=sample)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚òÅÔ∏è 7. Deploying Dockerized Model to Cloud\n",
    "\n",
    "**Options:**\n",
    "- **Docker Hub** ‚Üí Push your image for reuse:\n",
    "  ```bash\n",
    "  docker tag iris-ml-api username/iris-ml-api:v1\n",
    "  docker push username/iris-ml-api:v1\n",
    "  ```\n",
    "\n",
    "- **Cloud Platforms:**\n",
    "  - AWS Elastic Beanstalk\n",
    "  - Google Cloud Run\n",
    "  - Azure Container Apps\n",
    "  - Render / Railway (supports Dockerfile-based deployments)\n",
    "\n",
    "**Kubernetes (Advanced):**\n",
    "- Once Dockerized, you can deploy the container as a Pod in a K8s cluster using a `Deployment` and `Service` YAML manifest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîí 8. Best Practices\n",
    "- Use **`.dockerignore`** to avoid copying unnecessary files.\n",
    "- Pin dependency versions for reproducibility.\n",
    "- Use multi-stage builds to reduce image size.\n",
    "- Store model weights in a versioned artifact store (e.g., DVC, MLflow).\n",
    "- Add health checks in production containers.\n",
    "\n",
    "---\n",
    "‚úÖ **You have successfully Dockerized your ML Model!**\n",
    "\n",
    "Next ‚Üí **07-CI_CD_with_GitHub_Actions.ipynb** üö¶"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

