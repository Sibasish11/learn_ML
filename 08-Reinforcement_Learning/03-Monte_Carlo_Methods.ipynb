{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Methods in Reinforcement Learning\n",
    "\n",
    "In this notebook, we explore **Monte Carlo (MC) methods**, which learn from **experience** rather than a complete model of the environment.\n",
    "\n",
    "Unlike **Dynamic Programming**, MC methods **do not require knowing transition probabilities or rewards** ‚Äî they rely on **sampling episodes** and **averaging returns** to estimate value functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "- Understand the intuition behind Monte Carlo methods.\n",
    "- Implement **Monte Carlo prediction** for estimating value functions.\n",
    "- Apply **Monte Carlo control** to learn optimal policies.\n",
    "- Distinguish between **First-Visit** and **Every-Visit** Monte Carlo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß© What are Monte Carlo Methods?\n",
    "\n",
    "Monte Carlo methods learn directly from **episodes of experience** by averaging the returns observed following visits to a state or state-action pair.\n",
    "\n",
    "Key properties:\n",
    "- Learn from **complete episodes**.\n",
    "- Require **no knowledge** of transition dynamics.\n",
    "- Use **sample averages** to estimate expected returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîπ Return Definition\n",
    "\n",
    "For an episode consisting of states, actions, and rewards:\n",
    "\n",
    "$$\n",
    "G_t = R_{t+1} + Œ≥R_{t+2} + Œ≥^2R_{t+3} + ...\n",
    "$$\n",
    "\n",
    "The **value function** is estimated as the average of observed returns following a state:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "V(s) = \\frac{1}{N(s)} \\sum_{i=1}^{N(s)} G_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Example Environment ‚Äî Simple Gridworld\n",
    "\n",
    "We'll simulate a small **gridworld** where an agent starts at a random position and receives +1 when reaching the goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "grid_size = 4\n",
    "goal_state = (3, 3)\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "gamma = 0.9\n",
    "\n",
    "def step(state, action):\n",
    "    i, j = state\n",
    "    if action == 'up': i = max(i-1, 0)\n",
    "    elif action == 'down': i = min(i+1, grid_size-1)\n",
    "    elif action == 'left': j = max(j-1, 0)\n",
    "    elif action == 'right': j = min(j+1, grid_size-1)\n",
    "    next_state = (i, j)\n",
    "    reward = 1 if next_state == goal_state else 0\n",
    "    done = next_state == goal_state\n",
    "    return next_state, reward, done\n",
    "\n",
    "def generate_episode(policy):\n",
    "    state = (np.random.randint(0, grid_size), np.random.randint(0, grid_size))\n",
    "    episode = []\n",
    "    while True:\n",
    "        action = np.random.choice(actions, p=policy[state])\n",
    "        next_state, reward, done = step(state, action)\n",
    "        episode.append((state, action, reward))\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Monte Carlo Prediction (Estimating V(s))\n",
    "\n",
    "We estimate the **state-value function** `V(s)` under a given policy by averaging returns from sampled episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_prediction(policy, episodes=500):\n",
    "    V = { (i, j): 0 for i in range(grid_size) for j in range(grid_size) }\n",
    "    returns = { (i, j): [] for i in range(grid_size) for j in range(grid_size) }\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        episode = generate_episode(policy)\n",
    "        G = 0\n",
    "        visited_states = set()\n",
    "        for t in reversed(range(len(episode))):\n",
    "            state, _, reward = episode[t]\n",
    "            G = gamma * G + reward\n",
    "            if state not in visited_states:\n",
    "                returns[state].append(G)\n",
    "                V[state] = np.mean(returns[state])\n",
    "                visited_states.add(state)\n",
    "    return V\n",
    "\n",
    "# Uniform random policy (equal chance for all actions)\n",
    "policy = { (i,j): np.ones(len(actions))/len(actions) for i in range(grid_size) for j in range(grid_size) }\n",
    "V = mc_prediction(policy)\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ The result `V` is the **estimated value** of each state under the random policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ôªÔ∏è Monte Carlo Control ‚Äî Exploring Starts\n",
    "\n",
    "Monte Carlo **control** aims to find the **optimal policy** without knowing transitions.\n",
    "\n",
    "We use **Exploring Starts (ES)** ‚Äî assuming every state-action pair can be visited with non-zero probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_control_es(episodes=5000):\n",
    "    Q = { (i,j,a): 0 for i in range(grid_size) for j in range(grid_size) for a in actions }\n",
    "    returns = { (i,j,a): [] for i in range(grid_size) for j in range(grid_size) for a in actions }\n",
    "    policy = { (i,j): np.ones(len(actions))/len(actions) for i in range(grid_size) for j in range(grid_size) }\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        state = (np.random.randint(0, grid_size), np.random.randint(0, grid_size))\n",
    "        action = np.random.choice(actions)\n",
    "        episode = []\n",
    "        next_state, reward, done = step(state, action)\n",
    "        episode.append((state, action, reward))\n",
    "        while not done:\n",
    "            action = np.random.choice(actions, p=policy[next_state])\n",
    "            new_state, reward, done = step(next_state, action)\n",
    "            episode.append((next_state, action, reward))\n",
    "            next_state = new_state\n",
    "\n",
    "        G = 0\n",
    "        visited = set()\n",
    "        for t in reversed(range(len(episode))):\n",
    "            state, action, reward = episode[t]\n",
    "            G = gamma * G + reward\n",
    "            if (state, action) not in visited:\n",
    "                returns[(state[0], state[1], action)].append(G)\n",
    "                Q[(state[0], state[1], action)] = np.mean(returns[(state[0], state[1], action)])\n",
    "                best_a = np.argmax([Q[(state[0], state[1], a)] for a in actions])\n",
    "                policy[state] = np.eye(len(actions))[best_a]\n",
    "                visited.add((state, action))\n",
    "    return policy, Q\n",
    "\n",
    "optimal_policy, Q = mc_control_es(episodes=3000)\n",
    "optimal_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ The above code demonstrates **Monte Carlo Control with Exploring Starts**, which learns an **optimal policy** purely from sampled episodes ‚Äî no environment model needed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìò Summary\n",
    "\n",
    "- Monte Carlo methods estimate value functions by averaging **sampled returns**.\n",
    "- They work **without knowing** transition probabilities.\n",
    "- **MC Prediction** estimates values for a fixed policy.\n",
    "- **MC Control (Exploring Starts)** finds optimal policies via experience.\n",
    "\n",
    "Next up: **Temporal Difference (TD) Learning** ‚Äî combining ideas from **Monte Carlo** and **Dynamic Programming** for faster, online learning!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
