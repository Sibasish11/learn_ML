{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing Atari with Deep Q Networks (DQN)\n",
    "\n",
    "In this notebook, we'll train a Deep Q-Network (DQN) agent on an **Atari game** using the Gym environment (like `Breakout-v0`).\n",
    "\n",
    "We'll:\n",
    "- Understand how to preprocess Atari frames.\n",
    "- Build a convolutional DQN architecture.\n",
    "- Train and evaluate the agent.\n",
    "- Discuss the improvements for stable learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym gym[atari] gym[accept-rom-license] torch torchvision numpy matplotlib opencv-python --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing Atari Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "    frame = cv2.resize(frame, (84, 84))\n",
    "    return frame / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define the Convolutional DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvDQN(nn.Module):\n",
    "    def __init__(self, action_size):\n",
    "        super(ConvDQN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, action_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x / 255.0\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Replay Buffer for Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=100000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    def push(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return np.stack(states), actions, rewards, np.stack(next_states), dones\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Environment and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('ALE/Breakout-v5', render_mode=None)\n",
    "action_size = env.action_space.n\n",
    "\n",
    "q_net = ConvDQN(action_size).to(device)\n",
    "target_net = ConvDQN(action_size).to(device)\n",
    "target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "optimizer = optim.Adam(q_net.parameters(), lr=1e-4)\n",
    "memory = ReplayBuffer(100000)\n",
    "\n",
    "batch_size = 32\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.1\n",
    "epsilon_decay = 0.9995\n",
    "target_update = 1000\n",
    "episodes = 200\n",
    "reward_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Stack Frames for Temporal Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "def stack_frames(state, stack):\n",
    "    stack.append(preprocess_frame(state))\n",
    "    while len(stack) < 4:\n",
    "        stack.append(stack[-1])\n",
    "    stacked_state = np.stack(stack, axis=0)\n",
    "    return stacked_state, stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Loop (Simplified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ep in range(episodes):\n",
    "    state, _ = env.reset()\n",
    "    frame_stack = deque(maxlen=4)\n",
    "    state, frame_stack = stack_frames(state, frame_stack)\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        if random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            state_t = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            q_values = q_net(state_t)\n",
    "            action = q_values.argmax().item()\n",
    "\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        next_state, frame_stack = stack_frames(next_state, frame_stack)\n",
    "        memory.push((state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        if len(memory) >= batch_size:\n",
    "            states, actions, rewards, next_states, dones = memory.sample(batch_size)\n",
    "            states = torch.FloatTensor(states).to(device)\n",
    "            next_states = torch.FloatTensor(next_states).to(device)\n",
    "            actions = torch.LongTensor(actions).unsqueeze(1).to(device)\n",
    "            rewards = torch.FloatTensor(rewards).unsqueeze(1).to(device)\n",
    "            dones = torch.FloatTensor(dones).unsqueeze(1).to(device)\n",
    "\n",
    "            q_values = q_net(states).gather(1, actions)\n",
    "            next_q = target_net(next_states).max(1)[0].unsqueeze(1)\n",
    "            target = rewards + gamma * next_q * (1 - dones)\n",
    "\n",
    "            loss = nn.SmoothL1Loss()(q_values, target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "    reward_history.append(total_reward)\n",
    "\n",
    "    if ep % 10 == 0:\n",
    "        target_net.load_state_dict(q_net.state_dict())\n",
    "        print(f\"Episode {ep}, Reward: {total_reward}, Epsilon: {epsilon:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Training Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(reward_history)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('DQN Training on Atari Breakout')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Built a convolutional **Deep Q-Network (DQN)** for Atari gameplay.\n",
    "- Applied **frame stacking** and **experience replay** for stable learning.\n",
    "- Trained the model using **epsilon-greedy** exploration.\n",
    "\n",
    "This is the foundation for more advanced algorithms like **Double DQN**, **Dueling DQN**, and **Rainbow DQN**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
