{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Actor Critic Methods: A3C and A2C\n",
    "\n",
    "In this notebook, we explore two advanced policy-gradient algorithms in reinforcement learning : **Asynchronous Advantage Actor-Critic (A3C)** and **Advantage Actor-Critic (A2C)**.\n",
    "\n",
    "These algorithms improve upon the basic Actor-Critic method by introducing parallelization, stability, and more efficient policy updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Actor Critic methods combine the benefits of both policy based and value based learning. The **actor** learns the policy, while the **critic** estimates the value function.\n",
    "\n",
    "- **A3C (Asynchronous Advantage Actor-Critic):** Runs multiple agents in parallel on different threads, each interacting with its own copy of the environment.\n",
    "- **A2C (Advantage Actor-Critic):** A synchronous version of A3C that averages gradients from multiple agents before updating the global model.\n",
    "\n",
    "Both use the *advantage function* to stabilize training:\n",
    "\n",
    "$$ A(s, a) = R_t - V(s_t) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "num_actions = env.action_space.n\n",
    "obs_space = env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building the Actor-Critic Network\n",
    "\n",
    "We’ll define a shared network backbone with two heads:\n",
    "- **Actor head** for policy (action probabilities)\n",
    "- **Critic head** for state value estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_actor_critic(num_actions):\n",
    "    inputs = layers.Input(shape=(obs_space,))\n",
    "    common = layers.Dense(128, activation='relu')(inputs)\n",
    "    actor = layers.Dense(num_actions, activation='softmax')(common)\n",
    "    critic = layers.Dense(1)(common)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=[actor, critic])\n",
    "    return model\n",
    "\n",
    "model = build_actor_critic(num_actions)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advantage Calculation\n",
    "We compute the *advantage* to reduce variance in policy gradient updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_advantage(rewards, values, gamma=0.99):\n",
    "    returns = []\n",
    "    discounted_sum = 0\n",
    "    for r in rewards[::-1]:\n",
    "        discounted_sum = r + gamma * discounted_sum\n",
    "        returns.insert(0, discounted_sum)\n",
    "    returns = np.array(returns)\n",
    "    advantage = returns - np.array(values)\n",
    "    return returns, advantage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. A2C Training Loop (Simplified Example)\n",
    "This example uses a synchronous setup similar to A2C, collecting trajectories before applying updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "def train_a2c(model, env, episodes=500, gamma=0.99):\n",
    "    for ep in range(episodes):\n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "        states, actions, rewards, values = [], [], [], []\n",
    "\n",
    "        while not done:\n",
    "            state_tensor = tf.convert_to_tensor(state[None, :], dtype=tf.float32)\n",
    "            probs, value = model(state_tensor)\n",
    "            action = np.random.choice(num_actions, p=np.squeeze(probs))\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            values.append(value.numpy()[0, 0])\n",
    "            state = next_state\n",
    "\n",
    "        returns, advantage = compute_advantage(rewards, values)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            actor_probs, critic_values = model(tf.convert_to_tensor(np.array(states), dtype=tf.float32))\n",
    "            critic_loss = tf.keras.losses.MSE(returns, tf.squeeze(critic_values))\n",
    "\n",
    "            action_masks = tf.one_hot(actions, num_actions)\n",
    "            log_probs = tf.reduce_sum(action_masks * tf.math.log(actor_probs + 1e-8), axis=1)\n",
    "            actor_loss = -tf.reduce_mean(log_probs * advantage)\n",
    "            total_loss = actor_loss + critic_loss\n",
    "\n",
    "        grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        if ep % 10 == 0:\n",
    "            print(f\"Episode {ep}, Total Reward: {np.sum(rewards):.2f}\")\n",
    "\n",
    "# Uncomment to train\n",
    "# train_a2c(model, env, episodes=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. A3C Overview\n",
    "A3C is similar in logic but uses **asynchronous parallel agents** — each running in its own thread and sending gradients to a global network.\n",
    "\n",
    "**Advantages of A3C:**\n",
    "- Faster training due to parallelism\n",
    "- Better exploration\n",
    "- Reduced correlation in updates\n",
    "\n",
    "Implementation typically involves Python’s `threading` or `multiprocessing` along with shared global models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary\n",
    "- A2C is the synchronous version of A3C.\n",
    "- Both use the advantage function for stability.\n",
    "- Actor-Critic methods balance between exploration and exploitation.\n",
    "- A3C improves performance using asynchronous updates from multiple agents.\n",
    "\n",
    "**Next Step:** Try implementing A3C using multiprocessing and compare it with the synchronous A2C."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

