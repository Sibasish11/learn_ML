{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming in Reinforcement Learning\n",
    "\n",
    "In this notebook, we introduce **Dynamic Programming (DP)** : a class of algorithms used to solve **Markov Decision Processes (MDPs)** when the environment model (transitions and rewards) is **fully known**.\n",
    "\n",
    "DP provides foundational techniques for computing **optimal policies** and **value functions**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "- Understand the **core idea** of Dynamic Programming in RL.\n",
    "- Learn **policy evaluation**, **policy improvement**, and **policy iteration**.\n",
    "- Implement **value iteration** algorithmically.\n",
    "- Visualize how DP converges to an optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔹 What is Dynamic Programming?\n",
    "\n",
    "Dynamic Programming (DP) is a method for solving complex problems by breaking them down into **simpler subproblems**.\n",
    "\n",
    "In RL, DP helps us compute **optimal value functions** and **policies** for an MDP when we know:\n",
    "\n",
    "- Transition probabilities `P(s'|s,a)`\n",
    "- Reward function `R(s,a)`\n",
    "\n",
    "It relies on **Bellman Equations** to update values iteratively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚖️ The Bellman Expectation Equation\n",
    "\n",
    "For a policy `π`, the value of a state `s` is given by:\n",
    "\n",
    "$$\n",
    "V_π(s) = \\sum_a π(a|s) \\sum_{s'} P(s'|s,a) [R(s,a,s') + γ V_π(s')]\n",
    "$$\n",
    "\n",
    "This recursive equation expresses **how good** a state is under a given policy: it depends on expected rewards and future values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧩 Example: 3-State MDP\n",
    "\n",
    "Let's consider a simple example:\n",
    "\n",
    "- **States**: S1, S2, S3 (S3 is terminal)\n",
    "- **Actions**: Left, Right\n",
    "- **Rewards**: +1 when reaching terminal, 0 otherwise\n",
    "- **Discount factor (γ)**: 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# States: 0=S1, 1=S2, 2=S3\n",
    "n_states = 3\n",
    "gamma = 0.9\n",
    "\n",
    "# Transition probabilities (deterministic for simplicity)\n",
    "P = {\n",
    "    0: {0: [(1.0, 1, 0)], 1: [(1.0, 2, 1)]},  # from S1, Left->S2, Right->S3\n",
    "    1: {0: [(1.0, 2, 1)], 1: [(1.0, 0, 0)]},  # from S2, Left->S3, Right->S1\n",
    "    2: {0: [(1.0, 2, 0)], 1: [(1.0, 2, 0)]}   # terminal S3\n",
    "}\n",
    "\n",
    "# Initialize value function\n",
    "V = np.zeros(n_states)\n",
    "\n",
    "def policy_evaluation(policy, V, theta=1e-4):\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(n_states):\n",
    "            v = 0\n",
    "            for a, action_prob in enumerate(policy[s]):\n",
    "                for prob, next_state, reward in P[s][a]:\n",
    "                    v += action_prob * prob * (reward + gamma * V[next_state])\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "            V[s] = v\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "# Define uniform random policy\n",
    "policy = np.ones((n_states, 2)) / 2\n",
    "V = policy_evaluation(policy, V.copy())\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple **policy evaluation** estimates the expected value of each state under a uniform random policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ♻️ Policy Iteration\n",
    "\n",
    "**Policy Iteration** alternates between:\n",
    "\n",
    "1. **Policy Evaluation** : Compute `Vπ(s)` for the current policy.\n",
    "2. **Policy Improvement** : Update policy to choose the best action:\n",
    "\n",
    "$$\n",
    "π'(s) = argmax_a \\sum_{s'} P(s'|s,a)[R(s,a,s') + γV(s')]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_lookahead(s, V):\n",
    "    A = np.zeros(2)\n",
    "    for a in range(2):\n",
    "        for prob, next_state, reward in P[s][a]:\n",
    "            A[a] += prob * (reward + gamma * V[next_state])\n",
    "    return A\n",
    "\n",
    "def policy_iteration():\n",
    "    policy = np.ones((n_states, 2)) / 2\n",
    "    V = np.zeros(n_states)\n",
    "    stable = False\n",
    "\n",
    "    while not stable:\n",
    "        V = policy_evaluation(policy, V.copy())\n",
    "        stable = True\n",
    "\n",
    "        for s in range(n_states):\n",
    "            old_action = np.argmax(policy[s])\n",
    "            action_values = one_step_lookahead(s, V)\n",
    "            best_action = np.argmax(action_values)\n",
    "            if old_action != best_action:\n",
    "                stable = False\n",
    "            policy[s] = np.eye(2)[best_action]\n",
    "\n",
    "    return policy, V\n",
    "\n",
    "optimal_policy, optimal_values = policy_iteration()\n",
    "optimal_policy, optimal_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm converges when the policy stops changing. The resulting **optimal policy** and **value function** represent the best way to act in the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚡ Value Iteration\n",
    "\n",
    "Value Iteration combines **evaluation** and **improvement** into a single update:\n",
    "\n",
    "$$\n",
    "V(s) \\leftarrow \\max_a \\sum_{s'} P(s'|s,a)[R(s,a,s') + γV(s')]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(theta=1e-4):\n",
    "    V = np.zeros(n_states)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(n_states):\n",
    "            v = V[s]\n",
    "            action_values = one_step_lookahead(s, V)\n",
    "            V[s] = np.max(action_values)\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    policy = np.zeros((n_states, 2))\n",
    "    for s in range(n_states):\n",
    "        best_action = np.argmax(one_step_lookahead(s, V))\n",
    "        policy[s][best_action] = 1.0\n",
    "    return policy, V\n",
    "\n",
    "v_policy, v_values = value_iteration()\n",
    "v_policy, v_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ **Value Iteration** directly computes the optimal value function and corresponding policy, usually faster than full policy iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📘 Summary\n",
    "\n",
    "- **Dynamic Programming** requires a known model of the environment.\n",
    "- **Policy Evaluation** estimates how good a policy is.\n",
    "- **Policy Iteration** alternates between evaluation and improvement.\n",
    "- **Value Iteration** merges both into one step for efficiency.\n",
    "\n",
    "Next, we’ll explore **Model-Free Reinforcement Learning** : where the agent learns from experience without knowing the environment model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
