{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning for Continuous Control\n",
    "\n",
    "In many real-world problems, the **action space is continuous** — e.g., steering angles, joint torques, or throttle values. Unlike discrete actions (like left, right, jump), continuous control problems require **policy gradient methods** or **actor-critic algorithms** that can output continuous values.\n",
    "\n",
    "In this notebook, we’ll explore continuous control using algorithms such as **Deep Deterministic Policy Gradient (DDPG)** on environments like `Pendulum-v1` or `LunarLanderContinuous-v2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gymnasium torch numpy matplotlib --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment Setup\n",
    "We'll use the **Pendulum-v1** environment from Gym — a classic continuous control benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v1')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "action_limit = env.action_space.high[0]\n",
    "\n",
    "print(f\"State dim: {state_dim}, Action dim: {action_dim}, Action limit: {action_limit}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Actor and Critic Networks\n",
    "The **actor** outputs continuous actions; the **critic** estimates Q-values for (state, action) pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, action_limit):\n",
    "        super(Actor, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 256), nn.ReLU(),\n",
    "            nn.Linear(256, action_dim), nn.Tanh()\n",
    "        )\n",
    "        self.action_limit = action_limit\n",
    "    \n",
    "    def forward(self, state):\n",
    "        return self.action_limit * self.net(state)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=1)\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=100000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    def push(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return np.stack(states), np.stack(actions), rewards, np.stack(next_states), dones\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize Networks and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = Actor(state_dim, action_dim, action_limit).to(device)\n",
    "critic = Critic(state_dim, action_dim).to(device)\n",
    "target_actor = Actor(state_dim, action_dim, action_limit).to(device)\n",
    "target_critic = Critic(state_dim, action_dim).to(device)\n",
    "\n",
    "target_actor.load_state_dict(actor.state_dict())\n",
    "target_critic.load_state_dict(critic.state_dict())\n",
    "\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=1e-4)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=1e-3)\n",
    "\n",
    "buffer = ReplayBuffer(100000)\n",
    "\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "tau = 0.005\n",
    "noise_std = 0.1\n",
    "episodes = 200\n",
    "reward_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Utility Functions for Exploration and Soft Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(action, noise_std):\n",
    "    noise = np.random.normal(0, noise_std, size=action.shape)\n",
    "    return np.clip(action + noise, -action_limit, action_limit)\n",
    "\n",
    "def soft_update(target_net, source_net, tau):\n",
    "    for target_param, param in zip(target_net.parameters(), source_net.parameters()):\n",
    "        target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. DDPG Training Loop (Simplified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ep in range(episodes):\n",
    "    state, _ = env.reset()\n",
    "    ep_reward = 0\n",
    "\n",
    "    for t in range(200):\n",
    "        state_t = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        action = actor(state_t).cpu().data.numpy().flatten()\n",
    "        action = add_noise(action, noise_std)\n",
    "\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        buffer.push((state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "        ep_reward += reward\n",
    "\n",
    "        if len(buffer) > batch_size:\n",
    "            states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
    "\n",
    "            states = torch.FloatTensor(states).to(device)\n",
    "            actions = torch.FloatTensor(actions).to(device)\n",
    "            rewards = torch.FloatTensor(rewards).unsqueeze(1).to(device)\n",
    "            next_states = torch.FloatTensor(next_states).to(device)\n",
    "            dones = torch.FloatTensor(dones).unsqueeze(1).to(device)\n",
    "\n",
    "            # Critic update\n",
    "            next_actions = target_actor(next_states)\n",
    "            target_q = target_critic(next_states, next_actions)\n",
    "            y = rewards + gamma * target_q * (1 - dones)\n",
    "            critic_loss = nn.MSELoss()(critic(states, actions), y.detach())\n",
    "            critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            critic_optimizer.step()\n",
    "\n",
    "            # Actor update\n",
    "            actor_loss = -critic(states, actor(states)).mean()\n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_optimizer.step()\n",
    "\n",
    "            soft_update(target_actor, actor, tau)\n",
    "            soft_update(target_critic, critic, tau)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    reward_history.append(ep_reward)\n",
    "    if ep % 10 == 0:\n",
    "        print(f\"Episode {ep}, Reward: {ep_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(reward_history)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('DDPG Training on Continuous Control Environment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Used **DDPG** (Deep Deterministic Policy Gradient) for continuous action control.\n",
    "- Learned deterministic policies using **actor-critic** networks.\n",
    "- Introduced **soft updates**, **exploration noise**, and **replay buffer**.\n",
    "\n",
    "Next, we can extend this into more advanced methods like **Twin Delayed DDPG (TD3)** or **Soft Actor-Critic (SAC)** for improved stability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
