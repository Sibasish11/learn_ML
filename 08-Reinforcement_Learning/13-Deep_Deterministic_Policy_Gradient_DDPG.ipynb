{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Deterministic Policy Gradient (DDPG)\n",
    "\n",
    "In this notebook, we explore **Deep Deterministic Policy Gradient (DDPG)** : an advanced reinforcement learning algorithm designed for **continuous action spaces**.\n",
    "\n",
    "DDPG is an **off-policy, model-free, actor-critic** algorithm that combines ideas from **DQN** and **Policy Gradient** methods.\n",
    "\n",
    "Introduced by *Lillicrap et al. (2015)*, itâ€™s particularly suited for control problems like robotic arm manipulation or self driving steering control."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Key Concepts\n",
    "\n",
    "DDPG extends the Deep Q-Network (DQN) to continuous action spaces by:\n",
    "\n",
    "- Using a **deterministic policy** (actor network) to output continuous actions.\n",
    "- Using a **critic network** to estimate Q values.\n",
    "- Employing **target networks** for both actor and critic to stabilize learning.\n",
    "- Using a **replay buffer** to store and sample experiences for training.\n",
    "\n",
    "**Update equations:**\n",
    "\n",
    "$$ y_i = r_i + \\gamma Q'(s_{i+1}, \\mu'(s_{i+1})) $$\n",
    "\n",
    "$$ \\nabla_{\\theta^\\mu} J \\approx \\frac{1}{N} \\sum_i \\nabla_a Q(s, a|\\theta^Q) \\nabla_{\\theta^\\mu} \\mu(s|\\theta^\\mu) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import random\n",
    "\n",
    "env = gym.make('Pendulum-v1')\n",
    "\n",
    "num_states = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.shape[0]\n",
    "upper_bound = env.action_space.high[0]\n",
    "lower_bound = env.action_space.low[0]\n",
    "\n",
    "print(f\"State dim: {num_states}, Action dim: {num_actions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build Actor and Critic Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actor():\n",
    "    inputs = layers.Input(shape=(num_states,))\n",
    "    out = layers.Dense(256, activation='relu')(inputs)\n",
    "    out = layers.Dense(256, activation='relu')(out)\n",
    "    outputs = layers.Dense(num_actions, activation='tanh')(out)\n",
    "    outputs = outputs * upper_bound\n",
    "    return tf.keras.Model(inputs, outputs)\n",
    "\n",
    "def get_critic():\n",
    "    state_input = layers.Input(shape=(num_states,))\n",
    "    state_out = layers.Dense(16, activation='relu')(state_input)\n",
    "    state_out = layers.Dense(32, activation='relu')(state_out)\n",
    "\n",
    "    action_input = layers.Input(shape=(num_actions,))\n",
    "    action_out = layers.Dense(32, activation='relu')(action_input)\n",
    "\n",
    "    concat = layers.Concatenate()([state_out, action_out])\n",
    "    out = layers.Dense(256, activation='relu')(concat)\n",
    "    out = layers.Dense(256, activation='relu')(out)\n",
    "    outputs = layers.Dense(1)(out)\n",
    "    return tf.keras.Model([state_input, action_input], outputs)\n",
    "\n",
    "actor_model = get_actor()\n",
    "critic_model = get_critic()\n",
    "\n",
    "target_actor = get_actor()\n",
    "target_critic = get_critic()\n",
    "\n",
    "target_actor.set_weights(actor_model.get_weights())\n",
    "target_critic.set_weights(critic_model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Replay Buffer for Experience Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_capacity=100000, batch_size=64):\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "\n",
    "    def record(self, obs_tuple):\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "        self.buffer_counter += 1\n",
    "\n",
    "    def sample(self):\n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "        return state_batch, action_batch, reward_batch, next_state_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_lr = 0.002\n",
    "actor_lr = 0.001\n",
    "gamma = 0.99\n",
    "tau = 0.005\n",
    "\n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "buffer = ReplayBuffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target(target_weights, weights, tau):\n",
    "    for (a, b) in zip(target_weights, weights):\n",
    "        a.assign(b * tau + a * (1 - tau))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def update(state_batch, action_batch, reward_batch, next_state_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        target_actions = target_actor(next_state_batch, training=True)\n",
    "        y = reward_batch + gamma * target_critic([next_state_batch, target_actions], training=True)\n",
    "        critic_value = critic_model([state_batch, action_batch], training=True)\n",
    "        critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
    "    critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)\n",
    "    critic_optimizer.apply_gradients(zip(critic_grad, critic_model.trainable_variables))\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        actions = actor_model(state_batch, training=True)\n",
    "        critic_value = critic_model([state_batch, actions], training=True)\n",
    "        actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "    actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables)\n",
    "    actor_optimizer.apply_gradients(zip(actor_grad, actor_model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. DDPG Training Loop (Simplified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state, noise_object):\n",
    "    sampled_actions = tf.squeeze(actor_model(state))\n",
    "    noise = noise_object()\n",
    "    sampled_actions = sampled_actions.numpy() + noise\n",
    "    legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
    "    return [np.squeeze(legal_action)]\n",
    "\n",
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation):\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.reset()\n",
    "    def __call__(self):\n",
    "        x = self.x_prev + self.theta * (self.mean - self.x_prev) + self.std_dev * np.random.normal(size=self.mean.shape)\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "    def reset(self):\n",
    "        self.x_prev = np.zeros_like(self.mean)\n",
    "    theta = 0.15\n",
    "\n",
    "noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(0.2) * np.ones(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep_reward_list = []\n",
    "\n",
    "for ep in range(5):  # small number for demo\n",
    "    prev_state = env.reset()[0]\n",
    "    episodic_reward = 0\n",
    "\n",
    "    while True:\n",
    "        tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
    "        action = policy(tf_prev_state, noise)\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        buffer.record((prev_state, action, reward, state))\n",
    "        episodic_reward += reward\n",
    "\n",
    "        state_batch, action_batch, reward_batch, next_state_batch = buffer.sample()\n",
    "        update(state_batch, action_batch, reward_batch, next_state_batch)\n",
    "        update_target(target_actor.variables, actor_model.variables, tau)\n",
    "        update_target(target_critic.variables, critic_model.variables, tau)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "        prev_state = state\n",
    "\n",
    "    ep_reward_list.append(episodic_reward)\n",
    "    print(f\"Episode {ep}, Reward: {episodic_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary\n",
    "\n",
    "- DDPG works well for **continuous action spaces**.\n",
    "- Combines **actor-critic structure** with **replay buffer** and **target networks**.\n",
    "- Uses **Ornstein-Uhlenbeck noise** for exploration.\n",
    "\n",
    "**Next:** You can explore extensions like **TD3** or **SAC** for improved stability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
