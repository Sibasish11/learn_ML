{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Processes (MDPs)\n",
    "\n",
    "In this notebook, we'll explore **Markov Decision Processes (MDPs)** : the mathematical framework that forms the foundation of **Reinforcement Learning (RL)**.\n",
    "\n",
    "An MDP provides a formal way to describe an environment in terms of **states, actions, rewards, and transitions**. It helps an RL agent decide the best sequence of actions to maximize cumulative rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "- Understand the **components** of an MDP.\n",
    "- Define the **transition and reward functions**.\n",
    "- Represent an **MDP as a mathematical model**.\n",
    "- Compute **returns** and understand **Markov property**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¹ What is a Markov Decision Process?\n",
    "\n",
    "A **Markov Decision Process (MDP)** is defined by a tuple:\n",
    "\n",
    "$$\n",
    "MDP = (S, A, P, R, \\gamma)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- **S**: Set of possible states.\n",
    "- **A**: Set of possible actions.\n",
    "- **P(s' | s, a)**: Transition probability : the probability of moving from state `s` to `s'` after action `a`.\n",
    "- **R(s, a)**: Reward received after performing action `a` in state `s`.\n",
    "- **Î³ (gamma)**: Discount factor (0 â‰¤ Î³ â‰¤ 1), determines how much future rewards are valued."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”„ The Markov Property\n",
    "\n",
    "The process is **Markovian** if the next state depends **only on the current state and action**, not on any previous states.\n",
    "\n",
    "$$\n",
    "P(s_{t+1} | s_t, a_t, s_{t-1}, a_{t-1}, ...) = P(s_{t+1} | s_t, a_t)\n",
    "$$\n",
    "\n",
    "This simplifies the modeling of environments and is the key assumption behind RL algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§® Example: Simple Grid World\n",
    "\n",
    "Let's define a simple grid world where an agent can move **up, down, left, or right**. The goal is to reach the terminal state `G` while avoiding obstacles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define states and actions\n",
    "states = ['S', 'A', 'B', 'G']  # S=start, G=goal\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "\n",
    "# Define transitions and rewards (simplified)\n",
    "transition_prob = {\n",
    "    ('S', 'right'): 'A',\n",
    "    ('A', 'right'): 'B',\n",
    "    ('B', 'right'): 'G'\n",
    "}\n",
    "\n",
    "rewards = {\n",
    "    'S': -1,\n",
    "    'A': -1,\n",
    "    'B': -1,\n",
    "    'G': 10\n",
    "}\n",
    "\n",
    "# Simulate a simple trajectory\n",
    "state = 'S'\n",
    "total_reward = 0\n",
    "trajectory = []\n",
    "\n",
    "while state != 'G':\n",
    "    action = 'right'\n",
    "    next_state = transition_prob.get((state, action), state)\n",
    "    reward = rewards[next_state]\n",
    "    trajectory.append((state, action, reward, next_state))\n",
    "    total_reward += reward\n",
    "    state = next_state\n",
    "\n",
    "trajectory, total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Interpretation\n",
    "- The agent moves from **S â†’ A â†’ B â†’ G**.\n",
    "- The total reward is the **sum of intermediate penalties** and **final goal reward**.\n",
    "- This simple structure forms the basis of **policy evaluation** and **learning** in RL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§  Return and Value Functions\n",
    "\n",
    "The **return** ($G_t$) is the total discounted reward from time step `t`:\n",
    "\n",
    "$$\n",
    "G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ... = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}\n",
    "$$\n",
    "\n",
    "The **value function** gives the expected return from a state `s` under a policy `Ï€`:\n",
    "\n",
    "$$\n",
    "V_Ï€(s) = E_Ï€ [G_t | S_t = s]\n",
    "$$\n",
    "\n",
    "The **action-value function** is:\n",
    "\n",
    "$$\n",
    "Q_Ï€(s, a) = E_Ï€ [G_t | S_t = s, A_t = a]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Compute discounted return\n",
    "rewards_list = [1, 1, 1, 10]  # rewards over steps\n",
    "gamma = 0.9\n",
    "\n",
    "G = 0\n",
    "for r in reversed(rewards_list):\n",
    "    G = r + gamma * G\n",
    "\n",
    "print(f\"Total discounted return: {G:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“˜ Summary\n",
    "\n",
    "- MDPs provide the **theoretical foundation** for RL problems.\n",
    "- The **Markov property** ensures decisions depend only on the current state.\n",
    "- Rewards and transition functions define the **environment's behavior**.\n",
    "- **Value and Q-functions** estimate the long-term return for policies.\n",
    "\n",
    "Next, weâ€™ll explore how **policies** and **value iteration** help find optimal strategies for decision-making."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
