{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL in Robotics Simulation\n",
    "\n",
    "This notebook demonstrates how to use physics-based robotics simulators (PyBullet) with reinforcement learning. It covers:\n",
    "- setting up a PyBullet environment,\n",
    "- running a random agent to interact with the simulator,\n",
    "- collecting trajectories,\n",
    "- a short example of plugging in an RL library (optional),\n",
    "- practical notes and tips for robotics experiments.\n",
    "\n",
    "This notebook focuses on **practical, reproducible workflows** for learning and prototyping RL algorithms in robotics simulations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 Install (if needed)\n",
    "\n",
    "Uncomment and run the following if your environment doesn't already have the packages. Installing PyBullet and the PyBullet Gym wrappers is usually enough for many robotics examples.\n",
    "\n",
    "If you prefer using `stable-baselines3` for quick experiments, uncomment its install too (but it is optional and larger)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pybullet pybullet_envs gymnasium --quiet\n",
    "# Optional: a lightweight RL library for quick experiments\n",
    "# !pip install stable-baselines3[extra] --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧩 Imports and helper utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import gym\n",
    "import pybullet_envs  # registers PyBullet environments with Gym\n",
    "from collections import deque\n",
    "\n",
    "def run_random_episode(env, render=False, max_steps=1000):\n",
    "    obs, _ = env.reset()\n",
    "    total_reward = 0.0\n",
    "    steps = 0\n",
    "    done = False\n",
    "    while not done and steps < max_steps:\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "        if render:\n",
    "            env.render()\n",
    "            # small sleep for human-visualization\n",
    "            time.sleep(1 / 120)\n",
    "    return total_reward, steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ Create and inspect a PyBullet robotics environment\n",
    "\n",
    "Common lightweight robotics / continuous-control environments provided by `pybullet_envs` include:\n",
    "- `InvertedPendulumBulletEnv-v0` (simple control)\n",
    "- `InvertedDoublePendulumBulletEnv-v0`\n",
    "- `AntBulletEnv-v0` (quadruped-like agent)\n",
    "- `KukaBulletEnv-v0` (robot arm scenarios)\n",
    "\n",
    "Pick one that matches your experiment complexity; start simple and scale up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Inverted Pendulum (fast to run and observe)\n",
    "env_id = 'InvertedPendulumBulletEnv-v0'  # change to AntBulletEnv-v0 or KukaBulletEnv-v0 for harder tasks\n",
    "env = gym.make(env_id)\n",
    "print('Environment:', env_id)\n",
    "print('Observation space:', env.observation_space)\n",
    "print('Action space:', env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ▶️ Run a few random episodes to sanity-check the simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "for i in range(3):\n",
    "    r, steps = run_random_episode(env, render=False, max_steps=1000)\n",
    "    rewards.append(r)\n",
    "    print(f'Episode {i+1}: reward={r:.2f}, steps={steps}')\n",
    "print('Random policy mean reward:', np.mean(rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📦 Collect trajectories (example) — store transitions for offline learning or analysis\n",
    "\n",
    "This snippet collects a fixed number of episodes and stores `(s, a, r, s', done)` tuples in memory. Useful for debugging, curriculum learning or offline RL experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_episodes(env, policy_fn, n_episodes=5, max_steps=1000):\n",
    "    buffer = []\n",
    "    for ep in range(n_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        steps = 0\n",
    "        while not done and steps < max_steps:\n",
    "            action = policy_fn(obs)\n",
    "            next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            buffer.append((obs, action, reward, next_obs, done))\n",
    "            obs = next_obs\n",
    "            steps += 1\n",
    "    return buffer\n",
    "\n",
    "# Example: collect 3 episodes using a random policy\n",
    "random_policy = lambda s: env.action_space.sample()\n",
    "dataset = collect_episodes(env, random_policy, n_episodes=3)\n",
    "print(f'Collected {len(dataset)} transitions (sample)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ▶️ Quick example: plug-in `stable-baselines3` (optional)\n",
    "\n",
    "If you want to run a ready-made RL algorithm quickly, `stable-baselines3` provides clean implementations (PPO, SAC, TD3, DQN). The snippet below is optional — uncomment/install if you have the package and want a fast baseline.\n",
    "\n",
    "⚠️ Note: installing `stable-baselines3[extra]` can be large and may require additional system packages for some environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: run a quick PPO baseline with stable-baselines3 (uncomment to run)\n",
    "# from stable_baselines3 import PPO\n",
    "# model = PPO('MlpPolicy', env, verbose=1)\n",
    "# model.learn(total_timesteps=20000)\n",
    "# print('PPO baseline trained (20k steps)')\n",
    "# # Evaluate\n",
    "# mean_reward = 0.0\n",
    "# for _ in range(5):\n",
    "#     obs, _ = env.reset()\n",
    "#     done = False\n",
    "#     ep_r = 0\n",
    "#     while not done:\n",
    "#         action, _ = model.predict(obs, deterministic=True)\n",
    "#         obs, reward, terminated, truncated, info = env.step(action)\n",
    "#         done = terminated or truncated\n",
    "#         ep_r += reward\n",
    "#     mean_reward += ep_r\n",
    "# print('Mean eval reward (5 eps):', mean_reward / 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🛠 Practical tips for robotics simulation experiments\n",
    "\n",
    "- **Start simple:** test algorithms on `InvertedPendulumBulletEnv-v0` before moving to `AntBulletEnv-v0` or KUKA arm tasks.\n",
    "- **Deterministic seeds:** set seeds for NumPy, Python `random`, and the environment to reproduce results.\n",
    "- **Rendering:** run rendering only during evaluation. Rendering slows training significantly.\n",
    "- **Frame-rate & physics steps:** tune physics timestep and substeps if the simulator supports it — affects fidelity and stability.\n",
    "- **Domain randomization:** randomize simulation parameters (mass, friction, delays) to produce more robust policies for sim-to-real transfer.\n",
    "- **Observation design:** include velocities/angles/forces that are available in real sensors if planning sim-to-real transfer.\n",
    "- **Use lightweight wrappers:** to normalize observations, clip actions, and scale rewards consistently across algorithms.\n",
    "- **Monitor CPU/GPU:** complex simulators and parallel environments can be CPU-bound; profile to identify bottlenecks.\n",
    "- **Consider using vectorized envs** (many parallel sims) to increase sample throughput — packages like `stable-baselines3` accept vectorized environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ Summary\n",
    "\n",
    "- This notebook showed how to start RL experiments in robotics simulators using **PyBullet**.\n",
    "- You learned how to create environments, run random agents, collect transitions, and (optionally) plug into `stable-baselines3` for quick baselines.\n",
    "- Practical tips were provided to help scale experiments and prepare for sim-to-real transfer.\n",
    "\n",
    "If you'd like, I can:\n",
    "1. provide a **complete DDPG / TD3 / SAC** example tuned for a PyBullet robot (e.g., `AntBulletEnv-v0`),\n",
    "2. generate utility wrappers (normalization, vectorized envs) for faster training,\n",
    "3. or produce a **README + experiment template** (Dockerfile, requirements, run scripts) for reproducible robotics RL experiments.\n",
    "\n",
    "Which would you like next?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

