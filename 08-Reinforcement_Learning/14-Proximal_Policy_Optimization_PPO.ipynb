{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proximal Policy Optimization (PPO)\n",
    "\n",
    "This notebook introduces **Proximal Policy Optimization (PPO)** : a stable and widely-used policy-gradient method. We present the intuition, the clipped surrogate objective, and a concise PyTorch implementation applied to a discrete-action environment (`CartPole-v1`).\n",
    "\n",
    "You'll find:\n",
    "- PPO theory (clipped objective)\n",
    "- A compact actor-critic implementation (shared network)\n",
    "- Trajectory collection and advantage computation\n",
    "- PPO update loop\n",
    "\n",
    "This implementation is educational: it prioritizes clarity over raw performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ” Quick theory recap\n",
    "\n",
    "PPO maximizes a clipped surrogate objective to keep policy updates **proximal** (not too large):\n",
    "\n",
    "$$ L^{CLIP}(\\theta) = \\mathbb{E}_t\\Big[ \\min\\big( r_t(\\theta) \\hat{A}_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) \\hat{A}_t \\big)\\Big] $$\n",
    "where $r_t(\\theta)=\\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}$ and $\\hat{A}_t$ is the advantage estimate. Clipping prevents large policy shifts and stabilizes training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§© Minimal PyTorch Implementation (CartPole-v1)\n",
    "Run cells sequentially. If running in a fresh environment, install `gymnasium` and `torch` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install if needed\n",
    "# !pip install gymnasium torch --quiet\n",
    "\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”§ Actor-Critic (shared) network\n",
    "Outputs a policy (action probabilities) and state-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, obs_dim, n_actions, hidden=64):\n",
    "        super().__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.policy_head = nn.Linear(hidden, n_actions)\n",
    "        self.value_head = nn.Linear(hidden, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.shared(x)\n",
    "        logits = self.policy_head(x)\n",
    "        value = self.value_head(x).squeeze(-1)\n",
    "        return logits, value\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        obs_t = torch.tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        logits, value = self.forward(obs_t)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        m = torch.distributions.Categorical(probs)\n",
    "        a = m.sample()\n",
    "        return a.item(), m.log_prob(a).item(), value.item(), probs.detach().cpu().numpy()[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Trajectory buffer & advantage computation (simple returns minus baseline)\n",
    "We use **discounted returns** and compute advantages as `G - V` (you can replace with GAE for better performance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(rewards, dones, last_value, gamma=0.99):\n",
    "    returns = []\n",
    "    R = last_value\n",
    "    for r, d in zip(reversed(rewards), reversed(dones)):\n",
    "        if d:\n",
    "            R = 0.0\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "class RolloutBuffer:\n",
    "    def __init__(self):\n",
    "        self.obs = []\n",
    "        self.actions = []\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.values = []\n",
    "\n",
    "    def clear(self):\n",
    "        self.__init__()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª PPO Update step (clipped surrogate + value loss + entropy bonus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_update(model, optimizer, obs, actions, old_log_probs, returns, values, clip_eps=0.2, c1=0.5, c2=0.01, epochs=4, batch_size=64):\n",
    "    # Convert to tensors\n",
    "    obs = torch.tensor(np.array(obs), dtype=torch.float32, device=device)\n",
    "    actions = torch.tensor(np.array(actions), dtype=torch.long, device=device)\n",
    "    old_log_probs = torch.tensor(np.array(old_log_probs), dtype=torch.float32, device=device)\n",
    "    returns = torch.tensor(np.array(returns), dtype=torch.float32, device=device)\n",
    "    values = torch.tensor(np.array(values), dtype=torch.float32, device=device)\n",
    "\n",
    "    advantages = returns - values\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "    dataset_size = len(obs)\n",
    "    inds = np.arange(dataset_size)\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        np.random.shuffle(inds)\n",
    "        for start in range(0, dataset_size, batch_size):\n",
    "            batch_inds = inds[start:start+batch_size]\n",
    "            b_obs = obs[batch_inds]\n",
    "            b_actions = actions[batch_inds]\n",
    "            b_old_logp = old_log_probs[batch_inds]\n",
    "            b_returns = returns[batch_inds]\n",
    "            b_adv = advantages[batch_inds]\n",
    "\n",
    "            logits, vals = model(b_obs)\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            dist = torch.distributions.Categorical(probs)\n",
    "            new_logp = dist.log_prob(b_actions)\n",
    "            entropy = dist.entropy().mean()\n",
    "\n",
    "            ratio = torch.exp(new_logp - b_old_logp)\n",
    "            surr1 = ratio * b_adv\n",
    "            surr2 = torch.clamp(ratio, 1.0 - clip_eps, 1.0 + clip_eps) * b_adv\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "            value_loss = ((b_returns - vals) ** 2).mean()\n",
    "\n",
    "            loss = policy_loss + c1 * value_loss - c2 * entropy\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## â–¶ï¸ Training loop (compact)\n",
    "Adjust `TOTAL_UPDATES` / `steps_per_update` for longer training. This is a compact demo to show the full flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "model = ActorCritic(obs_dim, n_actions).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "buffer = RolloutBuffer()\n",
    "\n",
    "TOTAL_UPDATES = 300  # number of outer loops\n",
    "steps_per_update = 2048  # collect this many steps per update (reduce for faster demo)\n",
    "gamma = 0.99\n",
    "\n",
    "ep_rewards = []\n",
    "obs, _ = env.reset()\n",
    "episode_reward = 0\n",
    "\n",
    "for update in range(1, TOTAL_UPDATES + 1):\n",
    "    buffer.clear()\n",
    "    for step in range(steps_per_update):\n",
    "        action, logp, value, _ = model.get_action(obs)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        buffer.obs.append(obs)\n",
    "        buffer.actions.append(action)\n",
    "        buffer.log_probs.append(logp)\n",
    "        buffer.values.append(value)\n",
    "        buffer.rewards.append(reward)\n",
    "        buffer.dones.append(done)\n",
    "\n",
    "        obs = next_obs\n",
    "        episode_reward += reward\n",
    "\n",
    "        if done:\n",
    "            ep_rewards.append(episode_reward)\n",
    "            obs, _ = env.reset()\n",
    "            episode_reward = 0\n",
    "\n",
    "    # compute last value for bootstrapping\n",
    "    with torch.no_grad():\n",
    "        _, last_value = model.forward(torch.tensor(obs, dtype=torch.float32, device=device).unsqueeze(0))\n",
    "        last_value = last_value.item()\n",
    "\n",
    "    returns = compute_returns(buffer.rewards, buffer.dones, last_value, gamma=gamma)\n",
    "\n",
    "    # update\n",
    "    ppo_update(model, optimizer, buffer.obs, buffer.actions, buffer.log_probs, returns, buffer.values,\n",
    "               clip_eps=0.2, c1=0.5, c2=0.01, epochs=8, batch_size=64)\n",
    "\n",
    "    if update % 5 == 0:\n",
    "        avg_r = np.mean(ep_rewards[-20:]) if len(ep_rewards) > 0 else 0.0\n",
    "        print(f'Update {update}/{TOTAL_UPDATES} | AvgReward(20): {avg_r:.2f}')\n",
    "\n",
    "# Plot training rewards\n",
    "plt.plot(ep_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('PPO: Episode rewards')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§¾ Notes & Next steps\n",
    "- This implementation uses **returns - value** as advantage; for improved performance use **GAE (Generalized Advantage Estimation)**.\n",
    "- PPO has many practical tricks (learning rate schedules, entropy annealing, normalization, clipping values) â€” explore those once the base algorithm works.\n",
    "- For continuous actions, replace the categorical policy with a Gaussian policy (mean/std outputs).\n",
    "\n",
    "## âœ… Summary\n",
    "- PPO provides a reliable, easy-to-use policy gradient algorithm.\n",
    "- The clipped surrogate objective stabilizes updates and is the core innovation.\n",
    "- This notebook demonstrates a compact actor-critic PPO that you can extend and improve."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

