{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Networks (DQN)\n",
    "\n",
    "In this notebook we implement a **Deep Q-Network (DQN)** : a powerful extension of Q-Learning that uses a neural network to approximate the Q-function. We'll apply DQN to the classic control task **CartPole-v1** using **TensorFlow / Keras** and **Gymnasium**.\n",
    "\n",
    "### What you will learn\n",
    "- How DQN approximates Q-values with a neural network\n",
    "- Experience replay buffer\n",
    "- Epsilon-greedy exploration\n",
    "- Target network and soft/hard updates\n",
    "- A simple training loop to learn to solve CartPole\n",
    "\n",
    "This notebook is educational and intentionally compact. For production-level training use stable-baselines3 or RLlib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Install / Import libraries\n",
    "If you're running this in a fresh environment, install `gymnasium` and `tensorflow` first. (Commented install lines are provided.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run if needed\n",
    "# !pip install gymnasium --quiet\n",
    "# !pip install 'gymnasium[classic_control]' --quiet\n",
    "# !pip install tensorflow --quiet\n",
    "\n",
    "import random\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "print('TensorFlow version:', tf.__version__)\n",
    "print('Gymnasium version:', gym.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Build a simple Q-network (MLP)\n",
    "The network maps state → Q-values for each action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_q_model(state_shape, n_actions):\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=state_shape),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(n_actions, activation='linear')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# quick instantiation to show summary\n",
    "env = gym.make('CartPole-v1')\n",
    "obs, info = env.reset()\n",
    "state_shape = obs.shape\n",
    "n_actions = env.action_space.n\n",
    "qnet = build_q_model(state_shape, n_actions)\n",
    "qnet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Experience Replay Buffer\n",
    "Stores recent transitions `(s, a, r, s', done)` and samples mini-batches for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=100000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = map(np.array, zip(*batch))\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# quick test\n",
    "rb = ReplayBuffer(1000)\n",
    "for i in range(10):\n",
    "    rb.push(np.zeros(state_shape), 0, 1.0, np.zeros(state_shape), False)\n",
    "print('ReplayBuffer length:', len(rb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) DQN Agent (with target network and epsilon-greedy policy)\n",
    "We'll implement: update steps, epsilon decay, and target network (hard update)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_shape, n_actions,\n",
    "                 lr=1e-3, gamma=0.99, batch_size=64,\n",
    "                 buffer_capacity=50000, min_replay_size=1000,\n",
    "                 target_update_freq=1000):\n",
    "        self.n_actions = n_actions\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.min_replay_size = min_replay_size\n",
    "        self.target_update_freq = target_update_freq\n",
    "\n",
    "        self.q_network = build_q_model(state_shape, n_actions)\n",
    "        self.target_network = build_q_model(state_shape, n_actions)\n",
    "        self.optimizer = optimizers.Adam(learning_rate=lr)\n",
    "        self.loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "        # Initially copy weights\n",
    "        self.update_target(hard=True)\n",
    "\n",
    "        self.replay_buffer = ReplayBuffer(capacity=buffer_capacity)\n",
    "        self.train_steps = 0\n",
    "\n",
    "    def update_target(self, hard=False):\n",
    "        if hard:\n",
    "            self.target_network.set_weights(self.q_network.get_weights())\n",
    "        else:\n",
    "            # soft update (tau)\n",
    "            tau = 0.005\n",
    "            q_weights = np.array(self.q_network.get_weights(), dtype=object)\n",
    "            t_weights = np.array(self.target_network.get_weights(), dtype=object)\n",
    "            new_weights = [tau * q + (1 - tau) * t for q, t in zip(q_weights, t_weights)]\n",
    "            self.target_network.set_weights(new_weights)\n",
    "\n",
    "    def act(self, state, epsilon=0.1):\n",
    "        if np.random.rand() < epsilon:\n",
    "            return np.random.randint(self.n_actions)\n",
    "        q_values = self.q_network.predict(state[np.newaxis], verbose=0)[0]\n",
    "        return int(np.argmax(q_values))\n",
    "\n",
    "    def push(self, s, a, r, s2, done):\n",
    "        self.replay_buffer.push(s, a, r, s2, done)\n",
    "\n",
    "    def train_step(self):\n",
    "        if len(self.replay_buffer) < max(self.min_replay_size, self.batch_size):\n",
    "            return None\n",
    "\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "        # Convert to tensors\n",
    "        states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "        next_states = tf.convert_to_tensor(next_states, dtype=tf.float32)\n",
    "        actions = tf.convert_to_tensor(actions, dtype=tf.int32)\n",
    "        rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "        dones = tf.convert_to_tensor(dones.astype(np.float32), dtype=tf.float32)\n",
    "\n",
    "        # Compute target Q-values\n",
    "        next_q = self.target_network(next_states)\n",
    "        max_next_q = tf.reduce_max(next_q, axis=1)\n",
    "        target_q = rewards + (1.0 - dones) * self.gamma * max_next_q\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            q_vals = self.q_network(states)\n",
    "            # gather predicted q for taken actions\n",
    "            indices = tf.stack([tf.range(self.batch_size), actions], axis=1)\n",
    "            pred_q = tf.gather_nd(q_vals, indices)\n",
    "            loss = self.loss_fn(target_q, pred_q)\n",
    "\n",
    "        grads = tape.gradient(loss, self.q_network.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.q_network.trainable_variables))\n",
    "\n",
    "        # target network update\n",
    "        self.train_steps += 1\n",
    "        if self.train_steps % self.target_update_freq == 0:\n",
    "            self.update_target(hard=True)\n",
    "\n",
    "        return float(loss.numpy())\n",
    "\n",
    "    def save(self, path='dqn_model.h5'):\n",
    "        self.q_network.save(path)\n",
    "\n",
    "    def load(self, path='dqn_model.h5'):\n",
    "        self.q_network = tf.keras.models.load_model(path)\n",
    "        self.update_target(hard=True)\n",
    "\n",
    "# Instantiate the agent\n",
    "agent = DQNAgent(state_shape=state_shape, n_actions=n_actions)\n",
    "print('Agent ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Training loop\n",
    "We'll train for a small number of episodes to keep this demo fast. Increase `NUM_EPISODES` for a stronger agent.\n",
    "\n",
    "We use an epsilon that decays from `eps_start` → `eps_end` over `eps_decay_steps`.\n",
    "Training prints episodic reward and periodic loss statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPISODES = 300\n",
    "MAX_STEPS = 500\n",
    "eps_start = 1.0\n",
    "eps_end = 0.01\n",
    "eps_decay_steps = 20000\n",
    "\n",
    "epsilon = eps_start\n",
    "eps_decay = (eps_start - eps_end) / eps_decay_steps\n",
    "\n",
    "rewards_history = []\n",
    "loss_history = []\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "start_time = time.time()\n",
    "for ep in range(1, NUM_EPISODES + 1):\n",
    "    state, info = env.reset()\n",
    "    ep_reward = 0\n",
    "    for step in range(MAX_STEPS):\n",
    "        action = agent.act(state, epsilon)\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        done_flag = bool(done or truncated)\n",
    "        agent.push(state, action, reward, next_state, done_flag)\n",
    "        loss = agent.train_step()\n",
    "        if loss is not None:\n",
    "            loss_history.append(loss)\n",
    "        state = next_state\n",
    "        ep_reward += reward\n",
    "\n",
    "        # decay epsilon\n",
    "        if epsilon > eps_end:\n",
    "            epsilon -= eps_decay\n",
    "\n",
    "        if done_flag:\n",
    "            break\n",
    "\n",
    "    rewards_history.append(ep_reward)\n",
    "\n",
    "    if ep % 10 == 0:\n",
    "        avg_reward = np.mean(rewards_history[-10:])\n",
    "        avg_loss = np.mean(loss_history[-50:]) if loss_history else 0.0\n",
    "        print(f'Episode {ep:04d} | AvgReward(10) {avg_reward:.2f} | Epsilon {epsilon:.3f} | AvgLoss {avg_loss:.4f}')\n",
    "\n",
    "# final timing\n",
    "print(f\"Training finished in {time.time() - start_time:.1f}s\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Plot learning curves\n",
    "Plot episodic reward and training loss to inspect learning behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(rewards_history)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Episodic Reward')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(loss_history[-1000:])\n",
    "plt.xlabel('Training Steps (recent)')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss (recent)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Evaluate the trained agent\n",
    "Run a few episodes with epsilon=0 (greedy) to see the learned performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_EPISODES = 10\n",
    "eval_rewards = []\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "for ep in range(EVAL_EPISODES):\n",
    "    state, info = env.reset()\n",
    "    ep_r = 0\n",
    "    for _ in range(500):\n",
    "        action = agent.act(state, epsilon=0.0)  # greedy\n",
    "        state, reward, done, truncated, info = env.step(action)\n",
    "        ep_r += reward\n",
    "        if done or truncated:\n",
    "            break\n",
    "    eval_rewards.append(ep_r)\n",
    "env.close()\n",
    "print(f'Evaluation over {EVAL_EPISODES} episodes — avg reward: {np.mean(eval_rewards):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Save model (optional)\n",
    "Save the learned Q-network for later reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save('dqn_cartpole.h5')\n",
    "print('Saved model to dqn_cartpole.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes & Next steps\n",
    "- This is a compact DQN implementation. Practical improvements include:\n",
    "  - Prioritized Experience Replay\n",
    "  - Double DQN (to reduce Q-value overestimation)\n",
    "  - Dueling networks\n",
    "  - Better exploration schedules and reward normalization\n",
    "- For larger, harder environments (Atari) use established libraries (stable-baselines3, RLlib).\n",
    "\n",
    "If you'd like, I can convert this into a standalone Python training script, add Double DQN, or produce a lightweight visualization of agent frames as a GIF."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
