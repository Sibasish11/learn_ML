{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Improvements: Double DQN & Prioritized Replay\n",
    "\n",
    "This notebook demonstrates two important improvements to the original DQN:\n",
    "\n",
    "1. **Double DQN (DDQN)** : reduces overestimation bias by using the online network to select the next action and the target network to evaluate it.\n",
    "2. **Prioritized Experience Replay (PER)** : samples important transitions (with high TD error) more frequently to speed up learning.\n",
    "\n",
    "We'll provide clear, minimal implementations (educational, not production-optimized) using TensorFlow/Keras and `gymnasium` CartPole-v1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Imports & Setup\n",
    "\n",
    "Uncomment pip installs if you need to run in a fresh environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gymnasium --quiet\n",
    "# !pip install tensorflow --quiet\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import collections\n",
    "import gymnasium as gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('TF', tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Simple Prioritized Replay Buffer (Proportional)\n",
    "\n",
    "This is a straightforward implementation for learning/demo purposes: we store a priority for each transition and sample with probability proportional to that priority. We also include importance-sampling (IS) weights to partially correct sampling bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, alpha=0.6):\n",
    "        self.capacity = capacity\n",
    "        self.alpha = alpha  # how much prioritization is used (0 = uniform)\n",
    "        self.buffer = []\n",
    "        self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
    "        self.pos = 0\n",
    "\n",
    "    def push(self, transition, priority=None):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(transition)\n",
    "        else:\n",
    "            self.buffer[self.pos] = transition\n",
    "        if priority is None:\n",
    "            max_prio = self.priorities.max() if len(self.buffer) > 0 else 1.0\n",
    "            self.priorities[self.pos] = max_prio\n",
    "        else:\n",
    "            self.priorities[self.pos] = priority\n",
    "        self.pos = (self.pos + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        if len(self.buffer) == self.capacity:\n",
    "            prios = self.priorities\n",
    "        else:\n",
    "            prios = self.priorities[:self.pos]\n",
    "        probs = prios ** self.alpha\n",
    "        probs /= probs.sum()\n",
    "\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "\n",
    "        # importance-sampling weights\n",
    "        total = len(self.buffer)\n",
    "        weights = (total * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "        return samples, indices, weights\n",
    "\n",
    "    def update_priorities(self, indices, priorities):\n",
    "        for idx, prio in zip(indices, priorities):\n",
    "            self.priorities[idx] = prio\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# quick sanity\n",
    "buf = PrioritizedReplayBuffer(1000)\n",
    "buf.push((np.zeros(4), 0, 1.0, np.ones(4), False))\n",
    "print('PER buffer OK, len:', len(buf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Q-network (same as DQN notebook)\n",
    "Small MLP from state ‚Üí Q-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_q_network(state_shape, n_actions, hidden_units=(64,64), lr=1e-3):\n",
    "    inputs = layers.Input(shape=state_shape)\n",
    "    x = inputs\n",
    "    for h in hidden_units:\n",
    "        x = layers.Dense(h, activation='relu')(x)\n",
    "    outputs = layers.Dense(n_actions, activation='linear')(x)\n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=lr), loss='mse')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "ENV_NAME = 'CartPole-v1'\n",
    "NUM_EPISODES = 300\n",
    "MAX_STEPS = 500\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99\n",
    "BUFFER_CAPACITY = 20000\n",
    "LEARNING_RATE = 1e-3\n",
    "TARGET_UPDATE_FREQ = 1000\n",
    "\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = 0.995\n",
    "\n",
    "MIN_REPLAY_SIZE = 1000\n",
    "\n",
    "PER_ALPHA = 0.6\n",
    "PER_BETA_START = 0.4\n",
    "PER_BETA_FRAMES = NUM_EPISODES * 1.0\n",
    "\n",
    "print('Hyperparameters defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Training Loop with Double DQN + Prioritized Replay\n",
    "\n",
    "Key DDQN change: when computing targets use **argmax** from online network and **value** from target network:\n",
    "\n",
    "target = r + Œ≥ * target_q(next_state)[ argmax_a online_q(next_state)[a] ]\n",
    "\n",
    "We update priorities using absolute TD error + small epsilon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(ENV_NAME)\n",
    "state_shape = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "online_q = build_q_network(state_shape, n_actions, lr=LEARNING_RATE)\n",
    "target_q = build_q_network(state_shape, n_actions, lr=LEARNING_RATE)\n",
    "target_q.set_weights(online_q.get_weights())\n",
    "\n",
    "replay = PrioritizedReplayBuffer(BUFFER_CAPACITY, alpha=PER_ALPHA)\n",
    "\n",
    "eps = EPS_START\n",
    "total_steps = 0\n",
    "episode_rewards = []\n",
    "\n",
    "beta = PER_BETA_START\n",
    "\n",
    "for ep in range(1, NUM_EPISODES + 1):\n",
    "    obs, _ = env.reset(seed=SEED + ep)\n",
    "    state = np.array(obs, dtype=np.float32)\n",
    "    ep_reward = 0\n",
    "    done = False\n",
    "    steps = 0\n",
    "\n",
    "    while not done and steps < MAX_STEPS:\n",
    "        # Epsilon-greedy\n",
    "        if random.random() < eps:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            q_vals = online_q.predict(state.reshape(1, -1), verbose=0)[0]\n",
    "            action = int(np.argmax(q_vals))\n",
    "\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        next_state = np.array(next_obs, dtype=np.float32)\n",
    "\n",
    "        # initial priority = max priority so new transitions are likely sampled\n",
    "        transition = (state, action, reward, next_state, done)\n",
    "        replay.push(transition)\n",
    "\n",
    "        state = next_state\n",
    "        ep_reward += reward\n",
    "        steps += 1\n",
    "        total_steps += 1\n",
    "\n",
    "        # Learn\n",
    "        if len(replay) >= MIN_REPLAY_SIZE:\n",
    "            beta = min(1.0, PER_BETA_START + (ep / PER_BETA_FRAMES) * (1.0 - PER_BETA_START))\n",
    "            samples, indices, is_weights = replay.sample(BATCH_SIZE, beta=beta)\n",
    "            states_b = np.array([s for (s,a,r,ns,d) in samples], dtype=np.float32)\n",
    "            actions_b = np.array([a for (s,a,r,ns,d) in samples], dtype=np.int32)\n",
    "            rewards_b = np.array([r for (s,a,r,ns,d) in samples], dtype=np.float32)\n",
    "            next_states_b = np.array([ns for (s,a,r,ns,d) in samples], dtype=np.float32)\n",
    "            dones_b = np.array([d for (s,a,r,ns,d) in samples], dtype=np.float32)\n",
    "            is_weights = np.array(is_weights, dtype=np.float32)\n",
    "\n",
    "            # Double DQN target calculation\n",
    "            q_next_online = online_q.predict(next_states_b, verbose=0)\n",
    "            next_actions = np.argmax(q_next_online, axis=1)\n",
    "            q_next_target = target_q.predict(next_states_b, verbose=0)\n",
    "            q_next_target_vals = q_next_target[np.arange(BATCH_SIZE), next_actions]\n",
    "\n",
    "            targets = online_q.predict(states_b, verbose=0)\n",
    "            td_errors = np.zeros(BATCH_SIZE, dtype=np.float32)\n",
    "            for i in range(BATCH_SIZE):\n",
    "                if dones_b[i]:\n",
    "                    target_val = rewards_b[i]\n",
    "                else:\n",
    "                    target_val = rewards_b[i] + GAMMA * q_next_target_vals[i]\n",
    "                td_errors[i] = abs(targets[i, actions_b[i]] - target_val)\n",
    "                targets[i, actions_b[i]] = target_val\n",
    "\n",
    "            # apply importance-sampling weights by scaling the MSE loss during training\n",
    "            # We'll perform a manual gradient step using train_on_batch with scaled targets approach\n",
    "            # Simple approach: multiply (targets - pred) by IS weights when computing loss isn't directly supported\n",
    "            # Instead, use train_on_batch with full targets and then update priorities using td_errors + eps\n",
    "            online_q.train_on_batch(states_b, targets)\n",
    "\n",
    "            # update priorities (+ small epsilon to avoid zero)\n",
    "            new_prios = td_errors + 1e-6\n",
    "            replay.update_priorities(indices, new_prios)\n",
    "\n",
    "        # update target network periodically\n",
    "        if total_steps % TARGET_UPDATE_FREQ == 0 and total_steps > 0:\n",
    "            target_q.set_weights(online_q.get_weights())\n",
    "\n",
    "    episode_rewards.append(ep_reward)\n",
    "    eps = max(EPS_END, eps * EPS_DECAY)\n",
    "\n",
    "    if ep % 10 == 0:\n",
    "        avg_reward = np.mean(episode_rewards[-10:])\n",
    "        print(f'Episode {ep:03d} | AvgReward(10): {avg_reward:.2f} | Eps: {eps:.3f} | ReplaySize: {len(replay)}')\n",
    "\n",
    "# Save model\n",
    "online_q.save('ddqn_per_cartpole.h5')\n",
    "print('Training completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Results & Visualization\n",
    "Plot the episode rewards to inspect learning progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(episode_rewards, label='Episode Reward')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('DDQN + PER on CartPole: Episode Rewards')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Notes and Caveats\n",
    "\n",
    "- This PER implementation is simplified (proportional sampling via numpy). Production implementations use efficient data structures (e.g., sum trees) for O(log N) updates and sampling.\n",
    "- We didn't fully incorporate IS weights in the loss function (we used a simpler routine). For better correctness, multiply TD errors by the IS weights in the loss during gradient updates (or implement custom training step).\n",
    "- DDQN mitigates Q-value overestimation and usually improves stability.\n",
    "- Consider advanced improvements: Dueling networks, Noisy networks, Prioritized Replay with sum-tree, Multi-step returns, and PER with correct IS weighting.\n",
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "- Implemented Double DQN target computation (select action with online network, evaluate with target network).\n",
    "- Implemented a simple prioritized replay buffer with proportional prioritization and importance sampling scheduling.\n",
    "- These techniques often improve sample efficiency and stability compared to vanilla DQN."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
