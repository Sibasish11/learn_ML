{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning: Off-Policy Temporal Difference Control\n",
    "\n",
    "Q-Learning is a powerful **model-free**, **off-policy** reinforcement learning algorithm.\n",
    "\n",
    "It learns the **optimal action-value function** $Q^*(s, a)$, which tells us the expected reward of taking an action `a` in state `s`, and following the optimal policy thereafter.\n",
    "\n",
    "---\n",
    "### üîç Q-Learning Update Rule\n",
    "\n",
    "$$Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha [r_{t+1} + \\gamma \\max_a Q(s_{t+1}, a) - Q(s_t, a_t)]$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha$ ‚Üí Learning rate\n",
    "- $\\gamma$ ‚Üí Discount factor\n",
    "- $r_{t+1}$ ‚Üí Reward from environment\n",
    "- $\\max_a Q(s_{t+1}, a)$ ‚Üí Greedy estimate of the next state's best Q-value\n",
    "\n",
    "---\n",
    "### üß© Key Concepts\n",
    "| Term | Meaning |\n",
    "|------|----------|\n",
    "| **Model-free** | Doesn‚Äôt need environment dynamics |\n",
    "| **Off-policy** | Learns from greedy policy while exploring |\n",
    "| **Bootstrapping** | Updates estimates using current estimates |\n",
    "\n",
    "---\n",
    "### üß† Example: Q-Learning in a Simple Grid Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define environment parameters\n",
    "n_states = 6\n",
    "actions = [0, 1]  # 0 = left, 1 = right\n",
    "Q = np.zeros((n_states, len(actions)))\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "epsilon = 0.2  # Exploration rate\n",
    "\n",
    "def step(state, action):\n",
    "    if state == 0 or state == 5:\n",
    "        return state, 0\n",
    "    if action == 1:  # Move right\n",
    "        next_state = state + 1\n",
    "    else:  # Move left\n",
    "        next_state = state - 1\n",
    "    reward = 1 if next_state == 5 else 0\n",
    "    return next_state, reward\n",
    "\n",
    "# Training loop\n",
    "for episode in range(200):\n",
    "    state = 2  # Start from the middle\n",
    "    while state not in [0, 5]:\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = random.choice(actions)\n",
    "        else:\n",
    "            action = np.argmax(Q[state])\n",
    "\n",
    "        next_state, reward = step(state, action)\n",
    "\n",
    "        # Q-learning update\n",
    "        Q[state, action] = Q[state, action] + alpha * (\n",
    "            reward + gamma * np.max(Q[next_state]) - Q[state, action]\n",
    "        )\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "print(\"Learned Q-Table:\")\n",
    "print(Q)\n",
    "print(\"\\nDerived Policy (0=Left, 1=Right):\")\n",
    "print(np.argmax(Q, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### üìà Visualizing Learning\n",
    "\n",
    "We can visualize how Q-values evolve over time to show convergence toward optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot Q-values for each state-action pair\n",
    "plt.figure(figsize=(8, 5))\n",
    "for a in range(len(actions)):\n",
    "    plt.plot(Q[:, a], label=f'Action {a}')\n",
    "plt.title('Learned Q-values by Action')\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Q-value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### üß© Epsilon-Greedy Exploration\n",
    "To balance **exploration** and **exploitation**, Q-Learning uses an *Œµ-greedy policy*:\n",
    "- With probability `Œµ`, select a random action (exploration).\n",
    "- With probability `1-Œµ`, select the best known action (exploitation).\n",
    "\n",
    "---\n",
    "### ‚öôÔ∏è Q-Learning Algorithm Summary\n",
    "1. Initialize Q(s, a) arbitrarily.\n",
    "2. For each episode:\n",
    "   - Initialize state `s`.\n",
    "   - For each step:\n",
    "     - Choose `a` using Œµ-greedy.\n",
    "     - Take action ‚Üí observe `r`, `s'`.\n",
    "     - Update Q using TD rule.\n",
    "     - `s ‚Üê s'`.\n",
    "3. Derive policy: `œÄ(s) = argmax_a Q(s, a)`.\n",
    "\n",
    "---\n",
    "### üöÄ Advantages\n",
    "- Works without a model of the environment.\n",
    "- Converges to the optimal policy under certain conditions.\n",
    "- Can handle large or continuous state spaces (with Deep Q-Networks).\n",
    "\n",
    "---\n",
    "### üìö References\n",
    "- Sutton & Barto, *Reinforcement Learning: An Introduction*\n",
    "- OpenAI Spinning Up: Q-Learning\n",
    "- David Silver, *UCL RL Course* (Lecture 6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
