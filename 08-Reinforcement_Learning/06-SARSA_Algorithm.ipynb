{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA Algorithm : On-Policy Temporal Difference Control\n",
    "\n",
    "SARSA stands for **State‚ÄìAction‚ÄìReward‚ÄìState‚ÄìAction**, representing the sequence of experience tuples the algorithm learns from.\n",
    "\n",
    "Unlike Q-Learning (which is **off-policy**), **SARSA is an on-policy algorithm**: it learns the value of the policy it actually follows, not the optimal one derived from it.\n",
    "\n",
    "---\n",
    "### üîç SARSA Update Rule\n",
    "\n",
    "$$Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha [r_{t+1} + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)]$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha$ ‚Üí Learning rate\n",
    "- $\\gamma$ ‚Üí Discount factor\n",
    "- $r_{t+1}$ ‚Üí Reward from environment\n",
    "- $a_{t+1}$ ‚Üí Next action chosen by the **current policy**\n",
    "\n",
    "---\n",
    "### ‚öôÔ∏è Key Differences Between SARSA and Q-Learning\n",
    "| Feature | SARSA | Q-Learning |\n",
    "|----------|--------|-------------|\n",
    "| Type | On-policy | Off-policy |\n",
    "| Update Target | $r + \\gamma Q(s', a')$ | $r + \\gamma \\max_a Q(s', a)$ |\n",
    "| Exploration | Learns the actual Œµ-greedy behavior | Learns optimal greedy policy |\n",
    "\n",
    "---\n",
    "### üß© Example: SARSA in a Simple Grid Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define environment parameters\n",
    "n_states = 6\n",
    "actions = [0, 1]  # 0 = left, 1 = right\n",
    "Q = np.zeros((n_states, len(actions)))\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "epsilon = 0.2  # Exploration rate\n",
    "\n",
    "def step(state, action):\n",
    "    if state == 0 or state == 5:\n",
    "        return state, 0\n",
    "    if action == 1:  # Move right\n",
    "        next_state = state + 1\n",
    "    else:  # Move left\n",
    "        next_state = state - 1\n",
    "    reward = 1 if next_state == 5 else 0\n",
    "    return next_state, reward\n",
    "\n",
    "def choose_action(state):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return random.choice(actions)\n",
    "    else:\n",
    "        return np.argmax(Q[state])\n",
    "\n",
    "# Training loop for SARSA\n",
    "for episode in range(200):\n",
    "    state = 2  # Start from the middle\n",
    "    action = choose_action(state)\n",
    "    while state not in [0, 5]:\n",
    "        next_state, reward = step(state, action)\n",
    "        next_action = choose_action(next_state)\n",
    "\n",
    "        # SARSA update rule\n",
    "        Q[state, action] = Q[state, action] + alpha * (\n",
    "            reward + gamma * Q[next_state, next_action] - Q[state, action]\n",
    "        )\n",
    "\n",
    "        state, action = next_state, next_action\n",
    "\n",
    "print(\"Learned Q-Table:\")\n",
    "print(Q)\n",
    "print(\"\\nDerived Policy (0=Left, 1=Right):\")\n",
    "print(np.argmax(Q, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### üìä Comparing SARSA and Q-Learning\n",
    "- **SARSA** is more conservative: it accounts for exploration during learning.\n",
    "- **Q-Learning** assumes the agent always takes the optimal action next.\n",
    "\n",
    "Hence, in risky environments, **SARSA tends to produce safer policies**, while **Q-Learning** may produce more aggressive ones.\n",
    "\n",
    "---\n",
    "### üßÆ SARSA Algorithm Steps\n",
    "1. Initialize Q(s, a) arbitrarily.\n",
    "2. For each episode:\n",
    "   - Initialize `s` and choose `a` using Œµ-greedy.\n",
    "   - Repeat for each step:\n",
    "     - Take action `a`, observe `r` and `s'`.\n",
    "     - Choose `a'` from `s'` using Œµ-greedy.\n",
    "     - Update: `Q(s, a) ‚Üê Q(s, a) + Œ± [r + Œ≥Q(s', a') - Q(s, a)]`.\n",
    "     - Set `s ‚Üê s'`, `a ‚Üê a'`.\n",
    "3. Until `s` reaches terminal state.\n",
    "\n",
    "---\n",
    "### üìà Visualization of Q-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "for a in range(len(actions)):\n",
    "    plt.plot(Q[:, a], label=f'Action {a}')\n",
    "plt.title('Learned Q-values (SARSA)')\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Q-value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ‚öôÔ∏è Summary: SARSA vs Q-Learning\n",
    "| Algorithm | Policy Type | Update Target | Behavior |\n",
    "|------------|-------------|----------------|-----------|\n",
    "| **SARSA** | On-policy | $r + \\gamma Q(s', a')$ | Learns actual behavior |\n",
    "| **Q-Learning** | Off-policy | $r + \\gamma \\max_a Q(s', a)$ | Learns optimal behavior |\n",
    "\n",
    "---\n",
    "### üìö References\n",
    "- Sutton & Barto, *Reinforcement Learning: An Introduction* (Ch. 6)\n",
    "- David Silver, *UCL RL Course* : Lecture 5 (TD Control)\n",
    "- OpenAI Spinning Up : *SARSA Explained*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
