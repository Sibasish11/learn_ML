{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an Agent on CartPole üèãÔ∏è‚Äç‚ôÇÔ∏è\n",
    "\n",
    "In this notebook, we‚Äôll train an agent using **Deep Q-Learning (DQN)** on the popular `CartPole-v1` environment from OpenAI Gym.\n",
    "\n",
    "**Goals:**\n",
    "- Understand the CartPole environment.\n",
    "- Implement a Deep Q-Network agent using PyTorch.\n",
    "- Train and evaluate performance.\n",
    "- Visualize the learning progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym torch numpy matplotlib --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define the DQN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_size)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Replay Memory and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "    def push(self, transition):\n",
    "        self.memory.append(transition)\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize Environment and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "q_net = DQN(state_size, action_size).to(device)\n",
    "target_net = DQN(state_size, action_size).to(device)\n",
    "target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "optimizer = optim.Adam(q_net.parameters(), lr=1e-3)\n",
    "memory = ReplayBuffer(10000)\n",
    "\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.05\n",
    "target_update = 10\n",
    "episodes = 500\n",
    "reward_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return random.randrange(action_size)\n",
    "    state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        return q_net(state).argmax().item()\n",
    "\n",
    "for ep in range(episodes):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = select_action(state, epsilon)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        memory.push((state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        if len(memory) >= batch_size:\n",
    "            transitions = memory.sample(batch_size)\n",
    "            states, actions, rewards, next_states, dones = zip(*transitions)\n",
    "\n",
    "            states = torch.FloatTensor(states).to(device)\n",
    "            actions = torch.LongTensor(actions).unsqueeze(1).to(device)\n",
    "            rewards = torch.FloatTensor(rewards).unsqueeze(1).to(device)\n",
    "            next_states = torch.FloatTensor(next_states).to(device)\n",
    "            dones = torch.FloatTensor(dones).unsqueeze(1).to(device)\n",
    "\n",
    "            q_values = q_net(states).gather(1, actions)\n",
    "            next_q_values = target_net(next_states).max(1)[0].unsqueeze(1)\n",
    "            target_values = rewards + gamma * next_q_values * (1 - dones)\n",
    "\n",
    "            loss = nn.MSELoss()(q_values, target_values)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "    reward_history.append(total_reward)\n",
    "\n",
    "    if ep % target_update == 0:\n",
    "        target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "    if (ep + 1) % 50 == 0:\n",
    "        print(f\"Episode {ep+1}/{episodes}, Avg Reward (last 50): {np.mean(reward_history[-50:]):.2f}, Epsilon: {epsilon:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Training Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(reward_history)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('DQN Training on CartPole')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test the Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, _ = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "    action = select_action(state, epsilon=0.0)\n",
    "    next_state, reward, done, _, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "    state = next_state\n",
    "print(f'Total reward during test: {total_reward}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Summary\n",
    "- Implemented a **Deep Q-Network (DQN)** for CartPole.\n",
    "- Used **experience replay** and **target networks** for stability.\n",
    "- Visualized the training progress.\n",
    "\n",
    "This forms the foundation for advanced RL algorithms like **DDQN**, **Dueling DQN**, and **PPO**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
