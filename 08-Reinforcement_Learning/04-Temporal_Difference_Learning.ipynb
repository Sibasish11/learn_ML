{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Difference (TD) Learning\n",
    "\n",
    "Temporal Difference (TD) Learning combines ideas from **Monte Carlo methods** and **Dynamic Programming**.\n",
    "\n",
    "It learns directly from experience **without knowing the environment model**, updating value estimates based on **other learned estimates**.\n",
    "\n",
    "It‚Äôs a **bootstrapping** method : updating estimates using existing estimates.\n",
    "\n",
    "---\n",
    "### üîç Key Idea\n",
    "- Update value function $V(s)$ after every step:\n",
    "\n",
    "$$V(s_t) \\leftarrow V(s_t) + \\alpha [r_{t+1} + \\gamma V(s_{t+1}) - V(s_t)]$$\n",
    "\n",
    "Here:\n",
    "- $\\alpha$ = learning rate\n",
    "- $\\gamma$ = discount factor\n",
    "- $r_{t+1}$ = reward received after transitioning to $s_{t+1}$\n",
    "\n",
    "This formula defines **TD(0)** : the simplest form of Temporal Difference Learning.\n",
    "\n",
    "---\n",
    "### ‚öôÔ∏è Comparison\n",
    "| Method | Requires Model? | Updates When | Bootstraps? |\n",
    "|:--|:--:|:--:|:--:|\n",
    "| Monte Carlo | ‚ùå No | End of episode | ‚ùå No |\n",
    "| Dynamic Programming | ‚úÖ Yes | After full sweep | ‚úÖ Yes |\n",
    "| Temporal Difference | ‚ùå No | After each step | ‚úÖ Yes |\n",
    "\n",
    "---\n",
    "### ‚úÖ Advantages\n",
    "- Can learn directly from raw experience\n",
    "- Doesn‚Äôt require environment dynamics\n",
    "- More efficient than Monte Carlo methods for long episodes\n",
    "\n",
    "---\n",
    "### üß© Example: TD(0) Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "states = [0, 1, 2, 3, 4, 5]  # Example 1D chain environment\n",
    "terminal_states = [0, 5]\n",
    "V = np.zeros(len(states))\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "\n",
    "def step(state):\n",
    "    if state in terminal_states:\n",
    "        return state, 0\n",
    "    action = random.choice([-1, 1])  # move left or right\n",
    "    next_state = state + action\n",
    "    reward = 1 if next_state == 5 else 0\n",
    "    return next_state, reward\n",
    "\n",
    "for episode in range(100):\n",
    "    state = 3  # start from middle\n",
    "    while state not in terminal_states:\n",
    "        next_state, reward = step(state)\n",
    "        V[state] += alpha * (reward + gamma * V[next_state] - V[state])\n",
    "        state = next_state\n",
    "\n",
    "print(\"Learned Value Function:\")\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### üìò TD Control Methods\n",
    "TD learning can also be used for **control** ‚Äî to find the optimal policy.\n",
    "\n",
    "Two popular methods:\n",
    "- **SARSA (On-policy TD Control)**\n",
    "- **Q-Learning (Off-policy TD Control)**\n",
    "\n",
    "We‚Äôll explore both in upcoming notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### üß† Summary\n",
    "- **TD(0)** updates value after each step.\n",
    "- It bridges Monte Carlo and Dynamic Programming.\n",
    "- It is efficient and works in unknown environments.\n",
    "- Leads to **SARSA** and **Q-Learning**.\n",
    "\n",
    "---\n",
    "### üìö References\n",
    "- Sutton & Barto, *Reinforcement Learning: An Introduction*\n",
    "- OpenAI Spinning Up Tutorials\n",
    "- David Silver RL Course (Lecture 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
