{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization and Stopwords\n",
    "\n",
    "In NLP, **tokenization** means breaking a text into smaller units ‚Äî called **tokens** ‚Äî such as words, subwords, or sentences.\n",
    "\n",
    "**Stopwords** are commonly used words (like *and, is, the, in*) that are often removed to focus on more meaningful content.\n",
    "\n",
    "---\n",
    "## üéØ Objectives\n",
    "- Understand word and sentence tokenization\n",
    "- Compare tokenizers: NLTK, spaCy, and Hugging Face\n",
    "- Learn stopword removal techniques\n",
    "\n",
    "---\n",
    "## üß© 1Ô∏è‚É£ Word Tokenization using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "text = \"Natural Language Processing (NLP) helps machines understand human language. It‚Äôs amazing!\"\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìå Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üß† 2Ô∏è‚É£ Tokenization using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load small English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text)\n",
    "\n",
    "tokens_spacy = [token.text for token in doc]\n",
    "print(tokens_spacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî§ spaCy also provides detailed token-level information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(f\"{token.text:<15} POS: {token.pos_:<10} Lemma: {token.lemma_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ü§ó 3Ô∏è‚É£ Tokenization using Hugging Face Tokenizers\n",
    "\n",
    "Many modern NLP models like BERT or GPT use **subword tokenization** ‚Äî splitting text into smaller pieces based on word frequency.\n",
    "\n",
    "Let‚Äôs use a BERT tokenizer to see how it handles complex words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokens_bert = tokenizer.tokenize(text)\n",
    "print(tokens_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì¶ Converting Tokens to IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = tokenizer.convert_tokens_to_ids(tokens_bert)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üßπ 4Ô∏è‚É£ Stopword Removal using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered = [word for word in tokens if word.lower() not in stop_words and word.isalpha()]\n",
    "print(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üß∞ 5Ô∏è‚É£ Stopword Removal using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_spacy = [token.text for token in doc if not token.is_stop and token.is_alpha]\n",
    "print(filtered_spacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üßæ Summary\n",
    "- **Tokenization** splits text into tokens (words, subwords, or sentences).\n",
    "- **Stopwords** are removed to focus on meaningful words.\n",
    "- Libraries used: **NLTK, spaCy, Hugging Face Transformers.**\n",
    "\n",
    "---\n",
    " Next: Move to `03-Stemming_and_Lemmatization.ipynb` to explore how words are reduced to their root forms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
