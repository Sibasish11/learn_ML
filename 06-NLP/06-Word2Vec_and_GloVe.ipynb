{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec and GloVe\n",
    "\n",
    "**Objective:** Explore two popular word embedding models ‚Äî Word2Vec and GloVe ‚Äî and learn how to use them for semantic similarity and analogy tasks.\n",
    "\n",
    "---\n",
    "## üß† 1Ô∏è‚É£ Introduction\n",
    "\n",
    "Both **Word2Vec** (Google, 2013) and **GloVe** (Stanford, 2014) are methods to learn **dense vector representations** of words.\n",
    "\n",
    "- **Word2Vec:** Learns word meaning through *prediction* (neural network model).\n",
    "- **GloVe:** Learns word meaning through *count-based statistics* (co-occurrence matrix factorization).\n",
    "\n",
    "---\n",
    "## ‚öôÔ∏è 2Ô∏è‚É£ Training a Custom Word2Vec Model\n",
    "We‚Äôll train a simple Word2Vec model using a small text corpus to understand how embeddings are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = [\n",
    "    [\"the\", \"king\", \"rules\", \"the\", \"kingdom\"],\n",
    "    [\"the\", \"queen\", \"rules\", \"the\", \"empire\"],\n",
    "    [\"a\", \"man\", \"is\", \"strong\"],\n",
    "    [\"a\", \"woman\", \"is\", \"wise\"],\n",
    "    [\"the\", \"prince\", \"is\", \"the\", \"son\", \"of\", \"the\", \"king\"],\n",
    "    [\"the\", \"princess\", \"is\", \"the\", \"daughter\", \"of\", \"the\", \"queen\"]\n",
    "]\n",
    "\n",
    "model = Word2Vec(sentences, vector_size=100, window=3, min_count=1, sg=1)\n",
    "\n",
    "print(\"‚úÖ Word2Vec model trained successfully!\")\n",
    "print(\"\\nVector for 'king':\\n\", model.wv['king'][:10], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîç 3Ô∏è‚É£ Exploring Word Similarity and Analogies\n",
    "Word2Vec captures **semantic relationships** between words. Let‚Äôs explore a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most similar words to 'king'\n",
    "print(model.wv.most_similar('king'))\n",
    "\n",
    "# Analogy example: king - man + woman ‚âà ?\n",
    "print(\"\\nAnalogy result (king - man + woman):\")\n",
    "print(model.wv.most_similar(positive=['king', 'woman'], negative=['man'], topn=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ These relationships emerge naturally through context prediction during training.\n",
    "\n",
    "---\n",
    "## üìä 4Ô∏è‚É£ Visualizing Word2Vec Embeddings\n",
    "We can use PCA to project high-dimensional embeddings into 2D for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "words = list(model.wv.key_to_index.keys())\n",
    "X = model.wv[words]\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(result[:,0], result[:,1])\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, xy=(result[i,0], result[i,1]))\n",
    "plt.title('Word2Vec Embeddings (PCA Visualization)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üí° 5Ô∏è‚É£ Understanding GloVe\n",
    "\n",
    "**GloVe (Global Vectors for Word Representation)** uses **word co-occurrence statistics** to learn embeddings.\n",
    "\n",
    "- It counts how often words co-occur within a window.\n",
    "- Uses matrix factorization to find dense vectors that explain these co-occurrences.\n",
    "\n",
    "Pretrained GloVe models are widely available ‚Äî e.g., `glove.6B.100d.txt` from Stanford NLP.\n",
    "\n",
    "Let‚Äôs see how to use one (if downloaded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Example (requires pretrained GloVe file)\n",
    "# glove_model = KeyedVectors.load_word2vec_format('glove.6B.100d.txt', binary=False, no_header=True)\n",
    "# print(glove_model.most_similar('king'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **Note:** The GloVe model isn‚Äôt trained in real time here (as it‚Äôs large), but you can load pretrained embeddings to explore word relationships directly.\n",
    "\n",
    "---\n",
    "## üî¢ 6Ô∏è‚É£ Comparing Word2Vec vs GloVe\n",
    "\n",
    "| Feature | Word2Vec | GloVe |\n",
    "|----------|-----------|--------|\n",
    "| Training Type | Predictive (Neural Network) | Count-based (Matrix Factorization) |\n",
    "| Data Used | Context window | Global co-occurrence matrix |\n",
    "| Computation | Online (Stochastic Gradient Descent) | Offline (Matrix factorization) |\n",
    "| Interpretability | Harder | Easier (co-occurrence stats) |\n",
    "| Speed | Faster on small data | Better for large corpora |\n",
    "\n",
    "---\n",
    "## üß© 7Ô∏è‚É£ Applications of Word Embeddings\n",
    "- Sentiment analysis\n",
    "- Named Entity Recognition (NER)\n",
    "- Document similarity\n",
    "- Chatbots and question-answering\n",
    "- Machine translation\n",
    "\n",
    "---\n",
    "## ‚úÖ Summary\n",
    "- **Word2Vec** learns word meaning using context prediction.\n",
    "- **GloVe** learns embeddings from global word co-occurrences.\n",
    "- Both produce **dense, semantic vectors** used across modern NLP tasks.\n",
    "\n",
    "---\n",
    "üìò **Next:** `07-Contextual_Embeddings_BERT.ipynb` ‚Äî Learn how contextual embeddings like **BERT** go beyond Word2Vec by understanding words in context."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
