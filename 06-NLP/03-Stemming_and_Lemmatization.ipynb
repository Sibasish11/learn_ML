{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatization\n",
    "\n",
    "**Objective:** Reduce words to their base or root forms to normalize text and minimize redundancy in NLP tasks.\n",
    "\n",
    "### Why?\n",
    "Words like *running*, *runs*, and *ran* all mean the same thing — *run*. To make models understand this, we convert them into their **root forms** using stemming or lemmatization.\n",
    "\n",
    "---\n",
    "##  Stemming\n",
    "Stemming removes suffixes and prefixes to reach the base form of a word — often not a real dictionary word.\n",
    "\n",
    "Example: *studies → studi*, *running → run*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "text = \"The runners were running swiftly and studying various phenomena.\"\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Initialize stemmers\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "lancaster = LancasterStemmer()\n",
    "\n",
    "print(\"Porter Stemmer:\")\n",
    "print([porter.stem(w) for w in words])\n",
    "\n",
    "print(\"\\nSnowball Stemmer:\")\n",
    "print([snowball.stem(w) for w in words])\n",
    "\n",
    "print(\"\\nLancaster Stemmer:\")\n",
    "print([lancaster.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Common Stemmers\n",
    "- **PorterStemmer** → Most widely used; conservative.\n",
    "- **SnowballStemmer** → An improved version of Porter.\n",
    "- **LancasterStemmer** → Very aggressive (may over-trim words).\n",
    "\n",
    "---\n",
    "## Lemmatization\n",
    "Lemmatization converts words to their **base dictionary form (lemma)** using linguistic rules.\n",
    "\n",
    "Unlike stemming, it considers **Part of Speech (POS)** and ensures valid words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = [\"running\", \"ran\", \"better\", \"studies\", \"children\"]\n",
    "\n",
    "for w in words:\n",
    "    print(f\"{w:>10} → {lemmatizer.lemmatize(w)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization with POS Tags\n",
    "We can specify the **POS tag** to improve lemmatization results. For instance, *better* as adjective vs adverb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lemmatizer.lemmatize('running', pos='v'))  # verb\n",
    "print(lemmatizer.lemmatize('better', pos='a'))   # adjective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Lemmatization using spaCy\n",
    "spaCy automatically handles POS tagging and lemmatization efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    print(f\"{token.text:<15} → Lemma: {token.lemma_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 🧮 4️⃣ Visual Comparison of Stemming vs Lemmatization\n",
    "\n",
    "| Word | Stemming | Lemmatization |\n",
    "|------|-----------|---------------|\n",
    "| studies | studi | study |\n",
    "| running | run | run |\n",
    "| children | childr | child |\n",
    "| better | better | good |\n",
    "\n",
    "---\n",
    "## Summary\n",
    "- **Stemming** → Fast but crude (cuts off endings).\n",
    "- **Lemmatization** → Slower but linguistically accurate.\n",
    "- Libraries used: `nltk`, `spaCy`.\n",
    "\n",
    "---\n",
    " **Next:** `04-Bag_of_Words_and_TFIDF.ipynb` — Learn how to convert processed text into numerical features for ML models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
