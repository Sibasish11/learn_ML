{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering with Transformers\n",
    "\n",
    "In this notebook, you’ll learn how to perform **Question Answering (QA)** using **Transformer-based models** such as **BERT**, **RoBERTa**, or **DistilBERT**.\n",
    "\n",
    "These models can understand context and extract the correct answer from a given passage of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "- Understand what Question Answering is.\n",
    "- Learn how transformer models perform QA.\n",
    "- Use a pre-trained Hugging Face model to build a simple QA system.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is Question Answering?\n",
    "\n",
    "Question Answering (QA) is a **Natural Language Understanding (NLU)** task where the model reads a context (paragraph) and answers a question about it.\n",
    "\n",
    "**Example:**\n",
    "- **Context:** The Earth revolves around the Sun in 365 days.\n",
    "- **Question:** How long does the Earth take to revolve around the Sun?\n",
    "- **Answer:** 365 days.\n",
    "\n",
    "### Two main types of QA systems:\n",
    "1. **Extractive QA** → extracts the exact answer from context (e.g., BERT-based models)\n",
    "2. **Generative QA** → generates the answer from scratch (e.g., T5, GPT-based models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Uncomment if needed\n",
    "# !pip install transformers torch --quiet"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from transformers import pipeline"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Pre-trained QA Pipeline\n",
    "\n",
    "Hugging Face provides an easy-to-use **`question-answering`** pipeline that loads pre-trained models such as **DistilBERT** fine-tuned on the **SQuAD** dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "qa_pipeline = pipeline('question-answering', model='distilbert-base-uncased-distilled-squad')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Context and Question"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "context = (\n",
    "    \"Machine learning is a subfield of artificial intelligence that gives computers \"\n",
    "    \"the ability to learn from data without being explicitly programmed. It is used in various applications such as image recognition, speech processing, and recommendation systems.\"\n",
    ")\n",
    "\n",
    "question = \"What does machine learning allow computers to do?\""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Get the Answer"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "result = qa_pipeline(question=question, context=context)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {result['answer']}\")\n",
    "print(f\"Confidence: {result['score']:.4f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Try Different Questions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "questions = [\n",
    "    \"What is machine learning?\",\n",
    "    \"Where is machine learning used?\",\n",
    "    \"What is a subfield of artificial intelligence?\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    res = qa_pipeline(question=q, context=context)\n",
    "    print(f\"\\nQ: {q}\\nA: {res['answer']} (Confidence: {res['score']:.3f})\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. How Does It Work?\n",
    "\n",
    "- The model converts both the **question** and **context** into token embeddings.\n",
    "- It predicts two tokens — the **start** and **end** positions of the answer span.\n",
    "- The text between these positions is returned as the answer.\n",
    "\n",
    "This is known as **extractive question answering.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Other QA Models You Can Try\n",
    "\n",
    "| Model | Description |\n",
    "|--------|--------------|\n",
    "| `bert-large-uncased-whole-word-masking-finetuned-squad` | Classic BERT model fine-tuned on SQuAD. |\n",
    "| `roberta-base-squad2` | Robust and high-accuracy QA model. |\n",
    "| `deepset/roberta-base-squad2` | Optimized for extractive QA. |\n",
    "| `t5-base` or `flan-t5-base` | Generative QA — creates full-sentence answers. |\n",
    "\n",
    "You can replace the model name in the pipeline to test different ones!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "- **Question Answering** allows models to extract or generate answers from text.\n",
    "- **BERT-based models** are commonly used for extractive QA.\n",
    "- The Hugging Face **`pipeline`** makes it simple to implement.\n",
    "- Try experimenting with your own contexts to see how accurately the model responds.\n",
    "\n",
    "---\n",
    " **Next Recommended Notebook:** `16-Text_Summarization_with_Transformers.ipynb` — Learn how to summarize long texts efficiently!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
