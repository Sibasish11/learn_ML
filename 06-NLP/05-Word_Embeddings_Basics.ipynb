{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings Basics\n",
    "\n",
    "**Objective:** Understand how to represent words as continuous-valued vectors capturing meaning and relationships.\n",
    "\n",
    "---\n",
    "## What Are Word Embeddings?\n",
    "\n",
    "Traditional methods like **Bag of Words** or **TF-IDF** only count words — they do not capture **semantic relationships** between them.\n",
    "\n",
    "**Word Embeddings** are dense numerical representations of words where:\n",
    "- Similar words have similar vector representations.\n",
    "- Word relationships can be measured using distance (like cosine similarity).\n",
    "\n",
    "Example: *king - man + woman ≈ queen*\n",
    "\n",
    "---\n",
    "## Types of Embeddings\n",
    "1. **Learned from data** — Word2Vec, GloVe, FastText.\n",
    "2. **Pretrained contextual** — BERT, GPT, etc. (covered later).\n",
    "\n",
    "---\n",
    "## One-Hot Encoding Recap\n",
    "Before embeddings, let’s revisit one-hot encoding — where each word is represented as a binary vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "vocab = [\"king\", \"queen\", \"man\", \"woman\"]\n",
    "\n",
    "word_to_index = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "def one_hot(word):\n",
    "    vector = np.zeros(len(vocab))\n",
    "    vector[word_to_index[word]] = 1\n",
    "    return vector\n",
    "\n",
    "print(\"One-hot representation for 'queen':\")\n",
    "print(one_hot(\"queen\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " One-hot encoding is **simple but sparse** — it does not capture similarity. *King* and *Queen* are unrelated numerically.\n",
    "\n",
    "---\n",
    "## Introducing Word2Vec\n",
    "Word2Vec (by Google) learns embeddings using a **neural network** that predicts neighboring words.\n",
    "\n",
    "Two training models:\n",
    "- **CBOW (Continuous Bag of Words):** Predicts a word from its context.\n",
    "- **Skip-Gram:** Predicts context from a given word.\n",
    "\n",
    "Let’s use `gensim` to train a simple Word2Vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = [\n",
    "    [\"king\", \"rules\", \"the\", \"kingdom\"],\n",
    "    [\"queen\", \"rules\", \"the\", \"kingdom\"],\n",
    "    [\"man\", \"is\", \"strong\"],\n",
    "    [\"woman\", \"is\", \"wise\"]\n",
    "]\n",
    "\n",
    "model = Word2Vec(sentences, vector_size=50, window=3, min_count=1, sg=0)\n",
    "\n",
    "print(\"Vector for 'king':\\n\", model.wv['king'][:10], \"...\")\n",
    "print(\"\\nMost similar to 'king':\", model.wv.most_similar('king'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Each word now has a **50-dimensional vector** capturing its contextual meaning.\n",
    "\n",
    "---\n",
    "## Visualizing Word Embeddings\n",
    "Let’s project embeddings into 2D using PCA for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = model.wv[vocab]\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(result[:,0], result[:,1])\n",
    "\n",
    "for i, word in enumerate(vocab):\n",
    "    plt.annotate(word, xy=(result[i,0], result[i,1]))\n",
    "\n",
    "plt.title('Word Embeddings Visualized (PCA)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  GloVe (Global Vectors for Word Representation)\n",
    "GloVe (by Stanford) uses **word co-occurrence statistics** to learn embeddings.\n",
    "\n",
    "You can load pretrained vectors (e.g., 100D GloVe) to use them directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "# Example: Load pretrained GloVe or Word2Vec embeddings if available\n",
    "# glove_model = KeyedVectors.load_word2vec_format('glove.6B.100d.txt', binary=False)\n",
    "# glove_model.most_similar('king')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  Comparison Summary\n",
    "\n",
    "| Method | Representation | Captures Meaning | Sparse/Dense | Example |\n",
    "|---------|----------------|------------------|---------------|----------|\n",
    "| One-Hot | Binary | ❌ No | Sparse | `[0,0,1,0]` |\n",
    "| BoW / TF-IDF | Count-based | ❌ No | Sparse | `[1,0,2,1]` |\n",
    "| Word2Vec / GloVe | Learned | ✅ Yes | Dense | `[0.21, -0.13, 0.76, ...]` |\n",
    "\n",
    "---\n",
    "## ✅ Summary\n",
    "- Word embeddings capture **semantic similarity**.\n",
    "- Models like **Word2Vec** and **GloVe** transform text into meaningful vectors.\n",
    "- Paves the way for deep NLP models like RNNs, LSTMs, and Transformers.\n",
    "\n",
    "---\n",
    " **Next:** `06-Word2Vec_and_GloVe.ipynb` — Dive deeper into pretrained embedding models and advanced similarity operations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
