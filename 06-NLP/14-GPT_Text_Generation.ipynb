{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT Text Generation\n",
    "\n",
    "In this notebook, weâ€™ll explore **GPT (Generative Pre-trained Transformer)** and how it can be used for **text generation** â€” such as writing paragraphs, dialogues, or summaries.\n",
    "\n",
    "---\n",
    "\n",
    "### Objectives\n",
    "- Understand the **GPT architecture** and how it differs from BERT.\n",
    "- Use a pre-trained GPT model to generate text.\n",
    "- Control text generation using **temperature**, **max length**, and **top-k/top-p sampling**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is GPT?\n",
    "\n",
    "**GPT** (by OpenAI) stands for **Generative Pre-trained Transformer**. It is trained in a **unidirectional (left-to-right)** manner to predict the next word in a sequence.\n",
    "\n",
    "### ðŸ” Key Differences: BERT vs GPT\n",
    "| Feature | BERT | GPT |\n",
    "|----------|------|-----|\n",
    "| Type | Encoder | Decoder |\n",
    "| Direction | Bidirectional | Unidirectional |\n",
    "| Training Task | Masked Language Modeling | Next Word Prediction |\n",
    "| Use Case | Understanding (classification, QA) | Generation (writing, dialogue) |\n",
    "\n",
    "GPT learns to **generate coherent text** based on the prompt given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš™ï¸ 2. Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Uncomment if needed\n",
    "# !pip install transformers torch --quiet"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Pre-trained GPT-2 Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Text Generation Basics\n",
    "\n",
    "Weâ€™ll pass a **prompt** to GPT and let it generate the continuation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "prompt = \"In the future, artificial intelligence will\"\n",
    "\n",
    "inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "outputs = model.generate(\n",
    "    inputs,\n",
    "    max_length=50,        # total number of tokens in output\n",
    "    num_return_sequences=1,  # generate one text output\n",
    "    temperature=0.8,      # controls creativity (higher = more random)\n",
    "    top_k=50,             # top-k sampling\n",
    "    top_p=0.95,           # nucleus sampling\n",
    "    do_sample=True,       # enable random sampling\n",
    "    repetition_penalty=1.2 # discourage repeating words\n",
    ")\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš™ï¸ 6. Experiment with Parameters\n",
    "\n",
    "- **`temperature`** â†’ randomness control (0.2 = conservative, 1.0 = creative)\n",
    "- **`top_k`** â†’ limits vocabulary choices to top *k* most likely words\n",
    "- **`top_p` (nucleus sampling)** â†’ selects words whose cumulative probability â‰¤ *p*\n",
    "\n",
    "Try changing these to see how the output changes!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "prompt = \"Once upon a time in a futuristic city, there was a robot who\"\n",
    "inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "outputs = model.generate(\n",
    "    inputs,\n",
    "    max_length=70,\n",
    "    temperature=1.0,\n",
    "    top_k=40,\n",
    "    top_p=0.9,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generating Multiple Variations\n",
    "\n",
    "You can generate multiple different texts from the same prompt by setting `num_return_sequences > 1`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "prompt = \"The future of education with AI is\"\n",
    "inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "outputs = model.generate(\n",
    "    inputs,\n",
    "    max_length=60,\n",
    "    temperature=0.9,\n",
    "    num_return_sequences=3,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "for i, output in enumerate(outputs):\n",
    "    print(f\"\\nGenerated Text {i+1}:\\n{'-'*40}\")\n",
    "    print(tokenizer.decode(output, skip_special_tokens=True))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "- **GPT** models are **decoder-only Transformers**, trained to predict the next word.\n",
    "- Hugging Face makes it easy to experiment with GPT-2 and beyond.\n",
    "- You can control creativity using sampling parameters.\n",
    "- Advanced GPT models (GPT-3, GPT-4, etc.) build on this foundation for chatbots, content writing, and reasoning.\n",
    "\n",
    "---\n",
    "**Next:** `15-Text_Summarization_with_Transformers.ipynb` â†’ Learn how to summarize text using pre-trained models like T5 or BART!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
