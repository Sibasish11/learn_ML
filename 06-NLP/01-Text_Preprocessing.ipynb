{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "Text preprocessing is a crucial step in any NLP pipeline. Real-world text is messy ‚Äî it may include **punctuation, numbers, emojis, URLs, and casing inconsistencies**. \n",
    "\n",
    "The goal of preprocessing is to **clean and normalize text** so it can be effectively analyzed by NLP models.\n",
    "\n",
    "---\n",
    "## Common Preprocessing Steps\n",
    "1. Lowercasing text\n",
    "2. Removing punctuation\n",
    "3. Removing stopwords (like 'and', 'the', 'is')\n",
    "4. Removing numbers and special characters\n",
    "5. Tokenizing text (splitting into words)\n",
    "6. Lemmatization or stemming\n",
    "\n",
    "---\n",
    "## üõ†Ô∏è Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßæ Sample Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"NLP is AWESOME!!! It's transforming how computers understand #language üí¨. Visit https://openai.com for more info!\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1Ô∏è‚É£: Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_lower = text.lower()\n",
    "print(text_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2Ô∏è‚É£: Remove URLs, Special Characters, and Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove URLs\n",
    "text_clean = re.sub(r'http\\S+|www\\S+', '', text_lower)\n",
    "\n",
    "# Remove hashtags, numbers, and punctuation\n",
    "text_clean = re.sub(r'[^a-z\\s]', '', text_clean)\n",
    "\n",
    "print(text_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3Ô∏è‚É£: Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(text_clean)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4Ô∏è‚É£: Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5Ô∏è‚É£: Lemmatization (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmas = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Final Cleaned Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = ' '.join(lemmas)\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìò Summary\n",
    "- Text preprocessing standardizes messy text data.\n",
    "- Common steps: **lowercasing, cleaning, tokenization, and stopword removal.**\n",
    "- Optional: **stemming or lemmatization** to reduce words to their base forms.\n",
    "\n",
    "---\n",
    "‚úÖ Next: Move to `02-Tokenization_and_Stopwords.ipynb` to learn about **different tokenization methods** and **stopword handling techniques**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
