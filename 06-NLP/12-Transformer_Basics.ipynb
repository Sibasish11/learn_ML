{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Basics\n",
    "\n",
    "The **Transformer architecture** revolutionized Natural Language Processing (NLP) by replacing recurrent models (RNNs and LSTMs) with a fully **attention-based mechanism**.\n",
    "\n",
    "Introduced in 2017 in the paper _“Attention Is All You Need”_, it is now the foundation of powerful models like **BERT**, **GPT**, **T5**, and **Vision Transformers (ViT)**.\n",
    "\n",
    "---\n",
    "\n",
    "### Objectives\n",
    "- Understand the core components of the Transformer.\n",
    "- Learn how Attention works.\n",
    "- Implement a simple Transformer model using PyTorch.\n",
    "- Visualize how input sequences are transformed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Transformer Architecture Overview\n",
    "\n",
    "The Transformer consists of two main parts:\n",
    "\n",
    "```\n",
    "Input → [Encoder Stack] → [Decoder Stack] → Output\n",
    "```\n",
    "\n",
    "### Components\n",
    "| Component | Function |\n",
    "|------------|-----------|\n",
    "| **Encoder** | Reads and encodes input text into contextual representations. |\n",
    "| **Decoder** | Generates translated or predicted text using encoder outputs. |\n",
    "| **Attention Mechanism** | Helps the model focus on important words. |\n",
    "| **Feed Forward Network** | Adds non-linearity and complexity. |\n",
    "| **Positional Encoding** | Adds information about word order (since there’s no recurrence). |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Self-Attention\n",
    "\n",
    "Each word attends to every other word in the sentence to understand the full context.\n",
    "\n",
    "The **Self-Attention** mechanism calculates three vectors for every word:\n",
    "- **Q (Query)**\n",
    "- **K (Key)**\n",
    "- **V (Value)**\n",
    "\n",
    "The formula for attention is:\n",
    "\n",
    "$$ Attention(Q, K, V) = Softmax\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$\n",
    "\n",
    "where $d_k$ is the dimension of the Key vector."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    d_k = Q.size(-1)\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / (d_k ** 0.5)\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    return output, attention_weights\n",
    "\n",
    "# Example tensors\n",
    "Q = torch.rand(1, 3, 4)\n",
    "K = torch.rand(1, 3, 4)\n",
    "V = torch.rand(1, 3, 4)\n",
    "\n",
    "output, attn = scaled_dot_product_attention(Q, K, V)\n",
    "print(\"Attention Output:\\n\", output)\n",
    "print(\"\\nAttention Weights:\\n\", attn)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multi-Head Attention\n",
    "Instead of one attention head, the Transformer uses multiple heads to learn **different relationships** between words in parallel.\n",
    "\n",
    "This allows the model to attend to different parts of the sentence simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from torch import nn\n",
    "\n",
    "multihead_attn = nn.MultiheadAttention(embed_dim=8, num_heads=2, batch_first=True)\n",
    "\n",
    "x = torch.rand(2, 5, 8)  # batch_size=2, seq_len=5, embedding_dim=8\n",
    "attn_output, attn_weights = multihead_attn(x, x, x)\n",
    "\n",
    "print(\"Multi-Head Attention Output Shape:\", attn_output.shape)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚙️ 4. Building a Simple Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class SimpleTransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_hidden_dim, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        x = self.norm1(x + attn_output)\n",
    "        ff_output = self.ff(x)\n",
    "        x = self.norm2(x + ff_output)\n",
    "        return x\n",
    "\n",
    "# Test the block\n",
    "block = SimpleTransformerBlock(embed_dim=8, num_heads=2, ff_hidden_dim=32)\n",
    "inp = torch.rand(1, 5, 8)\n",
    "out = block(inp)\n",
    "print(\"Transformer Block Output Shape:\", out.shape)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Positional Encoding\n",
    "Since Transformers don’t have recurrence, we add **positional encodings** to represent word order.\n",
    "\n",
    "A simple sinusoidal encoding can be used to embed positions into vectors."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import math\n",
    "\n",
    "def positional_encoding(seq_len, d_model):\n",
    "    PE = torch.zeros(seq_len, d_model)\n",
    "    for pos in range(seq_len):\n",
    "        for i in range(0, d_model, 2):\n",
    "            PE[pos, i] = math.sin(pos / (10000 ** (i / d_model)))\n",
    "            if i + 1 < d_model:\n",
    "                PE[pos, i + 1] = math.cos(pos / (10000 ** ((i + 1) / d_model)))\n",
    "    return PE\n",
    "\n",
    "pos_encoding = positional_encoding(10, 8)\n",
    "print(pos_encoding[:5])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- Transformers rely entirely on **self-attention** (no recurrence).\n",
    "- **Encoder** extracts meaning; **Decoder** generates text.\n",
    "- **Multi-head attention** allows the model to focus on multiple parts of the sequence.\n",
    "- **Positional encodings** retain order information.\n",
    "\n",
    "Modern NLP models (like **BERT**, **GPT**, and **T5**) are built on top of this architecture.\n",
    "\n",
    "---\n",
    "**Next:** `13-BERT_Text_Classification.ipynb` → Understanding how BERT uses Transformers for downstream NLP tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
