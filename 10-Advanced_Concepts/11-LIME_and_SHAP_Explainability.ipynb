{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIME and SHAP Explainability\n",
    "\n",
    "In this notebook, we explore two of the most popular tools for model explainability:\n",
    "\n",
    "- **LIME (Local Interpretable Model-agnostic Explanations)**\n",
    "- **SHAP (SHapley Additive exPlanations)**\n",
    "\n",
    "These techniques help us understand **why** a machine learning model makes specific predictions, making AI systems more transparent and trustworthy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Objective\n",
    "- Understand the core idea behind LIME and SHAP.\n",
    "- Apply both methods on a classification model.\n",
    "- Compare their interpretability outputs visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lime.lime_tabular import LimeTabularExplainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üå∏ Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_iris()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç LIME Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = LimeTabularExplainer(\n",
    "    X_train.values,\n",
    "    feature_names=X_train.columns,\n",
    "    class_names=data.target_names,\n",
    "    mode='classification'\n",
    ")\n",
    "\n",
    "sample_index = 0\n",
    "exp = explainer.explain_instance(X_test.iloc[sample_index].values, model.predict_proba, num_features=4)\n",
    "exp.show_in_notebook(show_table=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIME builds a **local interpretable model** around a specific prediction to explain what features contributed the most to that decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è SHAP Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer_shap = shap.TreeExplainer(model)\n",
    "shap_values = explainer_shap.shap_values(X_test)\n",
    "\n",
    "# Display summary plot\n",
    "shap.summary_plot(shap_values[1], X_test, feature_names=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SHAP values assign each feature an **importance value** for a given prediction based on game theory (Shapley values). It helps explain both local and global feature impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Comparison: LIME vs SHAP\n",
    "\n",
    "| Criteria | LIME | SHAP |\n",
    "|-----------|------|------|\n",
    "| **Type** | Local explanation | Local + Global |\n",
    "| **Theory** | Linear approximation | Game-theoretic Shapley values |\n",
    "| **Speed** | Faster for small samples | Slower for large models |\n",
    "| **Interpretability** | Easy, approximate | Theoretically solid |\n",
    "| **Model Support** | Any model | Model-specific (optimized for trees, deep nets) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß© Key Takeaways\n",
    "- LIME gives **local** insights into model decisions.\n",
    "- SHAP provides both **local and global** interpretability.\n",
    "- SHAP is more **mathematically grounded**, while LIME is more **intuitive**.\n",
    "\n",
    "Both are valuable for **building trust** in AI models and ensuring **ethical transparency** in ML deployments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
