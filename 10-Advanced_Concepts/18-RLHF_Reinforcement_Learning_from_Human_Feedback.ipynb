{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning from Human Feedback (RLHF)\n",
    "\n",
    "**RLHF** stands for **Reinforcement Learning from Human Feedback**, a powerful method used to align AI models with human values and preferences.\n",
    "\n",
    "It is a key component in training models like **ChatGPT**, **Claude**, and **Gemini** : ensuring that their responses are not only factually accurate but also *helpful, harmless, and honest*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "- Understand what RLHF is and why it matters\n",
    "- Learn the three main stages of RLHF training\n",
    "- Explore how human feedback improves model behavior\n",
    "- See a simple simulation of preference-based fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧩 1. What is RLHF?\n",
    "\n",
    "RLHF bridges the gap between raw model pre training and real world helpfulness.\n",
    "\n",
    "It involves **human evaluators** rating model outputs and using those ratings to train a *reward model*, which guides further reinforcement learning fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Three Stages of RLHF:\n",
    "\n",
    "1. **Supervised Fine-Tuning (SFT):**\n",
    "   - Human-labeled examples are used to fine-tune the pre-trained language model.\n",
    "   - The model learns to produce more coherent and contextually relevant outputs.\n",
    "\n",
    "2. **Reward Model Training:**\n",
    "   - Multiple model responses are generated for the same prompt.\n",
    "   - Humans rank the responses from best to worst.\n",
    "   - A *reward model* is trained to predict these human preferences.\n",
    "\n",
    "3. **Reinforcement Learning (PPO Optimization):**\n",
    "   - The base model is fine tuned again using reinforcement learning (typically **Proximal Policy Optimization, PPO**).\n",
    "   - The reward model provides feedback on each response to encourage human-like answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚙️ 2. Conceptual Overview of the RLHF Pipeline\n",
    "\n",
    "```text\n",
    "          ┌─────────────────────────────────────┐\n",
    "          │        Pre-trained Language Model    │\n",
    "          └─────────────────────────────────────┘\n",
    "                           │\n",
    "                           ▼\n",
    "          ┌─────────────────────────────────────┐\n",
    "          │  Supervised Fine-Tuning (SFT)       │\n",
    "          │  → Trained on human-written answers │\n",
    "          └─────────────────────────────────────┘\n",
    "                           │\n",
    "                           ▼\n",
    "          ┌─────────────────────────────────────┐\n",
    "          │   Reward Model Training             │\n",
    "          │   → Learns from ranked responses    │\n",
    "          └─────────────────────────────────────┘\n",
    "                           │\n",
    "                           ▼\n",
    "          ┌─────────────────────────────────────┐\n",
    "          │ Reinforcement Learning (PPO)        │\n",
    "          │ → Fine-tunes with reward feedback   │\n",
    "          └─────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧠 Example: Simulating preference-based training\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "prompts = [\"Explain quantum computing in simple terms.\", \"Why is AI alignment important?\"]\n",
    "responses = [\n",
    "    [\"Quantum computing uses qubits that can be 0 and 1 at the same time.\", \"Quantum computers are very fast because they use magic.\"],\n",
    "    [\"AI alignment ensures models act according to human goals.\", \"AI alignment is about making robots friendly.\"]\n",
    "]\n",
    "\n",
    "# Human feedback (preferred = index 0 in both cases)\n",
    "human_prefs = [0, 0]\n",
    "\n",
    "reward_scores = []\n",
    "for i, pref in enumerate(human_prefs):\n",
    "    r = np.zeros(len(responses[i]))\n",
    "    r[pref] = 1.0  # assign reward to preferred response\n",
    "    reward_scores.append(r)\n",
    "\n",
    "reward_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧮 3. PPO – Reinforcement Learning Fine-Tuning\n",
    "\n",
    "In the last step, **PPO (Proximal Policy Optimization)** adjusts the model’s weights to maximize the *expected reward* predicted by the reward model.\n",
    "\n",
    "PPO balances *exploration* (trying new outputs) and *stability* (staying close to good behavior)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified PPO objective (conceptual)\n",
    "import torch\n",
    "\n",
    "old_log_probs = torch.tensor([0.2, 0.25])\n",
    "new_log_probs = torch.tensor([0.3, 0.28])\n",
    "advantages = torch.tensor([1.0, 0.5])\n",
    "\n",
    "ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "ppo_loss = -torch.min(ratio * advantages, torch.clamp(ratio, 0.8, 1.2) * advantages).mean()\n",
    "ppo_loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💡 4. Benefits of RLHF\n",
    "\n",
    "✅ **Improves alignment** with human goals and values  \n",
    "✅ **Reduces toxic or unsafe outputs**  \n",
    "✅ **Enhances usefulness and tone** of responses  \n",
    "✅ **Adapts to evolving user expectations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚠️ 5. Challenges and Limitations\n",
    "\n",
    "- **Expensive and time-consuming:** Human feedback collection is costly.\n",
    "- **Bias amplification:** Feedback data may reflect annotator bias.\n",
    "- **Reward hacking:** Model may exploit shortcuts to get higher rewards.\n",
    "- **Scalability issues:** Requires massive compute and annotation pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧭 6. Future Directions\n",
    "\n",
    "- **RLAIF (Reinforcement Learning from AI Feedback):** AI-generated synthetic feedback reduces reliance on humans.\n",
    "- **Constitutional AI:** Models follow a set of written principles instead of manual feedback.\n",
    "- **Direct Preference Optimization (DPO):** A simpler alternative to PPO that directly optimizes preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 7. Key Takeaways\n",
    "\n",
    "- RLHF aligns models with human expectations using reward-based learning.\n",
    "- It combines supervised learning, preference modeling, and reinforcement learning.\n",
    "- Used extensively in GPT-4, Claude, and Gemini training.\n",
    "- Evolving toward AI-assisted feedback and automated preference optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📘 References\n",
    "- OpenAI: [Training Language Models to Follow Instructions with Human Feedback (2022)](https://arxiv.org/abs/2203.02155)\n",
    "- Anthropic: [Constitutional AI: Harmlessness from AI Feedback](https://www.anthropic.com/news/constitutional-ai)\n",
    "- DeepMind: [Reward Modeling and Alignment Research](https://deepmind.google/discover/blog/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
