{
 "nbformat": 5,
 "nbformat_minor": 1,
 "metadata": {
  "colab": {
   "name": "04-Model_Agnostic_Meta_Learning_MAML.ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Agnostic Meta Learning (MAML)\n",
    "\n",
    "**Model Agnostic Meta-Learning (MAML)** is one of the most influential algorithms in meta-learning. It allows models to learn parameters that can be **fine-tuned to new tasks** with just a few gradient steps.\n",
    "\n",
    "In simple words: MAML helps models *learn how to learn* quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìò 1. Motivation\n",
    "\n",
    "Instead of training a separate model for every task, we want a model that can **adapt** to a new task using a small amount of data. MAML achieves this by optimizing the model parameters for **fast adaptation**.\n",
    "\n",
    "üí° Example: A model trained with MAML on various classification tasks can quickly adapt to a **new unseen class** with just a few samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 2. MAML Workflow\n",
    "\n",
    "The training involves two nested loops:\n",
    "\n",
    "1. **Inner Loop:** Update model parameters on each task using a few steps of gradient descent.\n",
    "2. **Outer Loop:** Update the *meta-parameters* using gradients from all tasks after their inner-loop updates.\n",
    "\n",
    "This way, the outer loop learns parameters that are *sensitive* to small changes enabling fast learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß© 3. Mathematical Formulation\n",
    "\n",
    "Let \\( \\theta \\) be the model parameters and \\( T_i \\) be a task with loss \\( L_{T_i}(\\theta) \\).\n",
    "\n",
    "- Inner Loop (Task-specific adaptation):\n",
    "  \\[ \\theta'_i = \\theta - \\alpha \\nabla_\\theta L_{T_i}(\\theta) \\]\n",
    "\n",
    "- Outer Loop (Meta-update):\n",
    "  \\[ \\theta \\leftarrow \\theta - \\beta \\nabla_\\theta \\sum_i L_{T_i}(\\theta'_i) \\]\n",
    "\n",
    "where \\( \\alpha \\) is the inner learning rate and \\( \\beta \\) is the meta learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß± 4. Implementation (PyTorch Example)\n",
    "\n",
    "We'll create a simple model and a minimalistic version of MAML training loop."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "class MAMLNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(1, 40)\n",
    "        self.fc2 = nn.Linear(40, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inner Loop: Task-Specific Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def inner_update(model, loss_fn, x, y, lr=0.01):\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    grads = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n",
    "    updated_params = [p - lr * g for p, g in zip(model.parameters(), grads)]\n",
    "    return updated_params"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outer Loop: Meta-Update"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def maml_train(model, tasks, loss_fn, lr_inner=0.01, lr_outer=0.001, epochs=3):\n",
    "    optimizer = Adam(model.parameters(), lr=lr_outer)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        meta_loss = 0.0\n",
    "        for x_train, y_train, x_val, y_val in tasks:\n",
    "            adapted_params = inner_update(model, loss_fn, x_train, y_train, lr_inner)\n",
    "            \n",
    "            y_pred_val = model(x_val)\n",
    "            loss_val = loss_fn(y_pred_val, y_val)\n",
    "            meta_loss += loss_val\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        meta_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Meta Loss: {meta_loss.item():.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† 5. Key Insights\n",
    "\n",
    "- MAML doesn‚Äôt depend on a specific model architecture ‚Äî it‚Äôs **model-agnostic**.\n",
    "- It can be used for **supervised**, **reinforcement**, or **unsupervised** tasks.\n",
    "- Learns initialization weights that are optimal for *fast adaptation*.\n",
    "- Requires **second-order derivatives**, which can be computationally expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° 6. Variants of MAML\n",
    "\n",
    "- **First-Order MAML (FOMAML):** Approximates gradients by ignoring second-order terms.\n",
    "- **Reptile:** Simplified optimization-based meta-learning without second-order derivatives.\n",
    "- **Meta-SGD:** Learns learning rates along with parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° 7. Real-World Applications\n",
    "\n",
    "- Few-shot image recognition.\n",
    "- Robotics: adapting to new environments.\n",
    "- Healthcare: adapting to new patient data.\n",
    "- Reinforcement learning agents that generalize across tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ 8. Summary\n",
    "\n",
    "- **MAML** is a cornerstone of modern meta-learning.\n",
    "- It finds an initialization that enables fast learning on new tasks.\n",
    "- Though computationally heavy, its principles inspire many later algorithms.\n",
    "\n",
    "‚û°Ô∏è Next: `05-Few_Shot_Learning_with_MAML.ipynb`"
   ]
  }
 ]
}
