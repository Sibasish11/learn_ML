{
 "nbformat": 5,
 "nbformat_minor": 1,
 "metadata": {
  "colab": {
   "name": "03-Meta_Learning_Basics.ipynb"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta Learning Basics\n",
    "\n",
    "**Meta Learning**, or *learning to learn*, focuses on building models that can quickly adapt to new tasks using only a few examples. It’s a crucial idea behind **few shot learning** and advanced AI systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Meta Learning?\n",
    "\n",
    "Traditional machine learning trains a model to perform *one* task (e.g., classify images of cats and dogs). In contrast, **meta-learning** trains a model to **learn how to learn new tasks efficiently**.\n",
    "\n",
    "It’s often framed as a *bi-level optimization* problem:\n",
    "\n",
    "- **Inner Loop:** Learn task-specific parameters.\n",
    "- **Outer Loop:** Update meta-parameters to generalize across tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "Humans can learn new tasks quickly from very few examples. For instance, you can learn to recognize a new object after seeing it just once. Meta-learning aims to enable such **rapid generalization** in neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Meta-Learning Approaches\n",
    "\n",
    "Meta-learning methods generally fall into three main categories:\n",
    "\n",
    "1. **Metric-Based:** Learn a similarity function between examples (e.g., **Siamese Networks**, **Prototypical Networks**).\n",
    "2. **Model-Based:** Use architectures that can adapt quickly (e.g., **Memory-Augmented Neural Networks**, **LSTMs as meta-learners**).\n",
    "3. **Optimization-Based:** Learn better learning algorithms (e.g., **MAML** – Model-Agnostic Meta-Learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Model-Agnostic Meta-Learning (MAML)\n",
    "\n",
    "Let’s understand the idea of **MAML** — one of the most popular meta-learning algorithms.\n",
    "\n",
    "### Core Idea\n",
    "Train a model’s parameters such that it can **quickly adapt** to new tasks with just a few gradient steps."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(1, 40)\n",
    "        self.fc2 = nn.Linear(40, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inner Loop: Task-Specific Adaptation\n",
    "\n",
    "We perform a few gradient steps on each task to adapt the model’s parameters locally."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def inner_loop(model, loss_fn, x, y, lr=0.01):\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    grads = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n",
    "\n",
    "    adapted_params = [p - lr * g for p, g in zip(model.parameters(), grads)]\n",
    "    return adapted_params"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outer Loop: Meta-Update\n",
    "\n",
    "The outer loop updates the model so that it can quickly adapt across tasks."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def meta_update(model, tasks, loss_fn, lr_inner=0.01, lr_outer=0.001):\n",
    "    optimizer = Adam(model.parameters(), lr=lr_outer)\n",
    "    \n",
    "    for epoch in range(3):  # demonstration only\n",
    "        meta_loss = 0\n",
    "        for x_train, y_train, x_val, y_val in tasks:\n",
    "            adapted_params = inner_loop(model, loss_fn, x_train, y_train, lr_inner)\n",
    "            \n",
    "            # Compute validation loss using adapted parameters\n",
    "            y_pred_val = model.forward(x_val)\n",
    "            loss_val = loss_fn(y_pred_val, y_val)\n",
    "            meta_loss += loss_val\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        meta_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Meta Loss: {meta_loss.item():.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Meta-Learning Matters\n",
    "\n",
    "- Enables **few shot** and **zero shot** learning.\n",
    "- Allows **transfer** of knowledge across tasks.\n",
    "- Reduces dependency on large labeled datasets.\n",
    "- Inspires general purpose intelligent agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real World Applications\n",
    "\n",
    "- Personalized recommendation systems.\n",
    "- Rapid adaptation in robotics.\n",
    "- Medical imaging (adapting to new patients quickly).\n",
    "- Few-shot classification and reinforcement learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Meta-learning teaches models **how to learn**, not just **what to learn**. It provides a foundation for adaptive and general AI systems.\n",
    "\n",
    "Next → `04-Few_Shot_Learning_with_Prototypical_Networks.ipynb`"
   ]
  }
 ]
}
