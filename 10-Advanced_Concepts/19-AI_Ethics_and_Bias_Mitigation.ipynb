{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Ethics and Bias Mitigation\n",
    "\n",
    "**AI Ethics** refers to the moral principles and guidelines that govern how artificial intelligence systems are developed and used. It ensures AI benefits humanity while minimizing harm, discrimination, or misuse.\n",
    "\n",
    "**Bias Mitigation** focuses on reducing unfairness and discrimination in AI models arising from data or design flaws."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Learning Objectives\n",
    "\n",
    "After completing this notebook, you will:\n",
    "\n",
    "- Understand what ethical AI means and why it‚Äôs important\n",
    "- Identify sources and types of bias in AI systems\n",
    "- Learn techniques for bias detection and mitigation\n",
    "- Explore fairness metrics and responsible AI design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåç 1. Why AI Ethics Matter\n",
    "\n",
    "AI influences major sectors like healthcare, finance, education, and law enforcement. Unethical use can cause:\n",
    "\n",
    "- **Discrimination:** Models can unintentionally favor or disfavor groups.\n",
    "- **Privacy Violations:** Misuse of personal or sensitive data.\n",
    "- **Lack of Accountability:** Hard to determine responsibility when AI makes mistakes.\n",
    "- **Misinformation:** Generative models can spread false or harmful content.\n",
    "\n",
    "**Ethical AI ensures transparency, fairness, and accountability in all AI-driven decisions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß© 2. Types of AI Bias\n",
    "\n",
    "| Type | Description | Example |\n",
    "|------|--------------|----------|\n",
    "| **Data Bias** | Bias present in training data | A dataset with mostly male resumes leads to biased hiring model |\n",
    "| **Algorithmic Bias** | Bias introduced by model design or parameters | Certain features get more weight unfairly |\n",
    "| **Societal Bias** | Biases reflecting existing inequalities | Facial recognition less accurate for darker skin tones |\n",
    "| **Selection Bias** | When sample data doesn‚Äôt represent the population | Model trained on urban data fails for rural users |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† 3. Detecting Bias\n",
    "\n",
    "Before mitigation, bias must be detected through metrics and visualization.\n",
    "\n",
    "### Example Fairness Metrics:\n",
    "- **Statistical Parity:** Equal probability of favorable outcomes across groups.\n",
    "- **Equal Opportunity:** Equal true positive rates for all groups.\n",
    "- **Demographic Parity:** Output should be independent of sensitive attributes (e.g., gender, race).\n",
    "\n",
    "Let's see a simple simulation of bias detection in model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Example: gender based prediction outcomes\n",
    "data = pd.DataFrame({\n",
    "    'gender': ['M','F','M','F','M','F','M','F'],\n",
    "    'predicted': [1,0,1,0,1,0,1,1],  # 1 = approved, 0 = rejected\n",
    "    'actual': [1,0,1,1,0,0,1,1]\n",
    "})\n",
    "\n",
    "approval_rate = data.groupby('gender')['predicted'].mean()\n",
    "approval_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If one group (say females) has a significantly lower approval rate than another, it indicates **potential bias**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßÆ 4. Bias Mitigation Techniques\n",
    "\n",
    "**1Ô∏è‚É£ Pre-processing:** Modify or rebalance data before training.\n",
    "- Data augmentation\n",
    "- Reweighting or resampling\n",
    "- Removing sensitive attributes\n",
    "\n",
    "**2Ô∏è‚É£ In-processing:** Add fairness constraints during model training.\n",
    "- Adversarial debiasing\n",
    "- Fair regularization terms in loss function\n",
    "\n",
    "**3Ô∏è‚É£ Post-processing:** Adjust predictions after training.\n",
    "- Calibrating thresholds for different groups\n",
    "- Equalizing predicted outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: simple reweighting\n",
    "from sklearn.utils import compute_sample_weight\n",
    "\n",
    "weights = compute_sample_weight('balanced', y=data['predicted'], X=data['gender'])\n",
    "data['sample_weight'] = weights\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß≠ 5. Ethical AI Design Principles\n",
    "\n",
    "Organizations like **OECD**, **EU AI Act**, and **UNESCO** recommend the following principles:\n",
    "\n",
    "- **Transparency:** Models and data sources should be interpretable.\n",
    "- **Accountability:** Clear responsibility for AI decisions.\n",
    "- **Fairness:** Equal treatment and outcomes for all users.\n",
    "- **Privacy:** Protection of user data and consent.\n",
    "- **Safety & Security:** Robustness against misuse and attacks.\n",
    "- **Human Oversight:** Humans must stay in the decision loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí¨ 6. Case Studies\n",
    "\n",
    "**1. COMPAS Algorithm:** Used in U.S. criminal justice system, found biased against African-Americans.\n",
    "\n",
    "**2. Facial Recognition Systems:** Less accurate for darker-skinned and female faces, causing ethical concerns.\n",
    "\n",
    "**3. Resume Screening AI:** Penalized applications with women-associated words (e.g., ‚Äúwomen‚Äôs chess club‚Äù)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ 7. Responsible AI in Practice\n",
    "\n",
    "- Conduct **AI Ethics Audits** regularly\n",
    "- Maintain **Model Cards** documenting fairness and limitations\n",
    "- Include **diverse teams** in model design\n",
    "- Use **explainable AI (XAI)** tools like **LIME** or **SHAP** for interpretability\n",
    "- Implement **continuous monitoring** post-deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ 8. Key Takeaways\n",
    "\n",
    "- AI systems must be **fair, transparent, and accountable**.\n",
    "- Bias can enter at any stage ‚Äî data, model, or deployment.\n",
    "- Ethical AI design helps build **trustworthy and inclusive technology**.\n",
    "- Continuous evaluation and stakeholder feedback are vital for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö References\n",
    "- [Google Responsible AI Practices](https://ai.google/responsibilities/responsible-ai-practices/)\n",
    "- [EU AI Act Summary](https://artificialintelligenceact.eu/)\n",
    "- [Fairlearn Toolkit (Microsoft)](https://fairlearn.org/)\n",
    "- [AI Ethics Guidelines Global Inventory ‚Äì OECD](https://oecd.ai/en/dashboards/ai-principles)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
