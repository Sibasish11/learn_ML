{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainable AI (XAI) : Basics\n",
    "\n",
    "Modern AI models, especially deep neural networks, often act as *black boxes*. **Explainable AI (XAI)** helps us understand **why** a model made a certain decision.\n",
    "\n",
    "In this notebook, we explore the motivation, types, and practical examples of explainability techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Learning Objectives\n",
    "- Understand what Explainable AI means and why it‚Äôs needed.\n",
    "- Learn about *global* and *local* explainability techniques.\n",
    "- Explore SHAP and LIME for model interpretation.\n",
    "- Visualize feature importance for model decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß© 1. Why Explainability Matters\n",
    "\n",
    "AI systems are increasingly used in high stakes areas like healthcare, finance, and law. Lack of transparency can lead to **bias, mistrust, and ethical issues.**\n",
    "\n",
    "Explainability helps in:\n",
    "- **Trust:** Users understand and accept AI decisions.\n",
    "- **Accountability:** Regulators can audit AI systems.\n",
    "- **Debugging:** Data scientists can identify data or model issues.\n",
    "- **Fairness:** Helps ensure model decisions aren‚Äôt biased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† 2. Types of Explainability\n",
    "\n",
    "### üîπ Global Explainability\n",
    "- Focuses on understanding the model as a whole.\n",
    "- Example: Feature importance in a random forest.\n",
    "\n",
    "### üîπ Local Explainability\n",
    "- Explains a single prediction.\n",
    "- Example: Why did the model classify this image as a ‚Äòcat‚Äô?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 3. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('‚úÖ Libraries loaded successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 4. Sample Dataset ‚Äî Breast Cancer Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(f'Model Accuracy: {accuracy_score(y_test, y_pred):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç 5. SHAP (SHapley Additive exPlanations)\n",
    "\n",
    "**SHAP** values show how each feature contributes (positively or negatively) to a specific prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Summary plot for global explainability\n",
    "shap.summary_plot(shap_values[1], X_test, plot_type='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üåç Global View ‚Äî Feature Importance\n",
    "The above plot shows which features are most important **overall** in the model‚Äôs decision process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîé Local Explanation Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = 10\n",
    "shap.force_plot(explainer.expected_value[1], shap_values[1][sample_index, :], X_test.iloc[sample_index, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This explains why the model predicted a particular class for one specific data instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° 6. LIME (Local Interpretable Model-agnostic Explanations)\n",
    "LIME approximates the local decision boundary using a simple, interpretable model like linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime import lime_tabular\n",
    "\n",
    "explainer_lime = lime_tabular.LimeTabularExplainer(\n",
    "    training_data=np.array(X_train),\n",
    "    feature_names=X_train.columns,\n",
    "    class_names=data.target_names,\n",
    "    mode='classification'\n",
    ")\n",
    "\n",
    "exp = explainer_lime.explain_instance(X_test.iloc[0].values, model.predict_proba)\n",
    "exp.show_in_notebook(show_table=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß≠ 7. Key Takeaways\n",
    "- **SHAP** is based on game theory and provides consistent feature attributions.\n",
    "- **LIME** explains single predictions using simple local models.\n",
    "- Explainability improves trust, fairness, and accountability.\n",
    "- Trade-off exists between model performance and interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "- Explore **Integrated Gradients** and **Grad-CAM** for deep networks.\n",
    "- Apply XAI to real-world domains (e.g., healthcare, credit scoring).\n",
    "- Learn about **Fairness-Aware ML** and bias detection techniques."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
