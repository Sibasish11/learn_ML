{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundation Models and Large Language Models (LLMs)\n",
    "\n",
    "In this notebook, we will explore the evolution, architecture, and applications of **Foundation Models (FMs)** and **Large Language Models (LLMs)** : the backbone of modern generative AI systems such as GPT, BERT, Claude, and Gemini."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "- Understand what Foundation Models and LLMs are\n",
    "- Learn about the Transformer architecture\n",
    "- Explore pre-training and fine-tuning processes\n",
    "- Understand key LLMs like BERT, GPT, and T5\n",
    "- Experiment with text generation and embeddings using Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß© 1. What are Foundation Models?\n",
    "\n",
    "**Foundation Models (FMs)** are large-scale AI models trained on vast datasets and designed to serve as general-purpose models that can be fine-tuned for various downstream tasks.\n",
    "\n",
    "### Key Characteristics:\n",
    "- Trained on massive, diverse datasets\n",
    "- Use self supervised learning (predict missing tokens or words)\n",
    "- Adaptable across modalities (text, image, audio, video)\n",
    "- Serve as a **base for fine-tuning specific applications**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of Foundation Models:\n",
    "- **GPT series (OpenAI):** Text generation and reasoning\n",
    "- **BERT (Google):** Text understanding and embeddings\n",
    "- **CLIP (OpenAI):** Image‚Äìtext alignment\n",
    "- **PaLM / Gemini (Google DeepMind):** Multimodal reasoning\n",
    "- **LLaMA / Mistral (Meta):** Open-source LLMs for research and development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 2. The Transformer Architecture\n",
    "\n",
    "Introduced in *‚ÄúAttention is All You Need‚Äù (Vaswani et al., 2017)*, the **Transformer** forms the basis of all modern large language models.\n",
    "\n",
    "### Key Components:\n",
    "- **Self-Attention:** Lets each token attend to others dynamically\n",
    "- **Multi-Head Attention:** Captures relationships from multiple perspectives\n",
    "- **Positional Encoding:** Adds order information to token embeddings\n",
    "- **Feed-Forward Layers:** Apply transformations to contextualized embeddings\n",
    "- **Layer Normalization & Residuals:** Stabilize deep network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of transformer attention (simplified pseudocode)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def simple_self_attention(x):\n",
    "    attn_weights = F.softmax(x @ x.T / (x.shape[-1] ** 0.5), dim=-1)\n",
    "    return attn_weights @ x\n",
    "\n",
    "x = torch.randn(4, 8)  # 4 tokens, 8-dim embeddings\n",
    "context = simple_self_attention(x)\n",
    "context.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† 3. Large Language Models (LLMs)\n",
    "\n",
    "**LLMs** are foundation models specialized for natural language understanding and generation. They are trained using **self-supervised learning** on enormous text corpora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß© Popular LLM Architectures:\n",
    "\n",
    "| Model | Type | Objective | Key Use |\n",
    "|--------|------|------------|----------|\n",
    "| **BERT** | Encoder | Masked Language Modeling | Text understanding |\n",
    "| **GPT (1‚Äì4)** | Decoder | Next Token Prediction | Text generation |\n",
    "| **T5** | Encoder‚ÄìDecoder | Text-to-Text | Translation, Summarization |\n",
    "| **PaLM / Gemini** | Multimodal | Reasoning | Text, Code, Image Understanding |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìò 4. Pre-training and Fine-tuning\n",
    "\n",
    "**Pre-training:** Models learn general knowledge by predicting missing or next tokens on large, unlabelled datasets.\n",
    "\n",
    "**Fine-tuning:** Models adapt to specific downstream tasks using smaller, labeled datasets.\n",
    "\n",
    "Example: GPT pre-trained on the internet, fine-tuned for chatbots or summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Using Hugging Face Transformers for text generation\n",
    "from transformers import pipeline\n",
    "\n",
    "# generator = pipeline('text-generation', model='gpt2')\n",
    "# output = generator('AI will transform education by', max_length=50, num_return_sequences=1)\n",
    "# print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç 5. Understanding Context Length and Scaling Laws\n",
    "\n",
    "As models scale in **parameters** and **training data**, their performance improves predictably (known as *scaling laws*).\n",
    "\n",
    "- **Context length** defines how many tokens a model can process in one go.\n",
    "- GPT-4 Turbo can handle up to 128k tokens, enabling long-context reasoning.\n",
    "- Larger models can store more knowledge but are expensive to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßÆ 6. Embeddings and Representation Learning\n",
    "\n",
    "LLMs produce **embeddings**, numerical representations of words, phrases, or sentences. These embeddings capture semantic meaning and can be used for:\n",
    "- Search and retrieval\n",
    "- Clustering and similarity detection\n",
    "- Recommendation systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Getting text embeddings\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "# embeddings = model.encode(['AI is amazing!', 'Machine learning is powerful.'])\n",
    "# print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåç 7. Ethical and Societal Considerations\n",
    "\n",
    "Foundation and LLMs raise important concerns:\n",
    "\n",
    "- **Bias and fairness:** Models may reflect societal biases in their training data.\n",
    "- **Privacy:** Sensitive information can appear in generated outputs.\n",
    "- **Environmental impact:** Training LLMs consumes huge energy resources.\n",
    "- **Misinformation risks:** Generative text can spread false or misleading information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ 8. Key Takeaways\n",
    "\n",
    "- Foundation Models form the basis for most modern AI systems.\n",
    "- Transformers power all state-of-the-art LLMs.\n",
    "- LLMs are trained via self-supervision, scaled massively, and fine-tuned for specific tasks.\n",
    "- Embeddings allow models to understand semantic relationships.\n",
    "- Ethical and responsible AI use is essential for deploying LLMs safely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÆ 9. What‚Äôs Next?\n",
    "\n",
    "Next steps in this module include:\n",
    "- Exploring **multimodal foundation models** (text + image + audio)\n",
    "- Understanding **prompt engineering** for controlling model outputs\n",
    "- Building **LLM-powered pipelines** with open-source models like LLaMA and Mistral"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
