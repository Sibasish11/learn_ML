{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Learning Basics\n",
    "\n",
    "In this notebook, we introduce **Federated Learning (FL)** : a modern approach that allows multiple clients (devices, institutions, or nodes) to collaboratively train a shared machine learning model **without sharing their raw data**.\n",
    "\n",
    "This paradigm enables **data privacy, security, and decentralized computation**, which is critical in healthcare, finance, and edge AI applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Learning Objectives\n",
    "- Understand the motivation and concept of Federated Learning\n",
    "- Learn the architecture of a federated learning system\n",
    "- Explore **Federated Averaging (FedAvg)** algorithm\n",
    "- Implement a simple simulation of Federated Learning using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is Federated Learning?\n",
    "\n",
    "**Federated Learning** allows training of ML models across multiple devices or servers that hold local datasets, **without centralizing the data**.\n",
    "\n",
    "Each client (e.g., smartphone, hospital, IoT device) trains the model locally on its data and only shares **model parameters (weights)** with a central server. The server then aggregates these updates to form a global model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèóÔ∏è Federated Learning Architecture\n",
    "1. **Server initializes a global model.**\n",
    "2. **Clients train** the model locally on their private data.\n",
    "3. **Clients send** updated model weights to the server.\n",
    "4. The **server aggregates** updates (e.g., via averaging).\n",
    "5. Repeat the process for multiple rounds until convergence.\n",
    "\n",
    "![Federated Learning Workflow](https://upload.wikimedia.org/wikipedia/commons/e/e2/Federated_learning_process.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 2. Federated Averaging (FedAvg) Algorithm\n",
    "\n",
    "The **FedAvg** algorithm is the cornerstone of most FL systems. It computes a **weighted average** of local model updates:\n",
    "\n",
    "$$ w_{t+1} = \\sum_{k=1}^{K} \\frac{n_k}{n} w_{t+1}^{(k)} $$\n",
    "\n",
    "Where:\n",
    "- $w_{t+1}^{(k)}$ ‚Üí local model weights from client k\n",
    "- $n_k$ ‚Üí number of samples at client k\n",
    "- $n = \\sum_k n_k$ ‚Üí total samples across all clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† 3. Simulating Federated Learning with PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import copy\n",
    "\n",
    "# Simple model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Dataset and data partitioning\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "client_data = random_split(train_data, [12000, 12000, 12000, 12000, 6000])  # 5 clients\n",
    "\n",
    "def train_local(model, data, epochs=1):\n",
    "    loader = DataLoader(data, batch_size=64, shuffle=True)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    model.train()\n",
    "    for _ in range(epochs):\n",
    "        for X, y in loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(X), y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return model.state_dict()\n",
    "\n",
    "def average_weights(client_weights):\n",
    "    avg_weights = copy.deepcopy(client_weights[0])\n",
    "    for key in avg_weights.keys():\n",
    "        for i in range(1, len(client_weights)):\n",
    "            avg_weights[key] += client_weights[i][key]\n",
    "        avg_weights[key] = torch.div(avg_weights[key], len(client_weights))\n",
    "    return avg_weights\n",
    "\n",
    "# Federated training simulation\n",
    "global_model = Net()\n",
    "for round in range(3):\n",
    "    local_weights = []\n",
    "    for client in client_data:\n",
    "        local_model = Net()\n",
    "        local_model.load_state_dict(global_model.state_dict())\n",
    "        client_update = train_local(local_model, client)\n",
    "        local_weights.append(client_update)\n",
    "    global_weights = average_weights(local_weights)\n",
    "    global_model.load_state_dict(global_weights)\n",
    "    print(f\"Round {round+1} complete ‚úÖ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîí 4. Advantages of Federated Learning\n",
    "- **Privacy preservation** ‚Äî data never leaves the client.\n",
    "- **Reduced data transfer** ‚Äî only weights are communicated.\n",
    "- **Collaboration** across organizations without data sharing.\n",
    "- **Scalable** ‚Äî can include many clients/devices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è 5. Challenges in Federated Learning\n",
    "- **Non-IID data**: clients may have very different data distributions.\n",
    "- **Communication cost** between clients and server.\n",
    "- **Privacy leaks** from model updates.\n",
    "- **Device heterogeneity** in compute and storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß≠ 6. Summary\n",
    "- Federated Learning trains models collaboratively without centralizing data.\n",
    "- The **FedAvg** algorithm aggregates local model updates.\n",
    "- Offers privacy-preserving machine learning at scale.\n",
    "- Widely applied in healthcare, mobile AI, and finance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Next Notebook: `07-Privacy_Preserving_ML.ipynb`\n",
    "In the next notebook, we‚Äôll explore **privacy-preserving ML** approaches such as **Differential Privacy** and **Secure Multi-party Computation**, which further enhance the trustworthiness of Federated Learning systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
