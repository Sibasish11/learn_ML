{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üå∏ Federated Learning with Flower (FLwr)\n",
    "\n",
    "In this notebook, we‚Äôll explore how to implement **Federated Learning (FL)** using the [**Flower**](https://flower.dev/) framework.\n",
    "\n",
    "Flower (FLwr) provides a simple yet powerful interface for simulating and deploying FL systems : ideal for experiments and production grade federated systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Learning Objectives\n",
    "- Understand the **Flower FL architecture**\n",
    "- Learn how to build a **client-server setup** for Federated Learning\n",
    "- Train a simple **MNIST model** using multiple clients\n",
    "- Observe how Flower handles **aggregation and orchestration** automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† 1. What is Flower (FLwr)?\n",
    "\n",
    "**Flower** is an open-source framework that makes federated learning accessible to everyone. It abstracts away the communication, orchestration, and aggregation details so that you can focus on model logic.\n",
    "\n",
    "![Flower Architecture](https://flower.dev/images/overview/flower-architecture.png)\n",
    "\n",
    "Each **client** trains locally on its data and communicates with a **server**, which performs aggregation (usually via Federated Averaging)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üå∏ 2. Installing Dependencies\n",
    "# Uncomment to install Flower (if not already installed)\n",
    "# !pip install flwr torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 3. Setting Up the Model and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import flwr as fl\n",
    "\n",
    "# Simple CNN model for MNIST\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
    "\n",
    "# Split data among clients\n",
    "client_datasets = random_split(train_dataset, [12000, 12000, 12000, 12000, 6000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ 4. Define the Flower Client\n",
    "\n",
    "Each client implements **three core methods**:\n",
    "- `get_parameters()` ‚Üí Returns current model parameters.\n",
    "- `fit(parameters)` ‚Üí Trains locally and returns updated parameters.\n",
    "- `evaluate(parameters)` ‚Üí Evaluates model on local data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowerClient(fl.client.NumPyClient):\n",
    "    def __init__(self, model, train_data, test_data):\n",
    "        self.model = model\n",
    "        self.train_data = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "        self.test_data = DataLoader(test_data, batch_size=64)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "    def get_parameters(self, config=None):\n",
    "        return [val.cpu().numpy() for val in self.model.state_dict().values()]\n",
    "\n",
    "    def set_parameters(self, parameters):\n",
    "        params_dict = zip(self.model.state_dict().keys(), parameters)\n",
    "        state_dict = {k: torch.tensor(v) for k, v in params_dict}\n",
    "        self.model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "    def fit(self, parameters, config=None):\n",
    "        self.set_parameters(parameters)\n",
    "        self.model.train()\n",
    "        for _ in range(1):\n",
    "            for X, y in self.train_data:\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self.criterion(self.model(X), y)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "        return self.get_parameters(), len(self.train_data.dataset), {}\n",
    "\n",
    "    def evaluate(self, parameters, config=None):\n",
    "        self.set_parameters(parameters)\n",
    "        self.model.eval()\n",
    "        loss, correct = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for X, y in self.test_data:\n",
    "                preds = self.model(X)\n",
    "                loss += self.criterion(preds, y).item()\n",
    "                correct += (preds.argmax(1) == y).type(torch.float).sum().item()\n",
    "        accuracy = correct / len(self.test_data.dataset)\n",
    "        return float(loss), len(self.test_data.dataset), {\"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß© 5. Launching a Flower Simulation\n",
    "\n",
    "Flower makes it easy to **simulate multiple clients** on a single machine using `flwr.simulation.start_simulation()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def client_fn(cid: str):\n",
    "    model = Net()\n",
    "    train_data = client_datasets[int(cid)]\n",
    "    test_data = random_split(test_dataset, [5000, 5000])[0]\n",
    "    return FlowerClient(model, train_data, test_data)\n",
    "\n",
    "strategy = fl.server.strategy.FedAvg(\n",
    "    fraction_fit=1.0,\n",
    "    min_fit_clients=5,\n",
    "    min_available_clients=5,\n",
    ")\n",
    "\n",
    "# Simulate 5 clients\n",
    "fl.simulation.start_simulation(\n",
    "    client_fn=client_fn,\n",
    "    num_clients=5,\n",
    "    config=fl.server.ServerConfig(num_rounds=3),\n",
    "    strategy=strategy,\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Federated Training Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 6. Observations\n",
    "- Each client trains **locally on its data subset**.\n",
    "- The server aggregates updates via **Federated Averaging (FedAvg)**.\n",
    "- Flower‚Äôs built-in simulation allows rapid prototyping before real-world deployment.\n",
    "- Supports PyTorch, TensorFlow, Scikit-learn, and even custom ML frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß≠ 7. Summary\n",
    "- **Flower (FLwr)** is a flexible framework for Federated Learning.\n",
    "- Supports both **simulation** and **real-world deployments**.\n",
    "- Handles client orchestration, aggregation, and communication seamlessly.\n",
    "\n",
    "Next, we‚Äôll explore **Privacy-Preserving ML** : combining Federated Learning with techniques like **Differential Privacy** and **Homomorphic Encryption** to secure model updates even further."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

