{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text to Image Generation\n",
    "\n",
    "In this notebook, we explore **Text-to-Image Generation**, a fascinating subfield of **Generative AI** that converts natural language prompts into images using advanced deep learning models like **DALL¬∑E**, **Stable Diffusion**, and **Midjourney**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "- Understand how text-to-image generation models work\n",
    "- Learn the architecture behind diffusion-based models\n",
    "- Implement basic image generation using the `diffusers` library\n",
    "- Explore prompt engineering for better image generation\n",
    "- Evaluate generated images qualitatively and quantitatively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß© 1. Introduction to Text-to-Image Models\n",
    "\n",
    "Text-to-Image models take a **text prompt** (e.g., *\"a futuristic city in the clouds\"*) and generate a corresponding image.\n",
    "\n",
    "### Key Model Types:\n",
    "- **GAN-based models:** e.g., AttnGAN, StackGAN\n",
    "- **Diffusion-based models:** e.g., Stable Diffusion, Imagen, DALL¬∑E 2\n",
    "- **Transformer-based models:** e.g., Parti, CogView\n",
    "\n",
    "Modern systems primarily use **Diffusion Models**, which iteratively *denoise* random noise into structured images guided by text embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† 2. Understanding Diffusion Models\n",
    "\n",
    "Diffusion models work in two phases:\n",
    "\n",
    "1. **Forward Process:** Gradually adds noise to an image until it becomes pure noise.\n",
    "2. **Reverse Process:** Learns to remove noise step-by-step, generating realistic images from random noise.\n",
    "\n",
    "These models are trained with **text embeddings** (from models like CLIP or T5) to align image generation with text descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required library (if running locally)\n",
    "# !pip install diffusers transformers accelerate torch torchvision matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 3. Setup for Stable Diffusion using ü§ó Diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# Load model (requires a token if using private models)\n",
    "# model_id = 'runwayml/stable-diffusion-v1-5'\n",
    "# pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "# pipe = pipe.to('cuda') if torch.cuda.is_available() else pipe.to('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üñºÔ∏è 4. Generate Images from Text Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text prompt\n",
    "prompt = \"A futuristic city in the clouds with flying cars and neon lights\"\n",
    "\n",
    "# Generate image (commented out for compatibility)\n",
    "# image = pipe(prompt).images[0]\n",
    "# image.save('futuristic_city.png')\n",
    "# image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßÆ 5. Prompt Engineering Techniques\n",
    "\n",
    "Improving prompt quality can significantly enhance output quality.\n",
    "\n",
    "### Example Prompt Improvements\n",
    "\n",
    "- **Simple:** ‚ÄúA cat on a mat.‚Äù  \n",
    "- **Better:** ‚ÄúA realistic photo of a fluffy white cat sitting on a red mat, sunlight through the window.‚Äù  \n",
    "- **Creative:** ‚ÄúA digital painting of a majestic cat meditating on a glowing mat under a neon moon.‚Äù\n",
    "\n",
    "You can also use **negative prompts** to specify unwanted details (e.g., ‚Äúblurry‚Äù, ‚Äúlow quality‚Äù, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† 6. Evaluating Generated Images\n",
    "\n",
    "Evaluation is mostly qualitative, but a few metrics exist:\n",
    "\n",
    "- **FID (Fr√©chet Inception Distance):** Measures image quality vs real data\n",
    "- **IS (Inception Score):** Measures diversity and realism\n",
    "- **CLIPScore:** Measures alignment between text and image semantics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example placeholder for FID or CLIP-based evaluation\n",
    "# from torchmetrics.multimodal import CLIPScore\n",
    "# metric = CLIPScore(model_name_or_path='openai/clip-vit-base-patch16')\n",
    "# score = metric(preds=[image], target=[prompt])\n",
    "# print('CLIPScore:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ 7. Key Takeaways\n",
    "\n",
    "- Text-to-image models translate language into rich visual representations.\n",
    "- Diffusion models like **Stable Diffusion** dominate modern generation tasks.\n",
    "- **Prompt engineering** plays a huge role in output quality.\n",
    "- Image evaluation is often subjective but can be aided with metrics like **FID** and **CLIPScore**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÆ 8. What‚Äôs Next?\n",
    "\n",
    "- Explore **text-to-video** models (e.g., Runway Gen-2, Pika Labs)\n",
    "- Learn **fine-tuning** for personalized image generation (DreamBooth, LoRA)\n",
    "- Experiment with **multi-modal pipelines** combining text, images, and audio."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
