{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization and Pruning\n",
    "\n",
    "### Objective\n",
    "This notebook explores two of the most widely used **model compression techniques** : **Quantization** and **Pruning** : to make deep learning models faster and smaller without significantly compromising accuracy.\n",
    "\n",
    "You‚Äôll learn:\n",
    "- What quantization and pruning are\n",
    "- How to apply them in PyTorch\n",
    "- How they impact model performance and size\n",
    "- How to fine-tune pruned and quantized models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Setup\n",
    "\n",
    "Let's import PyTorch and create a simple neural network for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Simple model\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "model = SimpleNN()\n",
    "print(model)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßÆ 1. Quantization\n",
    "\n",
    "**Quantization** reduces the numerical precision of model parameters and activations (e.g., from 32-bit floating-point to 8-bit integer), which:\n",
    "- Lowers memory usage\n",
    "- Speeds up inference\n",
    "- Reduces energy consumption\n",
    "\n",
    "PyTorch supports several types:\n",
    "- **Dynamic Quantization** (post-training)\n",
    "- **Static Quantization** (calibrated on data)\n",
    "- **Quantization-Aware Training (QAT)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîπ Dynamic Quantization\n",
    "Dynamic quantization quantizes weights and dynamically quantizes activations at runtime ‚Äî it's fast and works well for linear and recurrent layers."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch.quantization as quant\n",
    "\n",
    "model_fp32 = model.eval()\n",
    "\n",
    "# Apply dynamic quantization\n",
    "model_int8 = quant.quantize_dynamic(\n",
    "    model_fp32, {nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Dynamic quantization done!\")\n",
    "print(model_int8)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîπ Checking Size Reduction\n",
    "Let's compare model sizes before and after quantization."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import io, sys\n",
    "\n",
    "def get_size_of_model(model):\n",
    "    buffer = io.BytesIO()\n",
    "    torch.save(model.state_dict(), buffer)\n",
    "    return buffer.getbuffer().nbytes / 1e6  # in MB\n",
    "\n",
    "size_fp32 = get_size_of_model(model_fp32)\n",
    "size_int8 = get_size_of_model(model_int8)\n",
    "\n",
    "print(f\"FP32 model size: {size_fp32:.2f} MB\")\n",
    "print(f\"INT8 model size: {size_int8:.2f} MB\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Observation:** The INT8 model is often 3‚Äì4√ó smaller than the FP32 model, with minimal accuracy drop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîπ Static Quantization (with calibration)\n",
    "Static quantization requires a small dataset to calibrate the model‚Äôs activations."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model_to_quantize = SimpleNN().eval()\n",
    "\n",
    "model_prepared = torch.quantization.prepare(model_to_quantize)\n",
    "\n",
    "# Fake calibration with random data\n",
    "calib_data = torch.randn(100, 784)\n",
    "model_prepared(calib_data)\n",
    "\n",
    "model_int8_static = torch.quantization.convert(model_prepared)\n",
    "print(\"‚úÖ Static quantization complete!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÇÔ∏è 2. Pruning\n",
    "\n",
    "**Pruning** removes less important connections (weights or neurons) from a neural network, making it sparse.\n",
    "\n",
    "Types of pruning:\n",
    "- **Unstructured Pruning:** Individual weights are zeroed out.\n",
    "- **Structured Pruning:** Entire filters or neurons are removed (for hardware acceleration)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîπ Unstructured Pruning Example"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "# Apply pruning to 30% of weights in fc1\n",
    "prune.l1_unstructured(model.fc1, name='weight', amount=0.3)\n",
    "\n",
    "print(\"Sparsity in fc1 weight tensor:\",\n",
    "      100.0 * float(torch.sum(model.fc1.weight == 0)) / model.fc1.weight.nelement(), \"%\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîπ Removing the Pruning Reparametrization\n",
    "After pruning, you can make pruning permanent by removing the masks."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "prune.remove(model.fc1, 'weight')\n",
    "print(\"‚úÖ Pruning mask removed, weights permanently pruned!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîπ Structured Pruning Example\n",
    "We can prune entire channels or filters (useful for CNNs)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3)\n",
    "        self.fc = nn.Linear(32 * 26 * 26, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "cnn = ConvNet()\n",
    "prune.ln_structured(cnn.conv1, name='weight', amount=0.3, n=2, dim=0)\n",
    "print(\"Structured pruning applied on conv1 filters!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîπ Fine-Tuning the Pruned Model\n",
    "After pruning, fine-tuning (retraining) helps recover accuracy."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Example fine-tuning step\n",
    "for _ in range(3):\n",
    "    data = torch.randn(16, 784)\n",
    "    target = torch.randint(0, 10, (16,))\n",
    "    output = model(data)\n",
    "    loss = F.cross_entropy(output, target)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"‚úÖ Fine-tuning complete!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 3. Comparing Compression Techniques\n",
    "\n",
    "| Technique | What It Does | Compression | Accuracy Drop | Use Case |\n",
    "|------------|---------------|--------------|----------------|-----------|\n",
    "| Quantization | Lower precision of weights | 4x smaller | Low | Deployment on CPUs/Edge |\n",
    "| Unstructured Pruning | Zero out small weights | 30-90% smaller | Moderate | Research or custom hardware |\n",
    "| Structured Pruning | Remove entire filters | High | Low‚ÄìModerate | CNN deployment |\n",
    "| Combined | Quantization + Pruning | Very High | Low‚ÄìModerate | Efficient inference |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "- What quantization and pruning are\n",
    "- How to apply dynamic & static quantization in PyTorch\n",
    "- How to prune both individual weights and channels\n",
    "- How to fine-tune a pruned model\n",
    "\n",
    "Together, these techniques form the foundation of **energy-efficient AI deployment**.\n",
    "\n",
    "üöÄ **Next:** Try combining both quantization and pruning for your trained model and evaluate performance gains."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
