{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy Efficient AI and Model Compression\n",
    "\n",
    "### üéØ Objective\n",
    "In this notebook, we explore how to make deep learning models more energy-efficient and computationally lightweight. You‚Äôll learn about **model compression** techniques like pruning, quantization, and knowledge distillation, which help reduce computational cost and energy consumption without sacrificing too much accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Energy Efficiency Matters\n",
    "\n",
    "Modern AI models such as GPTs, BERT, and ResNet require massive computational power. As these models scale up, they consume large amounts of **energy**, increasing **carbon footprint** and **deployment costs**.\n",
    "\n",
    "Thus, **energy-efficient AI** focuses on optimizing models for:\n",
    "- Faster inference on edge devices (e.g., mobile phones, IoT)\n",
    "- Reduced energy consumption\n",
    "- Smaller memory footprint\n",
    "- Sustainable computing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Model Compression Overview\n",
    "\n",
    "**Model compression** techniques reduce model size and complexity while maintaining performance. Main strategies include:\n",
    "\n",
    "1. **Pruning** : Removing unnecessary weights or neurons.\n",
    "2. **Quantization** : Reducing precision of weights (e.g., from float32 to int8).\n",
    "3. **Knowledge Distillation** : Training a smaller model (student) to mimic a larger one (teacher).\n",
    "4. **Low-Rank Factorization** : Decomposing matrices to reduce redundant parameters.\n",
    "5. **Neural Architecture Search (NAS)** : Automatically discovering efficient model designs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 1. Model Pruning\n",
    "\n",
    "**Idea:** Remove weights that have little impact on the model‚Äôs output.\n",
    "\n",
    "There are two types of pruning:\n",
    "- **Unstructured pruning:** Removes individual weights.\n",
    "- **Structured pruning:** Removes entire neurons, filters, or channels.\n",
    "\n",
    "### Example (PyTorch):\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn as nn\n",
    "\n",
    "model = nn.Sequential(nn.Linear(10, 20), nn.ReLU(), nn.Linear(20, 1))\n",
    "\n",
    "# Apply pruning to 40% of weights in the first layer\n",
    "prune.l1_unstructured(model[0], name=\"weight\", amount=0.4)\n",
    "\n",
    "print(\"Sparsity after pruning:\", float(torch.sum(model[0].weight == 0)) / model[0].weight.nelement())"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Result:** Pruning introduces sparsity, reducing active parameters and computation time.\n",
    "\n",
    "After pruning, retrain the model briefly to recover accuracy (**fine-tuning**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è 2. Quantization\n",
    "\n",
    "**Idea:** Represent weights with lower bit precision to reduce memory and computation.\n",
    "\n",
    "Common formats:\n",
    "- FP32 ‚Üí INT8 or FP16\n",
    "- Reduces model size by up to 75%\n",
    "- Can accelerate inference on CPUs and edge devices.\n",
    "\n",
    "### Example (Post-training quantization with PyTorch):\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch.quantization\n",
    "\n",
    "model_fp32 = model.eval()\n",
    "model_int8 = torch.quantization.quantize_dynamic(model_fp32, {nn.Linear}, dtype=torch.qint8)\n",
    "\n",
    "print(\"Original model size:\", sum(p.numel() for p in model_fp32.parameters()))\n",
    "print(\"Quantized model size:\", sum(p.numel() for p in model_int8.parameters()))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Observation:** Quantization helps run models efficiently on hardware with limited resources (like smartphones or Raspberry Pi)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß© 3. Knowledge Distillation\n",
    "\n",
    "**Idea:** Train a small ‚Äústudent‚Äù model to replicate the predictions of a large ‚Äúteacher‚Äù model.\n",
    "\n",
    "The student learns from the teacher‚Äôs **soft targets** (probability distributions) instead of hard labels.\n",
    "\n",
    "### Example Flow:\n",
    "```python\n",
    "teacher_model = ... # large pretrained model\n",
    "student_model = ... # smaller model\n",
    "\n",
    "for data, target in dataloader:\n",
    "    with torch.no_grad():\n",
    "        teacher_logits = teacher_model(data)\n",
    "\n",
    "    student_logits = student_model(data)\n",
    "    loss = distillation_loss(student_logits, teacher_logits, target)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "```\n",
    "\n",
    "üìò The **distillation loss** is usually a combination of cross-entropy and Kullback-Leibler (KL) divergence between teacher and student logits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßÆ 4. Low-Rank Factorization\n",
    "\n",
    "**Idea:** Approximate weight matrices using low-rank decompositions (e.g., SVD). This reduces redundant parameters.\n",
    "\n",
    "For a weight matrix $W \\in \\mathbb{R}^{m \\times n}$, we approximate it as $W = UŒ£V^T$, keeping only top-k singular values.\n",
    "\n",
    "‚úÖ Helps compress large layers like fully connected or convolutional layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° 5. Neural Architecture Search (NAS)\n",
    "\n",
    "**Idea:** Automatically discover architectures that balance accuracy, latency, and efficiency.\n",
    "\n",
    "For example, **MobileNet** and **EfficientNet** families were designed using NAS to optimize for both accuracy and energy use.\n",
    "\n",
    "Techniques:\n",
    "- Reinforcement Learning-based NAS\n",
    "- Evolutionary algorithms\n",
    "- Gradient-based NAS (e.g., DARTS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üå± Sustainable AI Practices\n",
    "\n",
    "Beyond technical optimizations, developers can reduce AI‚Äôs environmental impact by:\n",
    "- Using **green data centers** powered by renewable energy.\n",
    "- Preferring **smaller pre-trained models** when possible.\n",
    "- Sharing models to reduce redundant training (e.g., via Hugging Face Hub).\n",
    "- Monitoring **energy usage** via tools like `CodeCarbon`.\n",
    "\n",
    "### Example: Estimating training carbon footprint\n",
    "```python\n",
    "from codecarbon import EmissionsTracker\n",
    "\n",
    "tracker = EmissionsTracker()\n",
    "tracker.start()\n",
    "# train_model()\n",
    "tracker.stop()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Summary\n",
    "\n",
    "| Technique | Goal | Benefit |\n",
    "|------------|------|----------|\n",
    "| Pruning | Remove redundant parameters | Reduces computation |\n",
    "| Quantization | Lower precision weights | Reduces memory & latency |\n",
    "| Knowledge Distillation | Smaller model mimics large model | Retains accuracy |\n",
    "| Low-Rank Factorization | Compress large matrices | Fewer parameters |\n",
    "| NAS | Auto-optimize models | Efficient architectures |\n",
    "\n",
    "These methods make models **greener, faster, and deployable** on limited hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Next Steps\n",
    "- Try pruning or quantizing your own model (e.g., a trained CNN).\n",
    "- Deploy a compressed model on an edge device (like Raspberry Pi).\n",
    "- Measure energy savings using tools like `CodeCarbon`.\n",
    "\n",
    "**Up Next:** `README.md` for `10-Advanced_Concepts` ‚Äî summarizing all advanced techniques and concepts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
