{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Pipeline Building\n",
    "\n",
    "Machine learning projects often require multiple steps:\n",
    "- Data preprocessing (handling missing values, scaling, encoding)\n",
    "- Feature selection or dimensionality reduction\n",
    "- Model training and evaluation\n",
    "\n",
    "Instead of applying these steps manually, we can use **Scikit-learn Pipelines** for a clean and reproducible workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Pipelines?\n",
    "- Ensure reproducibility\n",
    "- Prevent data leakage\n",
    "- Simplify code\n",
    "- Combine preprocessing + modeling in one object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building a Simple Pipeline\n",
    "Pipeline with:\n",
    "- StandardScaler (normalization)\n",
    "- PCA (dimensionality reduction)\n",
    "- Logistic Regression (classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=2)),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Train pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = pipeline.score(X_test, y_test)\n",
    "print(\"Pipeline accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using GridSearch with Pipelines\n",
    "- Pipelines integrate smoothly with **GridSearchCV**.\n",
    "- We can tune preprocessing and model hyperparameters together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'pca__n_components': [2, 3],\n",
    "    'classifier__C': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(pipeline, param_grid, cv=3)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters:\", grid.best_params_)\n",
    "print(\"Best cross-validation score:\", grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Summary\n",
    "- Pipelines combine preprocessing and modeling steps.\n",
    "- Prevents data leakage and ensures clean workflows.\n",
    "- Easily integrated with **GridSearchCV** for hyperparameter tuning.\n",
    "- A good practice for real-world ML projects!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
