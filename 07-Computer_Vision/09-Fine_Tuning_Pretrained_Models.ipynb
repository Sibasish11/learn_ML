{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Pretrained Models\n",
    "\n",
    "In this notebook, we’ll learn how to **fine-tune pretrained models** to achieve better results for custom datasets.\n",
    "\n",
    "**Transfer learning** uses the knowledge learned by a model trained on a large dataset (like ImageNet), and fine-tuning helps adapt it more specifically to your dataset.\n",
    "\n",
    "We'll use **ResNet50** pretrained on **ImageNet**, and perform:\n",
    "- Feature extraction (freezing base layers)\n",
    "- Fine-tuning last layers\n",
    "- Comparing both performances"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1️⃣ Load CIFAR-10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Resize images to match ResNet input size\n",
    "X_train_resized = tf.image.resize(X_train, (224, 224))\n",
    "X_test_resized = tf.image.resize(X_test, (224, 224))\n",
    "\n",
    "X_train_prep = preprocess_input(X_train_resized)\n",
    "X_test_prep = preprocess_input(X_test_resized)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2️⃣ Load Pretrained ResNet50 Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "base_model.trainable = False  # Freeze all layers initially\n",
    "\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history_base = model.fit(X_train_prep, y_train, validation_data=(X_test_prep, y_test), epochs=5, batch_size=64)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3️⃣ Fine-Tune the Model\n",
    "Now, let’s unfreeze some of the last layers of the ResNet model to fine-tune them with a smaller learning rate."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Unfreeze the last few layers\n",
    "for layer in base_model.layers[-10:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Compile with a lower learning rate\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history_fine = model.fit(X_train_prep, y_train, validation_data=(X_test_prep, y_test), epochs=3, batch_size=64)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4️⃣ Evaluate Both Models"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "base_loss, base_acc = model.evaluate(X_test_prep, y_test, verbose=0)\n",
    "print(f\"Before Fine-Tuning: Accuracy = {base_acc*100:.2f}%\")\n",
    "\n",
    "fine_loss, fine_acc = model.evaluate(X_test_prep, y_test, verbose=0)\n",
    "print(f\"After Fine-Tuning: Accuracy = {fine_acc*100:.2f}%\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5️⃣ Visualize Training Performance"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history_base.history['accuracy'], label='Base Train')\n",
    "plt.plot(history_base.history['val_accuracy'], label='Base Val')\n",
    "plt.plot(history_fine.history['accuracy'], label='Fine-Tune Train')\n",
    "plt.plot(history_fine.history['val_accuracy'], label='Fine-Tune Val')\n",
    "plt.legend()\n",
    "plt.title('Accuracy Over Epochs')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history_base.history['loss'], label='Base Train Loss')\n",
    "plt.plot(history_base.history['val_loss'], label='Base Val Loss')\n",
    "plt.plot(history_fine.history['loss'], label='Fine-Tune Train Loss')\n",
    "plt.plot(history_fine.history['val_loss'], label='Fine-Tune Val Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss Over Epochs')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ Summary\n",
    "- Started with **frozen ResNet50** to leverage pretrained ImageNet weights.\n",
    "- Fine-tuned last few layers for improved feature adaptation.\n",
    "- Achieved higher accuracy and better generalization.\n",
    "\n",
    "**Next Steps:**\n",
    "- Experiment with other models like VGG16, EfficientNet, or MobileNet.\n",
    "- Use learning rate schedulers for optimal training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
