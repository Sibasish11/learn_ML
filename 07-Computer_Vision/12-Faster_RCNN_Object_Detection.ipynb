{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster R-CNN Object Detection\n",
    "\n",
    "In this notebook, we’ll explore **Faster R-CNN (Region-based Convolutional Neural Network)** — a two-stage object detection architecture that combines accuracy and efficiency.\n",
    "\n",
    "We'll cover:\n",
    "- Concept of region-based CNNs\n",
    "- Faster R-CNN architecture overview\n",
    "- Using pre-trained Faster R-CNN from PyTorch\n",
    "- Performing inference on sample images\n",
    "- Understanding bounding boxes and predictions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1️⃣ What is Faster R-CNN?\n",
    "\n",
    "**Faster R-CNN** is a deep learning model for **object detection**. It builds upon earlier models like **R-CNN** and **Fast R-CNN** by introducing a **Region Proposal Network (RPN)** that makes the pipeline end-to-end trainable.\n",
    "\n",
    "### ⚙️ Pipeline Overview:\n",
    "1. **CNN Backbone (Feature Extractor)** – Extracts features from the image (e.g., ResNet50).\n",
    "2. **Region Proposal Network (RPN)** – Suggests regions (bounding boxes) that may contain objects.\n",
    "3. **ROI Pooling Layer** – Extracts fixed-size feature maps for each proposed region.\n",
    "4. **Fully Connected Layers + Classifier** – Predicts class label and refines box coordinates.\n",
    "\n",
    "**Key Benefits:**\n",
    "- High accuracy (used in benchmarks like COCO, Pascal VOC)\n",
    "- Suitable for small datasets\n",
    "- Easy to fine-tune on custom data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2️⃣ Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.transforms import functional as F\n",
    "from PIL import Image\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "print('Torch version:', torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3️⃣ Load Pre-trained Faster R-CNN Model\n",
    "\n",
    "We’ll use a **ResNet-50 backbone** version of Faster R-CNN pre-trained on the **COCO dataset**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "print('Model loaded successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4️⃣ Load and Preprocess an Image\n",
    "\n",
    "We’ll use a sample image from the internet for inference. The model expects images as **PyTorch tensors** normalized to `[0, 1]` range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and load an example image\n",
    "url = 'https://ultralytics.com/images/zidane.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw).convert('RGB')\n",
    "\n",
    "# Convert image to tensor\n",
    "img_tensor = F.to_tensor(image)\n",
    "print('Image shape:', img_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5️⃣ Run Object Detection\n",
    "\n",
    "We’ll pass the image tensor to the model and visualize the detected bounding boxes and class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    predictions = model([img_tensor])\n",
    "\n",
    "# Extract prediction data\n",
    "boxes = predictions[0]['boxes']\n",
    "labels = predictions[0]['labels']\n",
    "scores = predictions[0]['scores']\n",
    "\n",
    "print('Number of objects detected:', len(boxes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6️⃣ Visualize Detections\n",
    "\n",
    "We’ll draw bounding boxes and labels on the original image using OpenCV for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert PIL to OpenCV format\n",
    "img_cv = np.array(image)\n",
    "img_cv = cv2.cvtColor(img_cv, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# COCO class labels\n",
    "COCO_INSTANCE_CATEGORY_NAMES = [\n",
    "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter',\n",
    "    'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe',\n",
    "    'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle',\n",
    "    'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli',\n",
    "    'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table',\n",
    "    'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink',\n",
    "    'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "]\n",
    "\n",
    "# Draw boxes for predictions above a confidence threshold\n",
    "for i in range(len(boxes)):\n",
    "    if scores[i] > 0.6:\n",
    "        box = boxes[i].cpu().numpy().astype(int)\n",
    "        label = COCO_INSTANCE_CATEGORY_NAMES[labels[i]]\n",
    "        cv2.rectangle(img_cv, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\n",
    "        cv2.putText(img_cv, label, (box[0], box[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "# Convert BGR to RGB for display\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7️⃣ Custom Model Training (Optional Overview)\n",
    "\n",
    "Faster R-CNN can be fine-tuned on a **custom dataset** by following these steps:\n",
    "\n",
    "1. Prepare your dataset with **images + bounding box annotations** (Pascal VOC format or COCO JSON).\n",
    "2. Use **`torchvision.datasets`** or a custom dataset class to load them.\n",
    "3. Modify the classifier head to match your number of classes:\n",
    "```python\n",
    "num_classes = 3  # background + 2 classes\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "```\n",
    "4. Train using a standard optimizer and scheduler.\n",
    "5. Save the trained model using `torch.save()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ Summary\n",
    "\n",
    "- **Faster R-CNN** is a two-stage detector (RPN + classifier).\n",
    "- Provides high accuracy on complex object detection tasks.\n",
    "- Pre-trained models from **PyTorch** make it easy to use.\n",
    "- Can be fine-tuned for custom applications.\n",
    "\n",
    "---\n",
    "**Next:** `13-Instance_Segmentation_Basics.ipynb` → Learn how to detect and segment objects at the pixel level using **Mask R-CNN**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

