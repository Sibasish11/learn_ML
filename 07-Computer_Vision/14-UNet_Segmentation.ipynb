{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net Segmentation ðŸ§ \n",
    "\n",
    "In this notebook, weâ€™ll implement **U-Net**, a powerful convolutional neural network architecture for **semantic segmentation** â€” particularly useful in **biomedical image segmentation**.\n",
    "\n",
    "U-Net performs **pixel-wise classification** by combining local (spatial) and contextual (semantic) information using a **U-shaped encoder-decoder structure**.\n",
    "\n",
    "Weâ€™ll cover:\n",
    "- U-Net architecture overview\n",
    "- Building U-Net using TensorFlow/Keras\n",
    "- Training on a sample dataset (e.g., Oxford-IIIT Pet dataset)\n",
    "- Visualizing segmentation results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Dataset\n",
    "\n",
    "Weâ€™ll use the **Oxford-IIIT Pet Dataset** available through TensorFlow Datasets for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "dataset, info = tfds.load('oxford_iiit_pet:3.*.*', with_info=True)\n",
    "\n",
    "train = dataset['train']\n",
    "\n",
    "def normalize(input_image, input_mask):\n",
    "    input_image = tf.cast(input_image, tf.float32) / 255.0\n",
    "    input_mask -= 1  # classes: 1, 2, 3 â†’ 0, 1, 2\n",
    "    return input_image, input_mask\n",
    "\n",
    "train = train.map(lambda x: normalize(x['image'], x['segmentation_mask']))\n",
    "train_dataset = train.cache().shuffle(1000).batch(16).repeat()\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize a Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, mask in train.take(1):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(image)\n",
    "    plt.title('Input Image')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(tf.squeeze(mask))\n",
    "    plt.title('Segmentation Mask')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define U-Net Architecture\n",
    "\n",
    "The U-Net consists of:\n",
    "- **Encoder (Contracting path)** â€” series of Conv + MaxPool layers that extract features.\n",
    "- **Bottleneck** â€” deepest layer capturing high-level features.\n",
    "- **Decoder (Expanding path)** â€” upsampling + concatenation layers to recover spatial details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_model(input_size=(128,128,3)):\n",
    "    inputs = layers.Input(input_size)\n",
    "\n",
    "    # Encoder\n",
    "    c1 = layers.Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
    "    c1 = layers.Conv2D(64, 3, activation='relu', padding='same')(c1)\n",
    "    p1 = layers.MaxPooling2D((2,2))(c1)\n",
    "\n",
    "    c2 = layers.Conv2D(128, 3, activation='relu', padding='same')(p1)\n",
    "    c2 = layers.Conv2D(128, 3, activation='relu', padding='same')(c2)\n",
    "    p2 = layers.MaxPooling2D((2,2))(c2)\n",
    "\n",
    "    c3 = layers.Conv2D(256, 3, activation='relu', padding='same')(p2)\n",
    "    c3 = layers.Conv2D(256, 3, activation='relu', padding='same')(c3)\n",
    "    p3 = layers.MaxPooling2D((2,2))(c3)\n",
    "\n",
    "    # Bottleneck\n",
    "    b = layers.Conv2D(512, 3, activation='relu', padding='same')(p3)\n",
    "    b = layers.Conv2D(512, 3, activation='relu', padding='same')(b)\n",
    "\n",
    "    # Decoder\n",
    "    u1 = layers.Conv2DTranspose(256, 2, strides=(2,2), padding='same')(b)\n",
    "    u1 = layers.concatenate([u1, c3])\n",
    "    c4 = layers.Conv2D(256, 3, activation='relu', padding='same')(u1)\n",
    "    c4 = layers.Conv2D(256, 3, activation='relu', padding='same')(c4)\n",
    "\n",
    "    u2 = layers.Conv2DTranspose(128, 2, strides=(2,2), padding='same')(c4)\n",
    "    u2 = layers.concatenate([u2, c2])\n",
    "    c5 = layers.Conv2D(128, 3, activation='relu', padding='same')(u2)\n",
    "    c5 = layers.Conv2D(128, 3, activation='relu', padding='same')(c5)\n",
    "\n",
    "    u3 = layers.Conv2DTranspose(64, 2, strides=(2,2), padding='same')(c5)\n",
    "    u3 = layers.concatenate([u3, c1])\n",
    "    c6 = layers.Conv2D(64, 3, activation='relu', padding='same')(u3)\n",
    "    c6 = layers.Conv2D(64, 3, activation='relu', padding='same')(c6)\n",
    "\n",
    "    outputs = layers.Conv2D(3, (1,1), activation='softmax')(c6)\n",
    "\n",
    "    model = models.Model(inputs=[inputs], outputs=[outputs])\n",
    "    return model\n",
    "\n",
    "model = unet_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compile and Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "EPOCHS = 3  # small for demo\n",
    "steps_per_epoch = info.splits['train'].num_examples // 16\n",
    "\n",
    "history = model.fit(train_dataset, epochs=EPOCHS, steps_per_epoch=steps_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_sample(display_list):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    titles = ['Input Image', 'True Mask', 'Predicted Mask']\n",
    "    for i in range(len(display_list)):\n",
    "        plt.subplot(1,3,i+1)\n",
    "        plt.title(titles[i])\n",
    "        plt.imshow(tf.keras.utils.array_to_img(display_list[i]))\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "for image, mask in train.take(1):\n",
    "    pred_mask = model.predict(tf.expand_dims(image, axis=0))\n",
    "    pred_mask = tf.argmax(pred_mask, axis=-1)[0]\n",
    "    display_sample([image, mask, pred_mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Summary\n",
    "\n",
    "In this notebook, we learned how to:\n",
    "- Build the **U-Net** architecture from scratch.\n",
    "- Train it for semantic segmentation tasks.\n",
    "- Visualize segmentation masks.\n",
    "\n",
    "**Next Steps:**\n",
    "- Use pretrained backbones (ResNet, EfficientNet) for better results.\n",
    "- Apply **Dice loss / IoU metrics** for better segmentation accuracy.\n",
    "- Try **data augmentation** to improve generalization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
