{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering\n",
    "\n",
    "Hierarchical clustering is an **unsupervised learning method** that builds a hierarchy (tree structure) of clusters.\n",
    "\n",
    "### Key Concepts:\n",
    "- Two types:\n",
    "  1. **Agglomerative** (bottom-up) – each point starts as its own cluster, then merges step by step.\n",
    "  2. **Divisive** (top-down) – start with one big cluster, then split recursively.\n",
    "- Results can be visualized using a **dendrogram**.\n",
    "- Unlike K-Means, you don’t need to predefine the number of clusters (though you can cut the dendrogram at desired level).\n",
    "\n",
    "### Pros:\n",
    "- Produces a hierarchy of clusters.\n",
    "- No need to pre-specify K (but you may choose it later).\n",
    "\n",
    "### Cons:\n",
    "- Computationally expensive for large datasets.\n",
    "- Sensitive to noisy data/outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_blobs\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic dataset\n",
    "X, y_true = make_blobs(n_samples=150, centers=3, cluster_std=0.7, random_state=42)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], s=40)\n",
    "plt.title(\"Synthetic Data for Hierarchical Clustering\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform hierarchical clustering (Agglomerative)\n",
    "Z = linkage(X, method='ward')  # Ward minimizes variance within clusters\n",
    "\n",
    "# Plot dendrogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "dendrogram(Z)\n",
    "plt.title(\"Dendrogram (Hierarchical Clustering)\")\n",
    "plt.xlabel(\"Data Points\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut the dendrogram to form clusters (k=3)\n",
    "clusters = fcluster(Z, t=3, criterion='maxclust')\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=clusters, cmap='viridis', s=40)\n",
    "plt.title(\"Clusters from Hierarchical Clustering\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways:\n",
    "- Hierarchical clustering creates a dendrogram that shows how data points are merged.\n",
    "- You can decide the number of clusters by **cutting the dendrogram** at a chosen distance.\n",
    "- More flexible than K-Means (no need to fix K initially).\n",
    "- Works well on small to medium datasets.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

