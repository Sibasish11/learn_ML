{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "Gradient Descent is an optimization algorithm used to minimize a cost (loss) function by iteratively updating parameters in the opposite direction of the gradient.\n",
    "\n",
    "## Steps:\n",
    "1. Initialize parameters (weights)\n",
    "2. Compute predictions using current parameters\n",
    "3. Calculate loss (error)\n",
    "4. Compute gradients (partial derivatives)\n",
    "5. Update parameters\n",
    "6. Repeat until convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Example: Linear Regression with Gradient Descent\n",
    "We will fit a straight line $y = mx + c$ using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic dataset\n",
    "np.random.seed(42)\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "\n",
    "plt.scatter(X, y, alpha=0.7)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Synthetic Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gradient Descent Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, learning_rate=0.1, epochs=1000):\n",
    "    m, n = X.shape\n",
    "    X_b = np.c_[np.ones((m, 1)), X]  # add bias term\n",
    "    \n",
    "    theta = np.random.randn(n+1, 1)  # random initialization\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "        theta -= learning_rate * gradients\n",
    "        \n",
    "        loss = np.mean((X_b.dot(theta) - y) ** 2)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    return theta, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta, losses = gradient_descent(X, y, learning_rate=0.1, epochs=1000)\n",
    "print(\"Learned parameters:\", theta.ravel())\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.title(\"Loss over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Final Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.array([[0], [2]])\n",
    "X_new_b = np.c_[np.ones((2,1)), X_new]\n",
    "y_predict = X_new_b.dot(theta)\n",
    "\n",
    "plt.scatter(X, y, alpha=0.7)\n",
    "plt.plot(X_new, y_predict, \"r-\", linewidth=2)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Linear Regression with Gradient Descent\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Gradient Descent Variants\n",
    "- **Batch Gradient Descent**: Uses whole dataset per step (what we implemented)\n",
    "- **Stochastic Gradient Descent (SGD)**: Uses one sample per step (faster, noisier)\n",
    "- **Mini-Batch Gradient Descent**: Uses small random batches per step (balance of speed & accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Summary\n",
    "- Gradient Descent minimizes loss by iterative updates.\n",
    "- Learning rate controls step size.\n",
    "- Variants: Batch, Stochastic, Mini-Batch.\n",
    "- It is the foundation of optimization in ML/DL."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

