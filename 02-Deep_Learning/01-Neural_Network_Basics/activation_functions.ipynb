{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions in Neural Networks\n",
    "\n",
    "Activation functions decide whether a neuron should be activated or not. They introduce **non-linearity** into the network, helping it learn complex patterns.\n",
    "\n",
    "In this notebook, we will cover:\n",
    "- Sigmoid\n",
    "- Hyperbolic Tangent (Tanh)\n",
    "- ReLU (Rectified Linear Unit)\n",
    "- Leaky ReLU\n",
    "- Softmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sigmoid Function\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "- Output range: (0,1)\n",
    "- Good for probabilities\n",
    "- Problem: vanishing gradients for very large/small x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "x = np.linspace(-10, 10, 200)\n",
    "y = sigmoid(x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.title(\"Sigmoid Function\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"σ(x)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tanh Function\n",
    "\n",
    "$$\n",
    "\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "$$\n",
    "\n",
    "- Output range: (-1, 1)\n",
    "- Zero-centered (better than sigmoid)\n",
    "- Still suffers from vanishing gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.tanh(x)\n",
    "\n",
    "plt.plot(x, y, color=\"orange\")\n",
    "plt.title(\"Tanh Function\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"tanh(x)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ReLU (Rectified Linear Unit)\n",
    "\n",
    "$$\n",
    "ReLU(x) = max(0, x)\n",
    "$$\n",
    "\n",
    "- Output range: [0, ∞)\n",
    "- Very popular in deep learning\n",
    "- Solves vanishing gradient (mostly)\n",
    "- Problem: Dying ReLU (neurons stuck at 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "y = relu(x)\n",
    "\n",
    "plt.plot(x, y, color=\"green\")\n",
    "plt.title(\"ReLU Function\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"ReLU(x)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Leaky ReLU\n",
    "\n",
    "$$\n",
    "LeakyReLU(x) = \\begin{cases} x & x>0 \\\\ 0.01x & x \\leq 0 \\end{cases}\n",
    "$$\n",
    "\n",
    "- Allows small gradient when x < 0\n",
    "- Solves the dying ReLU problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(x, alpha=0.01):\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "y = leaky_relu(x)\n",
    "\n",
    "plt.plot(x, y, color=\"red\")\n",
    "plt.title(\"Leaky ReLU Function\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"LeakyReLU(x)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Softmax\n",
    "\n",
    "$$\n",
    "Softmax(z_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}}\n",
    "$$\n",
    "\n",
    "- Converts vector into probability distribution\n",
    "- Commonly used in **multi-class classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z))\n",
    "    return exp_z / exp_z.sum(axis=0)\n",
    "\n",
    "z = np.array([2.0, 1.0, 0.1])\n",
    "probs = softmax(z)\n",
    "print(\"Softmax Probabilities:\", probs)\n",
    "\n",
    "plt.bar([\"Class 1\", \"Class 2\", \"Class 3\"], probs, color=[\"blue\", \"orange\", \"green\"])\n",
    "plt.title(\"Softmax Output\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ Summary\n",
    "- **Sigmoid** → outputs (0,1), good for binary classification\n",
    "- **Tanh** → outputs (-1,1), better than sigmoid\n",
    "- **ReLU** → default in deep learning, fast & simple\n",
    "- **Leaky ReLU** → fixes dying ReLU\n",
    "- **Softmax** → multi-class probability output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

