{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Transformers\n",
    "\n",
    "Transformers are a deep learning architecture introduced in the paper *\"Attention is All You Need\"* (2017). They are the foundation of modern NLP models like **BERT, GPT, and T5**.\n",
    "\n",
    "Key concepts:\n",
    "- No recurrence (unlike RNNs/LSTMs).\n",
    "- Relies entirely on **self-attention**.\n",
    "- Highly parallelizable → faster training.\n",
    "- Handles long-range dependencies better.\n",
    "\n",
    "In this notebook, we’ll cover:\n",
    "1. What is attention?\n",
    "2. Transformer architecture.\n",
    "3. Using Hugging Face `transformers` library for text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is Attention?\n",
    "\n",
    "Attention allows a model to focus on **relevant parts of the input sequence** when making predictions.\n",
    "\n",
    "Formula:\n",
    "\\[ Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V \\]\n",
    "\n",
    "- **Q (Query)** → what we are looking for.\n",
    "- **K (Key)** → identifiers for values.\n",
    "- **V (Value)** → actual information.\n",
    "\n",
    "Self-attention helps the model understand relationships between words, regardless of their distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Transformer Architecture\n",
    "\n",
    "A Transformer consists of:\n",
    "- **Encoder**: Processes the input.\n",
    "- **Decoder**: Generates the output.\n",
    "\n",
    "Each block contains:\n",
    "- Multi-Head Self Attention\n",
    "- Feedforward Neural Network\n",
    "- Residual Connections + Layer Normalization\n",
    "\n",
    "![Transformer Architecture](https://jalammar.github.io/images/t/transformer_architecture.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using Hugging Face Transformers\n",
    "\n",
    "We’ll load a pretrained model (`distilbert-base-uncased`) for **sentiment analysis**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load sentiment analysis pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Test\n",
    "result = classifier(\"Transformers are revolutionizing NLP!\")[0]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Example:\n",
    "```python\n",
    "{'label': 'POSITIVE', 'score': 0.9998}\n",
    "```\n",
    "This means the model predicts the text is **positive sentiment**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- Transformers use **self-attention** instead of recurrence.\n",
    "- They power state-of-the-art NLP models (BERT, GPT, T5).\n",
    "- Hugging Face `transformers` makes it easy to use pretrained models.\n",
    "\n",
    "Next steps:\n",
    "- Explore **fine-tuning Transformers** for your own dataset.\n",
    "- Learn about **BERT embeddings** and **GPT text generation**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
