{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Networks (GANs) - Basics\n",
    "\n",
    "A **GAN (Generative Adversarial Network)** is made of two models:\n",
    "\n",
    "1. **Generator** â†’ Creates fake data (images, text, etc.)\n",
    "2. **Discriminator** â†’ Tries to distinguish between real and fake data\n",
    "\n",
    "ðŸ”¹ They compete in a **minimax game** until the generator produces realistic outputs.\n",
    "\n",
    "ðŸ“Œ Applications:\n",
    "- Image Generation (faces, art)\n",
    "- Data Augmentation\n",
    "- Style Transfer\n",
    "- Super Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU, Reshape, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(latent_dim):\n",
    "    model = Sequential([\n",
    "        Dense(128, input_dim=latent_dim),\n",
    "        LeakyReLU(0.2),\n",
    "        Dense(256),\n",
    "        LeakyReLU(0.2),\n",
    "        Dense(28*28, activation='tanh'),\n",
    "        Reshape((28, 28, 1))\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "generator = build_generator(100)\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=(28,28,1)),\n",
    "        Dense(256),\n",
    "        LeakyReLU(0.2),\n",
    "        Dense(128),\n",
    "        LeakyReLU(0.2),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(0.0002, 0.5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "discriminator = build_discriminator()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN Model (Generator + Discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gan(generator, discriminator):\n",
    "    discriminator.trainable = False  # freeze discriminator\n",
    "    model = Sequential([generator, discriminator])\n",
    "    model.compile(optimizer=Adam(0.0002, 0.5), loss='binary_crossentropy')\n",
    "    return model\n",
    "\n",
    "gan = build_gan(generator, discriminator)\n",
    "gan.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(X_train, _), (_, _) = mnist.load_data()\n",
    "X_train = X_train / 127.5 - 1.0  # normalize to [-1,1]\n",
    "X_train = np.expand_dims(X_train, axis=-1)\n",
    "\n",
    "latent_dim = 100\n",
    "epochs = 1000\n",
    "batch_size = 64\n",
    "half_batch = batch_size // 2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # ----------------- Train Discriminator -----------------\n",
    "    idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
    "    real_imgs = X_train[idx]\n",
    "    \n",
    "    noise = np.random.normal(0, 1, (half_batch, latent_dim))\n",
    "    fake_imgs = generator.predict(noise)\n",
    "    \n",
    "    d_loss_real = discriminator.train_on_batch(real_imgs, np.ones((half_batch, 1)))\n",
    "    d_loss_fake = discriminator.train_on_batch(fake_imgs, np.zeros((half_batch, 1)))\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "    \n",
    "    # ----------------- Train Generator -----------------\n",
    "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "    valid_y = np.ones((batch_size, 1))\n",
    "    g_loss = gan.train_on_batch(noise, valid_y)\n",
    "    \n",
    "    if epoch % 200 == 0:\n",
    "        print(f\"Epoch {epoch} [D loss: {d_loss[0]:.4f}, acc.: {100*d_loss[1]:.2f}%] [G loss: {g_loss:.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate New Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = np.random.normal(0, 1, (5, latent_dim))\n",
    "gen_imgs = generator.predict(noise)\n",
    "\n",
    "plt.figure(figsize=(10,2))\n",
    "for i in range(5):\n",
    "    plt.subplot(1,5,i+1)\n",
    "    plt.imshow(gen_imgs[i, :, :, 0], cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
