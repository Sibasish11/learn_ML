{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer (ViT)\n",
    "\n",
    "Vision Transformers (ViTs) apply the Transformer architecture—originally designed for NLP—directly to image recognition tasks.\n",
    "\n",
    "## Key Concepts\n",
    "- **Image patches**: An image is divided into fixed-size patches (e.g., 16×16 pixels).\n",
    "- **Linear embedding**: Each patch is flattened and projected into a vector embedding.\n",
    "- **Positional encoding**: Since Transformers lack inherent spatial information, positional encodings are added.\n",
    "- **Transformer encoder**: Multi-head self-attention layers process the sequence of patch embeddings.\n",
    "- **Classification token ([CLS])**: A learnable token prepended to the sequence; its final representation is used for classification.\n",
    "\n",
    "ViTs have achieved state-of-the-art results in image classification and beyond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels=3, patch_size=16, emb_size=768, img_size=224):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.projection = nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_size))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn((img_size // patch_size) ** 2 + 1, emb_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.projection(x).flatten(2).transpose(1, 2)\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "        x = x + self.pos_embedding\n",
    "        return x\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, emb_size=768, num_heads=8, hidden_dim=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=emb_size, num_heads=num_heads, batch_first=True)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(emb_size, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, emb_size)\n",
    "        )\n",
    "        self.ln1 = nn.LayerNorm(emb_size)\n",
    "        self.ln2 = nn.LayerNorm(emb_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_out, _ = self.attn(x, x, x)\n",
    "        x = self.ln1(x + self.dropout(attn_out))\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.ln2(x + self.dropout(ff_out))\n",
    "        return x\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, num_classes=10, emb_size=768, depth=6, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.embedding = PatchEmbedding(in_channels, patch_size, emb_size, img_size)\n",
    "        self.encoders = nn.ModuleList([\n",
    "            TransformerEncoder(emb_size, num_heads) for _ in range(depth)\n",
    "        ])\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(emb_size),\n",
    "            nn.Linear(emb_size, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        for encoder in self.encoders:\n",
    "            x = encoder(x)\n",
    "        cls_token = x[:, 0]\n",
    "        return self.mlp_head(cls_token)\n",
    "\n",
    "# Example usage\n",
    "model = VisionTransformer(img_size=224, patch_size=16, num_classes=10)\n",
    "dummy_input = torch.randn(4, 3, 224, 224)\n",
    "output = model(dummy_input)\n",
    "print(output.shape)  # torch.Size([4, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- Vision Transformers split images into patches instead of using convolutions.\n",
    "- Self-attention enables learning long-range dependencies.\n",
    "- Pretraining on large datasets (like ImageNet-21k or JFT-300M) is crucial.\n",
    "- ViTs compete with CNNs in image classification and extend well to detection, segmentation, and vision-language tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
