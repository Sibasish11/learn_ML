{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer From Scratch (PyTorch)\n",
    "\n",
    "This notebook builds a **mini Transformer** (the key components) from scratch using **PyTorch**:\n",
    "\n",
    "- Scaled Dot-Product Attention\n",
    "- Multi-Head Attention\n",
    "- Positional Encoding\n",
    "- Transformer Encoder & Decoder layers (simple versions)\n",
    "\n",
    "This is an educational implementation (not optimized). It demonstrates the main ideas so you can experiment and extend it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scaled Dot-Product Attention\n",
    "\n",
    "Given queries Q, keys K and values V, attention is:\n",
    "\\[ \\text{Attention}(Q,K,V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V \\]\n",
    "We add an optional mask to prevent attention to certain positions (useful for decoder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    \"\"\"\n",
    "    q: (batch, heads, seq_len_q, depth)\n",
    "    k: (batch, heads, seq_len_k, depth)\n",
    "    v: (batch, heads, seq_len_v, depth)\n",
    "    mask: (batch, 1, 1, seq_len_k) or None\n",
    "    \"\"\"\n",
    "    d_k = q.size(-1)\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (batch, heads, seq_q, seq_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "    attn = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attn, v)  # (batch, heads, seq_q, depth)\n",
    "    return output, attn\n",
    "\n",
    "# quick sanity check shapes\n",
    "B, H, Lq, Lk, D = 2, 4, 5, 6, 16\n",
    "q = torch.rand(B, H, Lq, D)\n",
    "k = torch.rand(B, H, Lk, D)\n",
    "v = torch.rand(B, H, Lk, D)\n",
    "out, att = scaled_dot_product_attention(q, k, v)\n",
    "print('out shape', out.shape, 'att shape', att.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multi-Head Attention\n",
    "\n",
    "We project inputs to multiple heads, apply scaled dot-product attention in each head, then concatenate and project back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, 'd_model must be divisible by num_heads'\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.depth = d_model // num_heads\n",
    "\n",
    "        self.wq = nn.Linear(d_model, d_model)\n",
    "        self.wk = nn.Linear(d_model, d_model)\n",
    "        self.wv = nn.Linear(d_model, d_model)\n",
    "        self.dense = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        # x: (batch, seq_len, d_model) -> (batch, heads, seq_len, depth)\n",
    "        B, S, _ = x.size()\n",
    "        x = x.view(B, S, self.num_heads, self.depth).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        # x: (batch, heads, seq_len, depth) -> (batch, seq_len, d_model)\n",
    "        x = x.transpose(1, 2).contiguous()\n",
    "        B, S, H, D = x.size()\n",
    "        x = x.view(B, S, H * D)\n",
    "        return x\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        # q,k,v: (batch, seq_len, d_model)\n",
    "        B = q.size(0)\n",
    "        q = self.split_heads(self.wq(q))\n",
    "        k = self.split_heads(self.wk(k))\n",
    "        v = self.split_heads(self.wv(v))\n",
    "        # mask expected shape: (batch, 1, 1, seq_k)\n",
    "        attn_output, attn_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "        concat = self.combine_heads(attn_output)\n",
    "        out = self.dense(concat)\n",
    "        return out, attn_weights\n",
    "\n",
    "# quick shape check\n",
    "mha = MultiHeadAttention(d_model=64, num_heads=8)\n",
    "x = torch.rand(2, 10, 64)\n",
    "o, w = mha(x, x, x)\n",
    "print('mha out', o.shape, 'att weights', w.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Positional Encoding\n",
    "Transformers need positional information since they have no recurrence or convolution.\n",
    "We'll implement the sinusoidal positional encoding from the original paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # shape (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, d_model)\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.pe[:, :seq_len]\n",
    "        return x\n",
    "\n",
    "# check\n",
    "pe = PositionalEncoding(d_model=64)\n",
    "test = torch.zeros(1, 10, 64)\n",
    "print(pe(test).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Transformer Encoder Layer\n",
    "Encoder = Multi-Head Attention + Add&Norm + Feedforward + Add&Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, src_mask=None):\n",
    "        attn_out, _ = self.mha(x, x, x, src_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_out))\n",
    "        ff_out = self.ffn(x)\n",
    "        x = self.norm2(x + self.dropout(ff_out))\n",
    "        return x\n",
    "\n",
    "# quick check\n",
    "enc_layer = EncoderLayer(d_model=64, num_heads=8, d_ff=256)\n",
    "dummy = torch.rand(2, 12, 64)\n",
    "out = enc_layer(dummy)\n",
    "print('encoder out', out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Transformer Decoder Layer (basic)\n",
    "Decoder includes masked self-attention, encoder-decoder attention, and feedforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_output, tgt_mask=None, memory_mask=None):\n",
    "        # Masked self-attention (prevent looking at future tokens)\n",
    "        self_attn_out, _ = self.self_mha(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(self_attn_out))\n",
    "        # Encoder-Decoder attention\n",
    "        enc_dec_out, _ = self.enc_dec_mha(x, enc_output, enc_output, memory_mask)\n",
    "        x = self.norm2(x + self.dropout(enc_dec_out))\n",
    "        ff_out = self.ffn(x)\n",
    "        x = self.norm3(x + self.dropout(ff_out))\n",
    "        return x\n",
    "\n",
    "# sanity check\n",
    "dec_layer = DecoderLayer(d_model=64, num_heads=8, d_ff=256)\n",
    "tgt = torch.rand(2, 8, 64)\n",
    "mem = torch.rand(2, 12, 64)\n",
    "out = dec_layer(tgt, mem)\n",
    "print('decoder out', out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Small Transformer Encoder / Decoder Stacks\n",
    "Assemble layers into encoder and decoder stacks and add input/output embeddings + positional encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniTransformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=64, num_heads=8, d_ff=256, num_enc_layers=2, num_dec_layers=2, max_len=100):\n",
    "        super().__init__()\n",
    "        self.src_embed = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embed = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model, max_len)\n",
    "        self.enc_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff) for _ in range(num_enc_layers)])\n",
    "        self.dec_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff) for _ in range(num_dec_layers)])\n",
    "        self.output_proj = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "    def make_tgt_mask(self, tgt_seq_len):\n",
    "        # subsequent mask for causal decoding: shape (1, 1, tgt_len, tgt_len)\n",
    "        mask = torch.tril(torch.ones((tgt_seq_len, tgt_seq_len), device=device)).unsqueeze(0).unsqueeze(0)\n",
    "        return mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        # src: (batch, src_len), tgt: (batch, tgt_len)\n",
    "        src_x = self.pos_enc(self.src_embed(src))\n",
    "        for layer in self.enc_layers:\n",
    "            src_x = layer(src_x)\n",
    "        memory = src_x\n",
    "\n",
    "        tgt_x = self.pos_enc(self.tgt_embed(tgt))\n",
    "        tgt_mask = self.make_tgt_mask(tgt.size(1))\n",
    "        for layer in self.dec_layers:\n",
    "            tgt_x = layer(tgt_x, memory, tgt_mask=tgt_mask, memory_mask=None)\n",
    "        logits = self.output_proj(tgt_x)  # (batch, tgt_len, tgt_vocab)\n",
    "        return logits\n",
    "\n",
    "# quick forward pass with dummy token ids\n",
    "src_vocab, tgt_vocab = 100, 100\n",
    "model = MiniTransformer(src_vocab, tgt_vocab).to(device)\n",
    "src = torch.randint(0, src_vocab, (2, 12)).to(device)\n",
    "tgt = torch.randint(0, tgt_vocab, (2, 8)).to(device)\n",
    "out = model(src, tgt)\n",
    "print('MiniTransformer output shape', out.shape)  # (batch, tgt_len, tgt_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Toy Example: Forward Pass & Loss\n",
    "We'll run a single training step on random data to illustrate usage. This is NOT real training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "model.train()\n",
    "optim.zero_grad()\n",
    "logits = model(src, tgt[:, :-1])  # teacher forcing: input tgt without last token\n",
    "target = tgt[:, 1:].to(device)    # target is next tokens\n",
    "loss = criterion(logits.view(-1, tgt_vocab), target.contiguous().view(-1))\n",
    "loss.backward()\n",
    "optim.step()\n",
    "print('toy training loss:', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Notes & Next Steps\n",
    "- This is a compact educational implementation. Real-world Transformers include many optimizations (layer dropout settings, careful initialization, label smoothing, learning rate schedulers like AdamW + warmup, positional embedding variants, etc.).\n",
    "- To extend:\n",
    "  - Implement masking for padded tokens in encoder/decoder.\n",
    "  - Add attention visualization.\n",
    "  - Train on a real dataset (machine translation toy dataset or tokenize and use small corpus).\n",
    "  - Compare to `torch.nn.Transformer` or Hugging Face `transformers` implementations.\n",
    "\n",
    "✅ You now have a readable from-scratch Transformer you can experiment with!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (PyTorch)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
