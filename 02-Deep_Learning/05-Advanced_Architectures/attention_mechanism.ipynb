{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Mechanism - Basics\n",
    "\n",
    "The **Attention Mechanism** allows a model to focus on the most relevant parts of the input sequence when making predictions.\n",
    "\n",
    "ðŸ”¹ Traditional RNNs/LSTMs compress all information into a single hidden state.\n",
    "ðŸ”¹ Attention creates **weighted combinations** of all hidden states, letting the model learn which parts of the sequence are most important.\n",
    "\n",
    "ðŸ“Œ Widely used in:\n",
    "- Machine Translation\n",
    "- Text Summarization\n",
    "- Speech Recognition\n",
    "- Transformers (BERT, GPT, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement a Simple Attention Layer\n",
    "\n",
    "We create a custom attention layer to demonstrate the concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAttention(Layer):\n",
    "    def __init__(self):\n",
    "        super(SimpleAttention, self).__init__()\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # Calculate attention scores (dot product)\n",
    "        scores = tf.matmul(query, values, transpose_b=True)\n",
    "        \n",
    "        # Apply softmax to get weights\n",
    "        weights = tf.nn.softmax(scores, axis=-1)\n",
    "        \n",
    "        # Weighted sum of values\n",
    "        context = tf.matmul(weights, values)\n",
    "        return context, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Attention Layer with Dummy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy query and values\n",
    "query = tf.random.normal(shape=(1, 1, 8))   # [batch, query_len, hidden_size]\n",
    "values = tf.random.normal(shape=(1, 5, 8))  # [batch, seq_len, hidden_size]\n",
    "\n",
    "attention = SimpleAttention()\n",
    "context, weights = attention(query, values)\n",
    "\n",
    "print(\"Query shape:\", query.shape)\n",
    "print(\"Values shape:\", values.shape)\n",
    "print(\"Context shape:\", context.shape)\n",
    "print(\"Attention weights:\", weights.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of Attention Weights\n",
    "Letâ€™s plot the attention weights to see how much focus is placed on each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.matshow(weights[0].numpy(), cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.title(\"Attention Weights\")\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Query\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
