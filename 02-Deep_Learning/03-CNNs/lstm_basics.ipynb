{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short-Term Memory (LSTM) Basics\n",
    "\n",
    "## 1. Introduction\n",
    "- LSTMs are a special type of RNN designed to overcome the **vanishing gradient problem**.\n",
    "- They use **gates** (input, forget, output) to control the flow of information.\n",
    "- This allows them to capture **long-term dependencies** in sequential data.\n",
    "\n",
    "### Applications:\n",
    "- Text classification (sentiment analysis)\n",
    "- Machine translation\n",
    "- Time-series forecasting\n",
    "- Speech recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LSTM Cell Structure\n",
    "- **Forget gate**: Decides what information to throw away.\n",
    "- **Input gate**: Decides what new information to store.\n",
    "- **Cell state**: Carries long-term memory.\n",
    "- **Output gate**: Decides the final output.\n",
    "\n",
    "This architecture helps LSTMs remember important data for longer periods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Example: LSTM for Sequence Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Example toy dataset: sequences of integers\n",
    "X = [\n",
    "    [1, 2, 3, 4],\n",
    "    [2, 3, 4, 5],\n",
    "    [3, 4, 5, 6],\n",
    "    [4, 5, 6, 7]\n",
    "]\n",
    "y = [0, 1, 0, 1]  # Binary labels\n",
    "\n",
    "X = pad_sequences(X, maxlen=6)\n",
    "y = np.array(y)\n",
    "\n",
    "# Define LSTM model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10, output_dim=8, input_length=6),\n",
    "    LSTM(16),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X, y, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Key Notes\n",
    "- LSTMs handle **long-term dependencies** better than vanilla RNNs.\n",
    "- Can model sequences like text paragraphs or long time-series.\n",
    "- Heavier than simple RNNs in terms of computation.\n",
    "- Alternatives: **GRU** (simpler, fewer gates, faster)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
