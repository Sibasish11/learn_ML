{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Mechanism with LSTM\n",
    "\n",
    "This notebook demonstrates how to use **Attention** with LSTMs.\n",
    "\n",
    "ðŸ”¹ LSTMs capture sequential dependencies but may struggle with long sequences.\n",
    "ðŸ”¹ Attention helps the model **focus on important time steps** instead of compressing everything into the final hidden state.\n",
    "\n",
    "ðŸ“Œ Applications: Machine Translation, Text Summarization, Question Answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Input, Attention\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Toy Sequence Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n_samples, timesteps, vocab_size):\n",
    "    X = np.random.randint(1, vocab_size, (n_samples, timesteps))\n",
    "    y = (np.sum(X, axis=1) % 2 == 0).astype(int)  # even sum â†’ class 1, else 0\n",
    "    return X, y\n",
    "\n",
    "n_samples, timesteps, vocab_size = 1000, 10, 50\n",
    "X, y = generate_data(n_samples, timesteps, vocab_size)\n",
    "print(\"X shape:\", X.shape, \"y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Attention-based LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(timesteps,))\n",
    "embed = Embedding(input_dim=vocab_size, output_dim=32)(inputs)\n",
    "\n",
    "# Return sequences for attention\n",
    "lstm_out = LSTM(64, return_sequences=True)(embed)\n",
    "\n",
    "# Apply Attention\n",
    "attention = Attention()([lstm_out, lstm_out])\n",
    "context_vector = tf.reduce_mean(attention, axis=1)\n",
    "\n",
    "# Output Layer\n",
    "output = Dense(1, activation='sigmoid')(context_vector)\n",
    "\n",
    "model = Model(inputs, output)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X, y, epochs=5, batch_size=32, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = np.random.randint(1, vocab_size, (1, timesteps))\n",
    "pred = model.predict(sample)\n",
    "print(\"Input sequence:\", sample)\n",
    "print(\"Predicted probability:\", pred[0][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
