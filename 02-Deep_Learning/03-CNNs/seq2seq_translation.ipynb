{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence-to-Sequence (Seq2Seq) Model Basics\n",
    "\n",
    "## 1. Introduction\n",
    "- **Seq2Seq** models are used when both input and output are sequences.\n",
    "- Examples:\n",
    "  - Machine Translation (English → French)\n",
    "  - Chatbots\n",
    "  - Text Summarization\n",
    "  - Speech-to-text\n",
    "\n",
    "### Key Components:\n",
    "1. **Encoder**: Reads the input sequence and produces a context vector (hidden state).\n",
    "2. **Decoder**: Takes the context vector and generates the output sequence.\n",
    "\n",
    "Often implemented with RNNs, LSTMs, or GRUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Workflow\n",
    "- Input sequence → Encoder → Context Vector → Decoder → Output sequence\n",
    "- Training uses **Teacher Forcing**: feeding the actual previous output as input to the decoder.\n",
    "- At inference, the decoder predicts step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Example: Toy Seq2Seq with Keras (Number Reversal)\n",
    "Here we’ll build a simple model that learns to **reverse a sequence of numbers**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "\n",
    "# Parameters\n",
    "num_samples = 1000\n",
    "timesteps = 5\n",
    "input_dim = 10  # numbers 0-9\n",
    "\n",
    "# Generate toy dataset (sequence → reversed sequence)\n",
    "X = np.random.randint(0, input_dim, size=(num_samples, timesteps, 1))\n",
    "Y = np.flip(X, axis=1)  # reverse the sequence\n",
    "\n",
    "# One-hot encode\n",
    "X_onehot = tf.keras.utils.to_categorical(X, num_classes=input_dim)\n",
    "Y_onehot = tf.keras.utils.to_categorical(Y, num_classes=input_dim)\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(timesteps, input_dim))\n",
    "encoder_lstm = LSTM(64, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(timesteps, input_dim))\n",
    "decoder_lstm = LSTM(64, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(input_dim, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Train (teacher forcing: decoder input = shifted Y)\n",
    "decoder_input_data = np.zeros_like(Y_onehot)\n",
    "decoder_input_data[:,1:,:] = Y_onehot[:,:-1,:]  # shift right by 1\n",
    "\n",
    "model.fit([X_onehot, decoder_input_data], Y_onehot, batch_size=32, epochs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Key Notes\n",
    "- Seq2Seq can handle input and output sequences of **different lengths**.\n",
    "- Commonly used with **attention mechanisms** for better performance.\n",
    "- Foundation for modern **transformers** (BERT, GPT)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
