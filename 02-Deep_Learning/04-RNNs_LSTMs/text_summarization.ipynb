{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization with Seq2Seq + Attention\n",
    "\n",
    "In this notebook, we’ll build a **sequence-to-sequence model with attention** for abstractive text summarization.\n",
    "\n",
    "- **Input:** A long text (e.g., news article)\n",
    "- **Output:** A shorter version capturing the main points\n",
    "\n",
    "We’ll use a small dataset to demonstrate the approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Dataset\n",
    "We’ll use a **toy dataset** for demonstration.\n",
    "\n",
    "In practice, you would use datasets like **CNN/DailyMail**, **Gigaword**, or **SAMSum**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"The stock market crashed yesterday due to unexpected economic news.\",\n",
    "    \"The football team won the championship after a thrilling match.\",\n",
    "    \"Scientists discovered a new planet that might support life.\"\n",
    "]\n",
    "\n",
    "summaries = [\n",
    "    \"Stock market crash\",\n",
    "    \"Team wins championship\",\n",
    "    \"New planet discovered\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "- Tokenize text and summaries\n",
    "- Pad sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 1000\n",
    "max_text_len = 20\n",
    "max_summary_len = 5\n",
    "\n",
    "tokenizer_text = Tokenizer(num_words=num_words)\n",
    "tokenizer_text.fit_on_texts(texts)\n",
    "X = tokenizer_text.texts_to_sequences(texts)\n",
    "X = pad_sequences(X, maxlen=max_text_len, padding='post')\n",
    "\n",
    "tokenizer_summary = Tokenizer(num_words=num_words)\n",
    "tokenizer_summary.fit_on_texts(summaries)\n",
    "y = tokenizer_summary.texts_to_sequences(summaries)\n",
    "y = pad_sequences(y, maxlen=max_summary_len, padding='post')\n",
    "\n",
    "print(\"Text shape:\", X.shape)\n",
    "print(\"Summary shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq Model\n",
    "- Encoder: LSTM that processes the input text\n",
    "- Decoder: LSTM that generates the summary\n",
    "- Dense layer with softmax for word prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 50\n",
    "latent_dim = 100\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_text_len,))\n",
    "enc_emb = Embedding(num_words, embedding_dim, mask_zero=True)(encoder_inputs)\n",
    "encoder_lstm, state_h, state_c = LSTM(latent_dim, return_state=True)(enc_emb)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(max_summary_len,))\n",
    "dec_emb = Embedding(num_words, embedding_dim, mask_zero=True)(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_words, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "(On real datasets this would take hours. Here we just show the setup.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_target_data = np.expand_dims(y, -1)\n",
    "history = model.fit([X, y], decoder_target_data, batch_size=2, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "Normally we would:\n",
    "- Use the encoder to encode input text.\n",
    "- Use the decoder step-by-step to generate words until an end token is predicted.\n",
    "\n",
    "But since this is a demo, we’ll stop here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
