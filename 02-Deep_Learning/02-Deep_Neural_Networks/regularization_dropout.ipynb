{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization and Dropout in Neural Networks\n",
    "\n",
    "Deep neural networks can easily **overfit** the training data. To reduce overfitting, we use:\n",
    "\n",
    "- **L2 Regularization (Weight Decay)**: Penalizes large weights.\n",
    "- **Dropout**: Randomly drops neurons during training to prevent co-adaptation.\n",
    "\n",
    "In this notebook, weâ€™ll demonstrate both techniques on the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras import regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "print(\"Training data:\", x_train.shape)\n",
    "print(\"Test data:\", x_test.shape)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model_l2 = Sequential([\n",
    "    Flatten(input_shape=(28,28)),\n",
    "    Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model_l2.compile(optimizer='adam',\n",
    "                 loss='sparse_categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "history_l2 = model_l2.fit(x_train, y_train, epochs=5, batch_size=32,\n",
    "                          validation_data=(x_test, y_test), verbose=0)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with Dropout"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model_dropout = Sequential([\n",
    "    Flatten(input_shape=(28,28)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),  # Drop 50% neurons during training\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model_dropout.compile(optimizer='adam',\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "history_dropout = model_dropout.fit(x_train, y_train, epochs=5, batch_size=32,\n",
    "                                    validation_data=(x_test, y_test), verbose=0)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.plot(history_l2.history['val_accuracy'], label='L2 Regularization')\n",
    "plt.plot(history_dropout.history['val_accuracy'], label='Dropout')\n",
    "plt.title('Validation Accuracy Comparison')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "- **L2 Regularization** keeps weights small and prevents overfitting.\n",
    "- **Dropout** forces the network to be more robust by not relying on specific neurons.\n",
    "- In practice, both techniques can be combined for better generalization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
